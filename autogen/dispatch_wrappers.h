
at::Tensor wrap__cast_Byte(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool non_blocking) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_cast_Byte(dispatchKeySet, self, non_blocking);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__CAST_BYTE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, non_blocking);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__cast_Char(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool non_blocking) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_cast_Char(dispatchKeySet, self, non_blocking);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__CAST_CHAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, non_blocking);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__cast_Double(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool non_blocking) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_cast_Double(dispatchKeySet, self, non_blocking);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__CAST_DOUBLE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, non_blocking);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__cast_Float(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool non_blocking) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_cast_Float(dispatchKeySet, self, non_blocking);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__CAST_FLOAT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, non_blocking);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__cast_Int(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool non_blocking) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_cast_Int(dispatchKeySet, self, non_blocking);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__CAST_INT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, non_blocking);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__cast_Long(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool non_blocking) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_cast_Long(dispatchKeySet, self, non_blocking);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__CAST_LONG, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, non_blocking);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__cast_Short(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool non_blocking) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_cast_Short(dispatchKeySet, self, non_blocking);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__CAST_SHORT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, non_blocking);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__cast_Half(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool non_blocking) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_cast_Half(dispatchKeySet, self, non_blocking);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__CAST_HALF, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, non_blocking);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

void wrap__backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::TensorList inputs, const c10::optional<at::Tensor> & gradient, c10::optional<bool> retain_graph, bool create_graph) {
  ensure_materialized(self);ensure_materialized(inputs);ensure_materialized(gradient);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::__dispatch__backward(dispatchKeySet, self, std::move(inputs), gradient, retain_graph, create_graph);
}

void wrap_set_data(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & new_data) {
  ensure_materialized(self);ensure_materialized(new_data);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::__dispatch_set_data(dispatchKeySet, self, new_data);
}

at::Tensor wrap_data(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__dispatch_data(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DATA, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

bool wrap_is_leaf(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::__dispatch_is_leaf(dispatchKeySet, self);
}

int64_t wrap_output_nr(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::__dispatch_output_nr(dispatchKeySet, self);
}

int64_t wrap__version(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::__dispatch__version(dispatchKeySet, self);
}

at::Tensor & wrap_requires_grad_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, bool requires_grad) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__dispatch_requires_grad_(dispatchKeySet, self, requires_grad);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_REQUIRES_GRAD_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, requires_grad);
  finish_in_place(tt, trace_idx);
  return self;
}

void wrap_retain_grad(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::__dispatch_retain_grad(dispatchKeySet, self);
}

at::Tensor wrap__fw_primal(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t level) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_fw_primal(dispatchKeySet, self, level);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__FW_PRIMAL, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, level);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__make_dual(c10::DispatchKeySet dispatchKeySet, const at::Tensor & primal, const at::Tensor & tangent, int64_t level) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_make_dual(dispatchKeySet, primal, tangent, level);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(primal.dtype(), primal.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__MAKE_DUAL, dispatchKeySet);
  trace.append_arg(trace_idx, primal);trace.append_arg(trace_idx, tangent);trace.append_arg(trace_idx, level);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap__unpack_dual(c10::DispatchKeySet dispatchKeySet, const at::Tensor & dual, int64_t level) {
  ensure_materialized(dual);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_unpack_dual(dispatchKeySet, dual, level);
}

at::Tensor & wrap_rename_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, c10::optional<at::DimnameList> names) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rename_(dispatchKeySet, self, std::move(names));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RENAME_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(names));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_rename(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::DimnameList> names) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rename(dispatchKeySet, self, std::move(names));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RENAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(names));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_align_to(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList names) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::align_to(dispatchKeySet, self, std::move(names));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ALIGN_TO, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(names));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_align_to_ellipsis_idx(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList order, int64_t ellipsis_idx) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::align_to(dispatchKeySet, self, std::move(order), ellipsis_idx);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ALIGN_TO_ELLIPSIS_IDX, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(order));trace.append_arg(trace_idx, ellipsis_idx);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_align_as(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::align_as(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ALIGN_AS, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::vector<at::Tensor> wrap_align_tensors(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::align_tensors(dispatchKeySet, std::move(tensors));
}

void wrap__assert_async(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_assert_async(dispatchKeySet, self);
}

at::Tensor wrap_refine_names(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList names) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::refine_names(dispatchKeySet, self, std::move(names));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_REFINE_NAMES, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(names));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

bool wrap__use_cudnn_ctc_loss(c10::DispatchKeySet dispatchKeySet, const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank) {
  ensure_materialized(log_probs);ensure_materialized(targets);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_use_cudnn_ctc_loss(dispatchKeySet, log_probs, targets, std::move(input_lengths), std::move(target_lengths), blank);
}

std::tuple<at::Tensor,at::Tensor> wrap__cudnn_ctc_loss(c10::DispatchKeySet dispatchKeySet, const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity) {
  ensure_materialized(log_probs);ensure_materialized(targets);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_cudnn_ctc_loss(dispatchKeySet, log_probs, targets, std::move(input_lengths), std::move(target_lengths), blank, deterministic, zero_infinity);
}

bool wrap__use_cudnn_rnn_flatten_weight(c10::DispatchKeySet dispatchKeySet) {
  
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_use_cudnn_rnn_flatten_weight(dispatchKeySet);
}

at::Tensor wrap__cudnn_rnn_flatten_weight(c10::DispatchKeySet dispatchKeySet, at::TensorList weight_arr, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t proj_size, int64_t num_layers, bool batch_first, bool bidirectional) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_cudnn_rnn_flatten_weight(dispatchKeySet, std::move(weight_arr), weight_stride0, input_size, mode, hidden_size, proj_size, num_layers, batch_first, bidirectional);
  }
  auto defaults = compute_dtype(weight_arr);
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(default_dtype, default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__CUDNN_RNN_FLATTEN_WEIGHT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(weight_arr));trace.append_arg(trace_idx, weight_stride0);trace.append_arg(trace_idx, input_size);trace.append_arg(trace_idx, mode);trace.append_arg(trace_idx, hidden_size);trace.append_arg(trace_idx, proj_size);trace.append_arg(trace_idx, num_layers);trace.append_arg(trace_idx, batch_first);trace.append_arg(trace_idx, bidirectional);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrap__cudnn_rnn(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const c10::optional<at::Tensor> & weight_buf, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, int64_t mode, int64_t hidden_size, int64_t proj_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, at::IntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state) {
  ensure_materialized(input);ensure_materialized(weight);ensure_materialized(weight_buf);ensure_materialized(hx);ensure_materialized(cx);ensure_materialized(dropout_state);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_cudnn_rnn(dispatchKeySet, input, std::move(weight), weight_stride0, weight_buf, hx, cx, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, std::move(batch_sizes), dropout_state);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,std::vector<at::Tensor>> wrap__cudnn_rnn_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const at::Tensor & weight_buf, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, const at::Tensor & output, const c10::optional<at::Tensor> & grad_output, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, int64_t mode, int64_t hidden_size, int64_t proj_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, at::IntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state, const at::Tensor & reserve, std::array<bool,4> output_mask) {
  ensure_materialized(input);ensure_materialized(weight);ensure_materialized(weight_buf);ensure_materialized(hx);ensure_materialized(cx);ensure_materialized(output);ensure_materialized(grad_output);ensure_materialized(grad_hy);ensure_materialized(grad_cy);ensure_materialized(dropout_state);ensure_materialized(reserve);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_cudnn_rnn_backward(dispatchKeySet, input, std::move(weight), weight_stride0, weight_buf, hx, cx, output, grad_output, grad_hy, grad_cy, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, std::move(batch_sizes), dropout_state, reserve, std::move(output_mask));
}

at::Tensor wrap__cudnn_init_dropout_state(c10::DispatchKeySet dispatchKeySet, double dropout, bool train, int64_t dropout_seed, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_cudnn_init_dropout_state(dispatchKeySet, dropout, train, dropout_seed, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__CUDNN_INIT_DROPOUT_STATE, dispatchKeySet);
  trace.append_arg(trace_idx, dropout);trace.append_arg(trace_idx, train);trace.append_arg(trace_idx, dropout_seed);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

int64_t wrap__debug_has_internal_overlap(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_debug_has_internal_overlap(dispatchKeySet, self);
}

std::tuple<at::Tensor,at::Tensor> wrap__fused_dropout(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double p, c10::optional<at::Generator> generator) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_fused_dropout(dispatchKeySet, self, p, std::move(generator));
}

at::Tensor wrap__masked_scale(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask, double scale) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_masked_scale(dispatchKeySet, self, mask, scale);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__MASKED_SCALE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mask);trace.append_arg(trace_idx, scale);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap__sobol_engine_draw(c10::DispatchKeySet dispatchKeySet, const at::Tensor & quasi, int64_t n, const at::Tensor & sobolstate, int64_t dimension, int64_t num_generated, c10::optional<at::ScalarType> dtype) {
  ensure_materialized(quasi);ensure_materialized(sobolstate);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_sobol_engine_draw(dispatchKeySet, quasi, n, sobolstate, dimension, num_generated, std::move(dtype));
}

at::Tensor & wrap__sobol_engine_ff_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t n, const at::Tensor & sobolstate, int64_t dimension, int64_t num_generated) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sobol_engine_ff_(dispatchKeySet, self, n, sobolstate, dimension, num_generated);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__SOBOL_ENGINE_FF_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, sobolstate);trace.append_arg(trace_idx, dimension);trace.append_arg(trace_idx, num_generated);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap__sobol_engine_scramble_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & ltm, int64_t dimension) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sobol_engine_scramble_(dispatchKeySet, self, ltm, dimension);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__SOBOL_ENGINE_SCRAMBLE_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, ltm);trace.append_arg(trace_idx, dimension);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap__sobol_engine_initialize_state_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dimension) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sobol_engine_initialize_state_(dispatchKeySet, self, dimension);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__SOBOL_ENGINE_INITIALIZE_STATE_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dimension);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap__reshape_from_tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & shape) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_reshape_from_tensor(dispatchKeySet, self, shape);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__RESHAPE_FROM_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, shape);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__shape_as_tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_shape_as_tensor(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SHAPE_AS_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_dropout(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, double p, bool train) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::dropout(dispatchKeySet, input, p, train);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DROPOUT, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, train);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_dropout_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double p, bool train) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::dropout_(dispatchKeySet, self, p, train);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_DROPOUT_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, train);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_feature_dropout(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, double p, bool train) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::feature_dropout(dispatchKeySet, input, p, train);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FEATURE_DROPOUT, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, train);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_feature_dropout_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double p, bool train) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::feature_dropout_(dispatchKeySet, self, p, train);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FEATURE_DROPOUT_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, train);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_alpha_dropout(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, double p, bool train) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::alpha_dropout(dispatchKeySet, input, p, train);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ALPHA_DROPOUT, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, train);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_alpha_dropout_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double p, bool train) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::alpha_dropout_(dispatchKeySet, self, p, train);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ALPHA_DROPOUT_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, train);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_feature_alpha_dropout(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, double p, bool train) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::feature_alpha_dropout(dispatchKeySet, input, p, train);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FEATURE_ALPHA_DROPOUT, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, train);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_feature_alpha_dropout_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double p, bool train) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::feature_alpha_dropout_(dispatchKeySet, self, p, train);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FEATURE_ALPHA_DROPOUT_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, train);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_abs(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::abs(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ABS, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_abs_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::abs_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ABS_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_abs_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::abs_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ABS_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_absolute(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::absolute(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ABSOLUTE, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_absolute_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::absolute_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ABSOLUTE_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_absolute_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::absolute_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ABSOLUTE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_angle(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::angle(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ANGLE, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_angle_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::angle_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ANGLE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_view_as_real(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::view_as_real(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_VIEW_AS_REAL, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_view_as_complex(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::view_as_complex(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_VIEW_AS_COMPLEX, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_sgn_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sgn_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SGN_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_real(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::real(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_REAL, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_imag(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::imag(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_IMAG, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_conj(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::conj(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CONJ, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_conj_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::conj_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CONJ_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap__conj(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_conj(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__CONJ, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_acos_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::acos_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ACOS_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_arccos(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arccos(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ARCCOS, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_arccos_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arccos_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ARCCOS_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_arccos_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arccos_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ARCCOS_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_avg_pool1d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::avg_pool1d(dispatchKeySet, self, std::move(kernel_size), std::move(stride), std::move(padding), ceil_mode, count_include_pad);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_AVG_POOL1D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, ceil_mode);trace.append_arg(trace_idx, count_include_pad);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_adaptive_avg_pool1d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::adaptive_avg_pool1d(dispatchKeySet, self, std::move(output_size));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ADAPTIVE_AVG_POOL1D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(output_size));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap_adaptive_max_pool1d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::adaptive_max_pool1d(dispatchKeySet, self, std::move(output_size));
}

at::Tensor wrap_add_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::add(dispatchKeySet, self, other, alpha);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ADD_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, alpha);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_add__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::add_(dispatchKeySet, self, other, alpha);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ADD__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, alpha);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_add_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::add_outf(dispatchKeySet, self, other, alpha, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ADD_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, alpha);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap__add_relu_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_add_relu(dispatchKeySet, self, other, alpha);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__ADD_RELU_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, alpha);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap__add_relu__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_add_relu_(dispatchKeySet, self, other, alpha);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__ADD_RELU__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, alpha);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap__add_relu_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_add_relu_outf(dispatchKeySet, self, other, alpha, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__ADD_RELU_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, alpha);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_add_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::add(dispatchKeySet, self, other, alpha);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ADD_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, alpha);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_add__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::add_(dispatchKeySet, self, other, alpha);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ADD__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, alpha);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_addmv_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat, const at::Tensor & vec, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::addmv_outf(dispatchKeySet, self, mat, vec, beta, alpha, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ADDMV_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mat);trace.append_arg(trace_idx, vec);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, alpha);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_addr(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::addr(dispatchKeySet, self, vec1, vec2, beta, alpha);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ADDR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, vec1);trace.append_arg(trace_idx, vec2);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, alpha);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_addr_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::addr_(dispatchKeySet, self, vec1, vec2, beta, alpha);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ADDR_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, vec1);trace.append_arg(trace_idx, vec2);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, alpha);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_addr_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::addr_outf(dispatchKeySet, self, vec1, vec2, beta, alpha, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ADDR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, vec1);trace.append_arg(trace_idx, vec2);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, alpha);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_affine_grid_generator(c10::DispatchKeySet dispatchKeySet, const at::Tensor & theta, at::IntArrayRef size, bool align_corners) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::affine_grid_generator(dispatchKeySet, theta, std::move(size), align_corners);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(theta.dtype(), theta.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_AFFINE_GRID_GENERATOR, dispatchKeySet);
  trace.append_arg(trace_idx, theta);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, align_corners);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_affine_grid_generator_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, at::IntArrayRef size, bool align_corners) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::affine_grid_generator_backward(dispatchKeySet, grad, std::move(size), align_corners);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_AFFINE_GRID_GENERATOR_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, align_corners);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_all_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::all(dispatchKeySet, self, dim, keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ALL_DIM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_all_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::all_outf(dispatchKeySet, self, dim, keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ALL_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_all_dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::all(dispatchKeySet, self, std::move(dim), keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ALL_DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_all_dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::all_outf(dispatchKeySet, self, std::move(dim), keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ALL_DIMNAME_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

bool wrap_allclose(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, double rtol, double atol, bool equal_nan) {
  ensure_materialized(self);ensure_materialized(other);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::allclose(dispatchKeySet, self, other, rtol, atol, equal_nan);
}

at::Tensor wrap_any_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::any(dispatchKeySet, self, dim, keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ANY_DIM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_any_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::any_outf(dispatchKeySet, self, dim, keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ANY_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_any_dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::any(dispatchKeySet, self, std::move(dim), keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ANY_DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_any_dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::any_outf(dispatchKeySet, self, std::move(dim), keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ANY_DIMNAME_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_arange(c10::DispatchKeySet dispatchKeySet, const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arange(dispatchKeySet, end, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ARANGE, dispatchKeySet);
  trace.append_arg(trace_idx, end);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_arange_start(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arange(dispatchKeySet, start, end, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ARANGE_START, dispatchKeySet);
  trace.append_arg(trace_idx, start);trace.append_arg(trace_idx, end);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_arange_start_step(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arange(dispatchKeySet, start, end, step, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ARANGE_START_STEP, dispatchKeySet);
  trace.append_arg(trace_idx, start);trace.append_arg(trace_idx, end);trace.append_arg(trace_idx, step);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_arange_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & end, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arange_outf(dispatchKeySet, end, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ARANGE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, end);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_arange_start_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arange_outf(dispatchKeySet, start, end, step, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ARANGE_START_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, start);trace.append_arg(trace_idx, end);trace.append_arg(trace_idx, step);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap__dim_arange(c10::DispatchKeySet dispatchKeySet, const at::Tensor & like, int64_t dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_dim_arange(dispatchKeySet, like, dim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(like.dtype(), like.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__DIM_ARANGE, dispatchKeySet);
  trace.append_arg(trace_idx, like);trace.append_arg(trace_idx, dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_argmax(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::argmax(dispatchKeySet, self, dim, keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ARGMAX, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_argmax_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::argmax_outf(dispatchKeySet, self, dim, keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ARGMAX_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_argmin(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::argmin(dispatchKeySet, self, dim, keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ARGMIN, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_argmin_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::argmin_outf(dispatchKeySet, self, dim, keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ARGMIN_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_acosh_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::acosh_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ACOSH_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_arccosh(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arccosh(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ARCCOSH, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_arccosh_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arccosh_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ARCCOSH_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_arccosh_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arccosh_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ARCCOSH_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_asinh_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::asinh_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ASINH_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_arcsinh(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arcsinh(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ARCSINH, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_arcsinh_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arcsinh_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ARCSINH_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_arcsinh_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arcsinh_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ARCSINH_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_atanh_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::atanh_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ATANH_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_arctanh(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arctanh(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ARCTANH, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_arctanh_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arctanh_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ARCTANH_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_arctanh_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arctanh_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ARCTANH_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_as_strided(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, at::IntArrayRef stride, c10::optional<int64_t> storage_offset) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::as_strided(dispatchKeySet, self, std::move(size), std::move(stride), storage_offset);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_AS_STRIDED, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, storage_offset);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

const at::Tensor & wrap_as_strided_(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, at::IntArrayRef stride, c10::optional<int64_t> storage_offset) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::as_strided_(dispatchKeySet, self, std::move(size), std::move(stride), storage_offset);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_AS_STRIDED_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, storage_offset);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_asin(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::asin(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ASIN, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_asin_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::asin_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ASIN_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_asin_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::asin_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ASIN_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_arcsin(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arcsin(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ARCSIN, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_arcsin_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arcsin_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ARCSIN_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_arcsin_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arcsin_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ARCSIN_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_atan_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::atan_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ATAN_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_arctan(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arctan(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ARCTAN, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_arctan_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arctan_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ARCTAN_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_arctan_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::arctan_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ARCTAN_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_atleast_1d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::atleast_1d(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ATLEAST_1D, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::vector<at::Tensor> wrap_atleast_1d_Sequence(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::atleast_1d(dispatchKeySet, std::move(tensors));
}

at::Tensor wrap_atleast_2d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::atleast_2d(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ATLEAST_2D, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::vector<at::Tensor> wrap_atleast_2d_Sequence(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::atleast_2d(dispatchKeySet, std::move(tensors));
}

at::Tensor wrap_atleast_3d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::atleast_3d(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ATLEAST_3D, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::vector<at::Tensor> wrap_atleast_3d_Sequence(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::atleast_3d(dispatchKeySet, std::move(tensors));
}

at::Tensor wrap_baddbmm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::baddbmm(dispatchKeySet, self, batch1, batch2, beta, alpha);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BADDBMM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, batch1);trace.append_arg(trace_idx, batch2);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, alpha);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_baddbmm_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::baddbmm_(dispatchKeySet, self, batch1, batch2, beta, alpha);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BADDBMM_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, batch1);trace.append_arg(trace_idx, batch2);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, alpha);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap__baddbmm_mkl_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_baddbmm_mkl_(dispatchKeySet, self, batch1, batch2, beta, alpha);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__BADDBMM_MKL_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, batch1);trace.append_arg(trace_idx, batch2);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, alpha);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_baddbmm_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::baddbmm_outf(dispatchKeySet, self, batch1, batch2, beta, alpha, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BADDBMM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, batch1);trace.append_arg(trace_idx, batch2);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, alpha);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_bartlett_window(c10::DispatchKeySet dispatchKeySet, int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bartlett_window(dispatchKeySet, window_length, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BARTLETT_WINDOW, dispatchKeySet);
  trace.append_arg(trace_idx, window_length);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_bartlett_window_periodic(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bartlett_window(dispatchKeySet, window_length, periodic, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BARTLETT_WINDOW_PERIODIC, dispatchKeySet);
  trace.append_arg(trace_idx, window_length);trace.append_arg(trace_idx, periodic);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_batch_norm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, bool cudnn_enabled) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::batch_norm(dispatchKeySet, input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BATCH_NORM, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, running_mean);trace.append_arg(trace_idx, running_var);trace.append_arg(trace_idx, training);trace.append_arg(trace_idx, momentum);trace.append_arg(trace_idx, eps);trace.append_arg(trace_idx, cudnn_enabled);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_quantized_batch_norm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & mean, const at::Tensor & var, double eps, double output_scale, int64_t output_zero_point) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::quantized_batch_norm(dispatchKeySet, input, weight, bias, mean, var, eps, output_scale, output_zero_point);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_QUANTIZED_BATCH_NORM, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, mean);trace.append_arg(trace_idx, var);trace.append_arg(trace_idx, eps);trace.append_arg(trace_idx, output_scale);trace.append_arg(trace_idx, output_zero_point);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,int64_t> wrap__batch_norm_impl_index(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, bool cudnn_enabled) {
  ensure_materialized(input);ensure_materialized(weight);ensure_materialized(bias);ensure_materialized(running_mean);ensure_materialized(running_var);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_batch_norm_impl_index(dispatchKeySet, input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap__batch_norm_impl_index_backward(c10::DispatchKeySet dispatchKeySet, int64_t impl_index, const at::Tensor & input, const at::Tensor & grad_output, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_var_transform, bool train, double eps, std::array<bool,3> output_mask, const at::Tensor & reservedSpace) {
  ensure_materialized(input);ensure_materialized(grad_output);ensure_materialized(weight);ensure_materialized(running_mean);ensure_materialized(running_var);ensure_materialized(save_mean);ensure_materialized(save_var_transform);ensure_materialized(reservedSpace);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_batch_norm_impl_index_backward(dispatchKeySet, impl_index, input, grad_output, weight, running_mean, running_var, save_mean, save_var_transform, train, eps, std::move(output_mask), reservedSpace);
}

at::Tensor wrap_bernoulli(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bernoulli(dispatchKeySet, self, std::move(generator));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BERNOULLI, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(generator));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_bernoulli_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Generator> generator, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bernoulli_outf(dispatchKeySet, self, std::move(generator), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BERNOULLI_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(generator));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_bernoulli__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & p, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bernoulli_(dispatchKeySet, self, p, std::move(generator));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BERNOULLI__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, std::move(generator));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_bernoulli__float(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double p, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bernoulli_(dispatchKeySet, self, p, std::move(generator));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BERNOULLI__FLOAT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, std::move(generator));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_bernoulli_p(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double p, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bernoulli(dispatchKeySet, self, p, std::move(generator));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BERNOULLI_P, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, std::move(generator));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_bilinear(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bilinear(dispatchKeySet, input1, input2, weight, bias);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input1.dtype(), input1.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BILINEAR, dispatchKeySet);
  trace.append_arg(trace_idx, input1);trace.append_arg(trace_idx, input2);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_binary_cross_entropy(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::binary_cross_entropy(dispatchKeySet, self, target, weight, reduction);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BINARY_CROSS_ENTROPY, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, reduction);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_binary_cross_entropy_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::binary_cross_entropy_outf(dispatchKeySet, self, target, weight, reduction, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BINARY_CROSS_ENTROPY_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_binary_cross_entropy_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::binary_cross_entropy_backward(dispatchKeySet, grad_output, self, target, weight, reduction);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BINARY_CROSS_ENTROPY_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, reduction);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_binary_cross_entropy_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::binary_cross_entropy_backward_outf(dispatchKeySet, grad_output, self, target, weight, reduction, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BINARY_CROSS_ENTROPY_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_binary_cross_entropy_with_logits(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & pos_weight, int64_t reduction) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::binary_cross_entropy_with_logits(dispatchKeySet, self, target, weight, pos_weight, reduction);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BINARY_CROSS_ENTROPY_WITH_LOGITS, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, pos_weight);trace.append_arg(trace_idx, reduction);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_binary_cross_entropy_with_logits_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & pos_weight, int64_t reduction) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::binary_cross_entropy_with_logits_backward(dispatchKeySet, grad_output, self, target, weight, pos_weight, reduction);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BINARY_CROSS_ENTROPY_WITH_LOGITS_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, pos_weight);trace.append_arg(trace_idx, reduction);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_bincount(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Tensor> & weights, int64_t minlength) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bincount(dispatchKeySet, self, weights, minlength);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BINCOUNT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weights);trace.append_arg(trace_idx, minlength);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_bitwise_not_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bitwise_not_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BITWISE_NOT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_copysign_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::copysign_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_COPYSIGN_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_copysign_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::copysign(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_COPYSIGN_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_copysign__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::copysign_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_COPYSIGN__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_copysign_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::copysign_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_COPYSIGN_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_logical_not(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logical_not(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LOGICAL_NOT, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_logical_not_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logical_not_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOGICAL_NOT_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_logical_not_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logical_not_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOGICAL_NOT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_logical_xor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logical_xor(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LOGICAL_XOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_logical_xor_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logical_xor_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOGICAL_XOR_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_logical_xor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logical_xor_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOGICAL_XOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_logical_and(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logical_and(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LOGICAL_AND, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_logical_and_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logical_and_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOGICAL_AND_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_logical_and_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logical_and_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOGICAL_AND_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_logical_or(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logical_or(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LOGICAL_OR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_logical_or_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logical_or_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOGICAL_OR_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_logical_or_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logical_or_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOGICAL_OR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_blackman_window(c10::DispatchKeySet dispatchKeySet, int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::blackman_window(dispatchKeySet, window_length, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BLACKMAN_WINDOW, dispatchKeySet);
  trace.append_arg(trace_idx, window_length);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_blackman_window_periodic(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::blackman_window(dispatchKeySet, window_length, periodic, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BLACKMAN_WINDOW_PERIODIC, dispatchKeySet);
  trace.append_arg(trace_idx, window_length);trace.append_arg(trace_idx, periodic);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_bmm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bmm(dispatchKeySet, self, mat2);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BMM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mat2);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__bmm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2, bool deterministic) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_bmm(dispatchKeySet, self, mat2, deterministic);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__BMM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mat2);trace.append_arg(trace_idx, deterministic);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_bmm_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bmm_outf(dispatchKeySet, self, mat2, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BMM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mat2);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap__bmm_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2, bool deterministic, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_bmm_outf(dispatchKeySet, self, mat2, deterministic, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__BMM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mat2);trace.append_arg(trace_idx, deterministic);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

std::vector<at::Tensor> wrap_broadcast_tensors(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::broadcast_tensors(dispatchKeySet, std::move(tensors));
}

at::Tensor wrap_broadcast_to(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::broadcast_to(dispatchKeySet, self, std::move(size));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BROADCAST_TO, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(size));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_cat(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cat(dispatchKeySet, std::move(tensors), dim);
  }
  auto defaults = compute_dtype(tensors);
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(default_dtype, default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CAT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));trace.append_arg(trace_idx, dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_cat_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cat_outf(dispatchKeySet, std::move(tensors), dim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CAT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_cat_names(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Dimname dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cat(dispatchKeySet, std::move(tensors), std::move(dim));
  }
  auto defaults = compute_dtype(tensors);
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(default_dtype, default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CAT_NAMES, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));trace.append_arg(trace_idx, std::move(dim));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_cat_names_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cat_outf(dispatchKeySet, std::move(tensors), std::move(dim), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CAT_NAMES_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_block_diag(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::block_diag(dispatchKeySet, std::move(tensors));
  }
  auto defaults = compute_dtype(tensors);
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(default_dtype, default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BLOCK_DIAG, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_ceil(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ceil(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CEIL, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_ceil_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ceil_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CEIL_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_ceil_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ceil_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CEIL_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_chain_matmul(c10::DispatchKeySet dispatchKeySet, at::TensorList matrices) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::chain_matmul(dispatchKeySet, std::move(matrices));
  }
  auto defaults = compute_dtype(matrices);
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(default_dtype, default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CHAIN_MATMUL, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(matrices));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_chain_matmul_out(c10::DispatchKeySet dispatchKeySet, at::TensorList matrices, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::chain_matmul_outf(dispatchKeySet, std::move(matrices), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CHAIN_MATMUL_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(matrices));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

std::vector<at::Tensor> wrap_unsafe_chunk(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t chunks, int64_t dim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::unsafe_chunk(dispatchKeySet, self, chunks, dim);
}

std::vector<at::Tensor> wrap_chunk(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t chunks, int64_t dim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::chunk(dispatchKeySet, self, chunks, dim);
}

std::vector<at::Tensor> wrap_tensor_split_sections(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t sections, int64_t dim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::tensor_split(dispatchKeySet, self, sections, dim);
}

std::vector<at::Tensor> wrap_tensor_split_indices(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef indices, int64_t dim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::tensor_split(dispatchKeySet, self, std::move(indices), dim);
}

std::vector<at::Tensor> wrap_tensor_split_tensor_indices_or_sections(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & tensor_indices_or_sections, int64_t dim) {
  ensure_materialized(self);ensure_materialized(tensor_indices_or_sections);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::tensor_split(dispatchKeySet, self, tensor_indices_or_sections, dim);
}

at::Tensor wrap_clamp(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clamp(dispatchKeySet, self, min, max);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CLAMP, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min);trace.append_arg(trace_idx, max);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_clamp_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clamp(dispatchKeySet, self, min, max);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CLAMP_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min);trace.append_arg(trace_idx, max);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_clamp_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clamp_(dispatchKeySet, self, min, max);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CLAMP_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min);trace.append_arg(trace_idx, max);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_clamp__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clamp_(dispatchKeySet, self, min, max);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CLAMP__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min);trace.append_arg(trace_idx, max);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_clamp_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clamp_outf(dispatchKeySet, self, min, max, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CLAMP_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min);trace.append_arg(trace_idx, max);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_clamp_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clamp_outf(dispatchKeySet, self, min, max, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CLAMP_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min);trace.append_arg(trace_idx, max);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_clamp_max(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & max) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clamp_max(dispatchKeySet, self, max);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CLAMP_MAX, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, max);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_clamp_max_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & max) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clamp_max(dispatchKeySet, self, max);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CLAMP_MAX_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, max);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_clamp_max_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & max) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clamp_max_(dispatchKeySet, self, max);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CLAMP_MAX_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, max);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_clamp_max__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & max) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clamp_max_(dispatchKeySet, self, max);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CLAMP_MAX__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, max);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_clamp_max_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & max, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clamp_max_outf(dispatchKeySet, self, max, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CLAMP_MAX_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, max);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_clamp_max_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & max, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clamp_max_outf(dispatchKeySet, self, max, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CLAMP_MAX_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, max);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_clamp_min(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & min) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clamp_min(dispatchKeySet, self, min);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CLAMP_MIN, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_clamp_min_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & min) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clamp_min(dispatchKeySet, self, min);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CLAMP_MIN_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_clamp_min_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & min) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clamp_min_(dispatchKeySet, self, min);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CLAMP_MIN_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_clamp_min__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & min) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clamp_min_(dispatchKeySet, self, min);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CLAMP_MIN__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_clamp_min_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & min, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clamp_min_outf(dispatchKeySet, self, min, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CLAMP_MIN_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_clamp_min_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & min, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clamp_min_outf(dispatchKeySet, self, min, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CLAMP_MIN_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_clip(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clip(dispatchKeySet, self, min, max);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CLIP, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min);trace.append_arg(trace_idx, max);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_clip_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clip(dispatchKeySet, self, min, max);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CLIP_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min);trace.append_arg(trace_idx, max);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_clip_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clip_(dispatchKeySet, self, min, max);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CLIP_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min);trace.append_arg(trace_idx, max);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_clip__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clip_(dispatchKeySet, self, min, max);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CLIP__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min);trace.append_arg(trace_idx, max);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_clip_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clip_outf(dispatchKeySet, self, min, max, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CLIP_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min);trace.append_arg(trace_idx, max);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_clip_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clip_outf(dispatchKeySet, self, min, max, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CLIP_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min);trace.append_arg(trace_idx, max);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

bool wrap_cudnn_is_acceptable(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::cudnn_is_acceptable(dispatchKeySet, self);
}

at::Tensor wrap_complex(c10::DispatchKeySet dispatchKeySet, const at::Tensor & real, const at::Tensor & imag) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::complex(dispatchKeySet, real, imag);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(real.dtype(), real.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_COMPLEX, dispatchKeySet);
  trace.append_arg(trace_idx, real);trace.append_arg(trace_idx, imag);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_complex_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & real, const at::Tensor & imag, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::complex_outf(dispatchKeySet, real, imag, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_COMPLEX_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, real);trace.append_arg(trace_idx, imag);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_polar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & abs, const at::Tensor & angle) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::polar(dispatchKeySet, abs, angle);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(abs.dtype(), abs.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_POLAR, dispatchKeySet);
  trace.append_arg(trace_idx, abs);trace.append_arg(trace_idx, angle);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_polar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & abs, const at::Tensor & angle, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::polar_outf(dispatchKeySet, abs, angle, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_POLAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, abs);trace.append_arg(trace_idx, angle);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_constant_pad_nd(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef pad, const at::Scalar & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::constant_pad_nd(dispatchKeySet, self, std::move(pad), value);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CONSTANT_PAD_ND, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(pad));trace.append_arg(trace_idx, value);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_contiguous(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::MemoryFormat memory_format) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__dispatch_contiguous(dispatchKeySet, self, std::move(memory_format));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CONTIGUOUS, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(memory_format));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_convolution(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::convolution(dispatchKeySet, input, weight, bias, std::move(stride), std::move(padding), std::move(dilation), transposed, std::move(output_padding), groups);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CONVOLUTION, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, transposed);trace.append_arg(trace_idx, std::move(output_padding));trace.append_arg(trace_idx, groups);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_convolution_overrideable(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::convolution_overrideable(dispatchKeySet, input, weight, bias, std::move(stride), std::move(padding), std::move(dilation), transposed, std::move(output_padding), groups);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CONVOLUTION_OVERRIDEABLE, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, transposed);trace.append_arg(trace_idx, std::move(output_padding));trace.append_arg(trace_idx, groups);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_convolution_backward_overrideable(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, std::array<bool,3> output_mask) {
  ensure_materialized(grad_output);ensure_materialized(input);ensure_materialized(weight);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::convolution_backward_overrideable(dispatchKeySet, grad_output, input, weight, std::move(stride), std::move(padding), std::move(dilation), transposed, std::move(output_padding), groups, std::move(output_mask));
}

at::Tensor wrap__convolution(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_convolution(dispatchKeySet, input, weight, bias, std::move(stride), std::move(padding), std::move(dilation), transposed, std::move(output_padding), groups, benchmark, deterministic, cudnn_enabled, allow_tf32);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__CONVOLUTION, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, transposed);trace.append_arg(trace_idx, std::move(output_padding));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, benchmark);trace.append_arg(trace_idx, deterministic);trace.append_arg(trace_idx, cudnn_enabled);trace.append_arg(trace_idx, allow_tf32);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__convolution_deprecated(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_convolution(dispatchKeySet, input, weight, bias, std::move(stride), std::move(padding), std::move(dilation), transposed, std::move(output_padding), groups, benchmark, deterministic, cudnn_enabled);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__CONVOLUTION_DEPRECATED, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, transposed);trace.append_arg(trace_idx, std::move(output_padding));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, benchmark);trace.append_arg(trace_idx, deterministic);trace.append_arg(trace_idx, cudnn_enabled);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__convolution_mode(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_convolution_mode(dispatchKeySet, input, weight, bias, std::move(stride), std::move(padding), std::move(dilation), groups);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__CONVOLUTION_MODE, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__convolution_nogroup(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_convolution_nogroup(dispatchKeySet, input, weight, bias, std::move(stride), std::move(padding), std::move(dilation), transposed, std::move(output_padding));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__CONVOLUTION_NOGROUP, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, transposed);trace.append_arg(trace_idx, std::move(output_padding));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap__convolution_double_backward(c10::DispatchKeySet dispatchKeySet, const c10::optional<at::Tensor> & ggI, const c10::optional<at::Tensor> & ggW, const c10::optional<at::Tensor> & ggb, const at::Tensor & gO, const at::Tensor & weight, const at::Tensor & self, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32, std::array<bool,3> output_mask) {
  ensure_materialized(ggI);ensure_materialized(ggW);ensure_materialized(ggb);ensure_materialized(gO);ensure_materialized(weight);ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_convolution_double_backward(dispatchKeySet, ggI, ggW, ggb, gO, weight, self, std::move(stride), std::move(padding), std::move(dilation), transposed, std::move(output_padding), groups, benchmark, deterministic, cudnn_enabled, allow_tf32, std::move(output_mask));
}

at::Tensor wrap_conv1d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::conv1d(dispatchKeySet, input, weight, bias, std::move(stride), std::move(padding), std::move(dilation), groups);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CONV1D, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_conv2d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::conv2d(dispatchKeySet, input, weight, bias, std::move(stride), std::move(padding), std::move(dilation), groups);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CONV2D, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_conv3d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::conv3d(dispatchKeySet, input, weight, bias, std::move(stride), std::move(padding), std::move(dilation), groups);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CONV3D, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_conv1d_padding(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::conv1d(dispatchKeySet, input, weight, bias, std::move(stride), std::move(padding), std::move(dilation), groups);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CONV1D_PADDING, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_conv2d_padding(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::conv2d(dispatchKeySet, input, weight, bias, std::move(stride), std::move(padding), std::move(dilation), groups);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CONV2D_PADDING, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_conv3d_padding(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::conv3d(dispatchKeySet, input, weight, bias, std::move(stride), std::move(padding), std::move(dilation), groups);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CONV3D_PADDING, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_conv_tbc(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const at::Tensor & bias, int64_t pad) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::conv_tbc(dispatchKeySet, self, weight, bias, pad);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CONV_TBC, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, pad);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_conv_tbc_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & input, const at::Tensor & weight, const at::Tensor & bias, int64_t pad) {
  ensure_materialized(self);ensure_materialized(input);ensure_materialized(weight);ensure_materialized(bias);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::conv_tbc_backward(dispatchKeySet, self, input, weight, bias, pad);
}

at::Tensor wrap_conv_transpose1d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, int64_t groups, at::IntArrayRef dilation) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::conv_transpose1d(dispatchKeySet, input, weight, bias, std::move(stride), std::move(padding), std::move(output_padding), groups, std::move(dilation));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CONV_TRANSPOSE1D, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(output_padding));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, std::move(dilation));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_conv_transpose2d_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, int64_t groups, at::IntArrayRef dilation) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::conv_transpose2d(dispatchKeySet, input, weight, bias, std::move(stride), std::move(padding), std::move(output_padding), groups, std::move(dilation));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CONV_TRANSPOSE2D_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(output_padding));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, std::move(dilation));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_conv_transpose3d_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, int64_t groups, at::IntArrayRef dilation) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::conv_transpose3d(dispatchKeySet, input, weight, bias, std::move(stride), std::move(padding), std::move(output_padding), groups, std::move(dilation));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CONV_TRANSPOSE3D_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(output_padding));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, std::move(dilation));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_copy_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & src, bool non_blocking) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::copy_(dispatchKeySet, self, src, non_blocking);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_COPY_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, src);trace.append_arg(trace_idx, non_blocking);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_cos_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cos_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_COS_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_cosh_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cosh_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_COSH_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_cosine_embedding_loss(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & target, double margin, int64_t reduction) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cosine_embedding_loss(dispatchKeySet, input1, input2, target, margin, reduction);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input1.dtype(), input1.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_COSINE_EMBEDDING_LOSS, dispatchKeySet);
  trace.append_arg(trace_idx, input1);trace.append_arg(trace_idx, input2);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, margin);trace.append_arg(trace_idx, reduction);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_count_nonzero_dim_IntList(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::count_nonzero(dispatchKeySet, self, std::move(dim));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_COUNT_NONZERO_DIM_INTLIST, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_count_nonzero(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::count_nonzero(dispatchKeySet, self, dim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_COUNT_NONZERO, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_cudnn_affine_grid_generator(c10::DispatchKeySet dispatchKeySet, const at::Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cudnn_affine_grid_generator(dispatchKeySet, theta, N, C, H, W);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(theta.dtype(), theta.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CUDNN_AFFINE_GRID_GENERATOR, dispatchKeySet);
  trace.append_arg(trace_idx, theta);trace.append_arg(trace_idx, N);trace.append_arg(trace_idx, C);trace.append_arg(trace_idx, H);trace.append_arg(trace_idx, W);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_cudnn_affine_grid_generator_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cudnn_affine_grid_generator_backward(dispatchKeySet, grad, N, C, H, W);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CUDNN_AFFINE_GRID_GENERATOR_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, N);trace.append_arg(trace_idx, C);trace.append_arg(trace_idx, H);trace.append_arg(trace_idx, W);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrap_cudnn_batch_norm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double exponential_average_factor, double epsilon) {
  ensure_materialized(input);ensure_materialized(weight);ensure_materialized(bias);ensure_materialized(running_mean);ensure_materialized(running_var);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::cudnn_batch_norm(dispatchKeySet, input, weight, bias, running_mean, running_var, training, exponential_average_factor, epsilon);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_cudnn_batch_norm_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grad_output, const at::Tensor & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_var, double epsilon, const at::Tensor & reserveSpace) {
  ensure_materialized(input);ensure_materialized(grad_output);ensure_materialized(weight);ensure_materialized(running_mean);ensure_materialized(running_var);ensure_materialized(save_mean);ensure_materialized(save_var);ensure_materialized(reserveSpace);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::cudnn_batch_norm_backward(dispatchKeySet, input, grad_output, weight, running_mean, running_var, save_mean, save_var, epsilon, reserveSpace);
}

at::Tensor wrap_cudnn_convolution_deprecated(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cudnn_convolution(dispatchKeySet, self, weight, bias, std::move(padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CUDNN_CONVOLUTION_DEPRECATED, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, benchmark);trace.append_arg(trace_idx, deterministic);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_cudnn_convolution_deprecated2(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cudnn_convolution(dispatchKeySet, self, weight, std::move(padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CUDNN_CONVOLUTION_DEPRECATED2, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, benchmark);trace.append_arg(trace_idx, deterministic);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_cudnn_convolution(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cudnn_convolution(dispatchKeySet, self, weight, std::move(padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic, allow_tf32);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CUDNN_CONVOLUTION, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, benchmark);trace.append_arg(trace_idx, deterministic);trace.append_arg(trace_idx, allow_tf32);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_cudnn_convolution_backward_input(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef self_size, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cudnn_convolution_backward_input(dispatchKeySet, std::move(self_size), grad_output, weight, std::move(padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic, allow_tf32);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CUDNN_CONVOLUTION_BACKWARD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(self_size));trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, benchmark);trace.append_arg(trace_idx, deterministic);trace.append_arg(trace_idx, allow_tf32);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap_cudnn_convolution_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32, std::array<bool,2> output_mask) {
  ensure_materialized(self);ensure_materialized(grad_output);ensure_materialized(weight);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::cudnn_convolution_backward(dispatchKeySet, self, grad_output, weight, std::move(padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic, allow_tf32, std::move(output_mask));
}

at::Tensor wrap_cudnn_convolution_backward_weight(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cudnn_convolution_backward_weight(dispatchKeySet, std::move(weight_size), grad_output, self, std::move(padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic, allow_tf32);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CUDNN_CONVOLUTION_BACKWARD_WEIGHT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(weight_size));trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, benchmark);trace.append_arg(trace_idx, deterministic);trace.append_arg(trace_idx, allow_tf32);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_cudnn_convolution_transpose_deprecated(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cudnn_convolution_transpose(dispatchKeySet, self, weight, bias, std::move(padding), std::move(output_padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CUDNN_CONVOLUTION_TRANSPOSE_DEPRECATED, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(output_padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, benchmark);trace.append_arg(trace_idx, deterministic);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_cudnn_convolution_transpose_deprecated2(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cudnn_convolution_transpose(dispatchKeySet, self, weight, std::move(padding), std::move(output_padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CUDNN_CONVOLUTION_TRANSPOSE_DEPRECATED2, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(output_padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, benchmark);trace.append_arg(trace_idx, deterministic);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_cudnn_convolution_transpose(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cudnn_convolution_transpose(dispatchKeySet, self, weight, std::move(padding), std::move(output_padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic, allow_tf32);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CUDNN_CONVOLUTION_TRANSPOSE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(output_padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, benchmark);trace.append_arg(trace_idx, deterministic);trace.append_arg(trace_idx, allow_tf32);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap_cudnn_convolution_transpose_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32, std::array<bool,2> output_mask) {
  ensure_materialized(self);ensure_materialized(grad_output);ensure_materialized(weight);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::cudnn_convolution_transpose_backward(dispatchKeySet, self, grad_output, weight, std::move(padding), std::move(output_padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic, allow_tf32, std::move(output_mask));
}

at::Tensor wrap_cudnn_convolution_transpose_backward_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cudnn_convolution_transpose_backward_input(dispatchKeySet, grad_output, weight, std::move(padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic, allow_tf32);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CUDNN_CONVOLUTION_TRANSPOSE_BACKWARD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, benchmark);trace.append_arg(trace_idx, deterministic);trace.append_arg(trace_idx, allow_tf32);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_cudnn_convolution_transpose_backward_weight(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cudnn_convolution_transpose_backward_weight(dispatchKeySet, std::move(weight_size), grad_output, self, std::move(padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic, allow_tf32);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CUDNN_CONVOLUTION_TRANSPOSE_BACKWARD_WEIGHT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(weight_size));trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, benchmark);trace.append_arg(trace_idx, deterministic);trace.append_arg(trace_idx, allow_tf32);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_cudnn_convolution_relu(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cudnn_convolution_relu(dispatchKeySet, self, weight, bias, std::move(stride), std::move(padding), std::move(dilation), groups);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CUDNN_CONVOLUTION_RELU, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_cudnn_convolution_add_relu(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const at::Tensor & z, const c10::optional<at::Scalar> & alpha, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cudnn_convolution_add_relu(dispatchKeySet, self, weight, z, alpha, bias, std::move(stride), std::move(padding), std::move(dilation), groups);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CUDNN_CONVOLUTION_ADD_RELU, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, z);trace.append_arg(trace_idx, alpha);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_cudnn_grid_sampler(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grid) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cudnn_grid_sampler(dispatchKeySet, self, grid);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CUDNN_GRID_SAMPLER, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, grid);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap_cudnn_grid_sampler_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grid, const at::Tensor & grad_output) {
  ensure_materialized(self);ensure_materialized(grid);ensure_materialized(grad_output);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::cudnn_grid_sampler_backward(dispatchKeySet, self, grid, grad_output);
}

std::tuple<at::Tensor,at::Tensor> wrap_cummax(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::cummax(dispatchKeySet, self, dim);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_cummax_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & values, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(values);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::cummax_outf(dispatchKeySet, self, dim, values, indices);
}

std::tuple<at::Tensor,at::Tensor> wrap_cummax_dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::cummax(dispatchKeySet, self, std::move(dim));
}

std::tuple<at::Tensor &,at::Tensor &> wrap_cummax_dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, at::Tensor & values, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(values);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::cummax_outf(dispatchKeySet, self, std::move(dim), values, indices);
}

void wrap__cummax_helper(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & values, at::Tensor & indices, int64_t dim) {
  ensure_materialized(self);ensure_materialized(values);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_cummax_helper(dispatchKeySet, self, values, indices, dim);
}

std::tuple<at::Tensor,at::Tensor> wrap_cummin(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::cummin(dispatchKeySet, self, dim);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_cummin_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & values, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(values);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::cummin_outf(dispatchKeySet, self, dim, values, indices);
}

std::tuple<at::Tensor,at::Tensor> wrap_cummin_dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::cummin(dispatchKeySet, self, std::move(dim));
}

std::tuple<at::Tensor &,at::Tensor &> wrap_cummin_dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, at::Tensor & values, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(values);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::cummin_outf(dispatchKeySet, self, std::move(dim), values, indices);
}

void wrap__cummin_helper(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & values, at::Tensor & indices, int64_t dim) {
  ensure_materialized(self);ensure_materialized(values);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_cummin_helper(dispatchKeySet, self, values, indices, dim);
}

at::Tensor wrap_cummaxmin_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & input, const at::Tensor & indices, int64_t dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cummaxmin_backward(dispatchKeySet, grad, input, indices, dim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CUMMAXMIN_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_cumprod(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cumprod(dispatchKeySet, self, dim, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CUMPROD, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_cumprod_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cumprod_(dispatchKeySet, self, dim, std::move(dtype));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CUMPROD_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(dtype));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_cumprod_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cumprod_outf(dispatchKeySet, self, dim, std::move(dtype), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CUMPROD_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_cumprod_dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cumprod(dispatchKeySet, self, std::move(dim), std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CUMPROD_DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_cumprod__dimname(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cumprod_(dispatchKeySet, self, std::move(dim), std::move(dtype));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CUMPROD__DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(dtype));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_cumprod_dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cumprod_outf(dispatchKeySet, self, std::move(dim), std::move(dtype), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CUMPROD_DIMNAME_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_cumprod_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & input, int64_t dim, const at::Tensor & output) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cumprod_backward(dispatchKeySet, grad, input, dim, output);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CUMPROD_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, output);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_cumsum(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cumsum(dispatchKeySet, self, dim, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CUMSUM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_cumsum_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cumsum_(dispatchKeySet, self, dim, std::move(dtype));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CUMSUM_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(dtype));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_cumsum_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cumsum_outf(dispatchKeySet, self, dim, std::move(dtype), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CUMSUM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_cumsum_dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cumsum(dispatchKeySet, self, std::move(dim), std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CUMSUM_DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_cumsum__dimname(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cumsum_(dispatchKeySet, self, std::move(dim), std::move(dtype));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CUMSUM__DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(dtype));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_cumsum_dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cumsum_outf(dispatchKeySet, self, std::move(dim), std::move(dtype), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CUMSUM_DIMNAME_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_ctc_loss_IntList(c10::DispatchKeySet dispatchKeySet, const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, int64_t reduction, bool zero_infinity) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ctc_loss(dispatchKeySet, log_probs, targets, std::move(input_lengths), std::move(target_lengths), blank, reduction, zero_infinity);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(log_probs.dtype(), log_probs.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CTC_LOSS_INTLIST, dispatchKeySet);
  trace.append_arg(trace_idx, log_probs);trace.append_arg(trace_idx, targets);trace.append_arg(trace_idx, std::move(input_lengths));trace.append_arg(trace_idx, std::move(target_lengths));trace.append_arg(trace_idx, blank);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, zero_infinity);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_ctc_loss_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & log_probs, const at::Tensor & targets, const at::Tensor & input_lengths, const at::Tensor & target_lengths, int64_t blank, int64_t reduction, bool zero_infinity) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ctc_loss(dispatchKeySet, log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(log_probs.dtype(), log_probs.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CTC_LOSS_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, log_probs);trace.append_arg(trace_idx, targets);trace.append_arg(trace_idx, input_lengths);trace.append_arg(trace_idx, target_lengths);trace.append_arg(trace_idx, blank);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, zero_infinity);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap__ctc_loss(c10::DispatchKeySet dispatchKeySet, const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, bool zero_infinity) {
  ensure_materialized(log_probs);ensure_materialized(targets);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_ctc_loss(dispatchKeySet, log_probs, targets, std::move(input_lengths), std::move(target_lengths), blank, zero_infinity);
}

at::Tensor wrap__ctc_loss_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, const at::Tensor & neg_log_likelihood, const at::Tensor & log_alpha, int64_t blank, bool zero_infinity) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_ctc_loss_backward(dispatchKeySet, grad, log_probs, targets, std::move(input_lengths), std::move(target_lengths), neg_log_likelihood, log_alpha, blank, zero_infinity);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__CTC_LOSS_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, log_probs);trace.append_arg(trace_idx, targets);trace.append_arg(trace_idx, std::move(input_lengths));trace.append_arg(trace_idx, std::move(target_lengths));trace.append_arg(trace_idx, neg_log_likelihood);trace.append_arg(trace_idx, log_alpha);trace.append_arg(trace_idx, blank);trace.append_arg(trace_idx, zero_infinity);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_diag_embed(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::diag_embed(dispatchKeySet, self, offset, dim1, dim2);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DIAG_EMBED, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, offset);trace.append_arg(trace_idx, dim1);trace.append_arg(trace_idx, dim2);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_diagflat(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t offset) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::diagflat(dispatchKeySet, self, offset);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DIAGFLAT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, offset);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_diagonal(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::diagonal(dispatchKeySet, self, offset, dim1, dim2);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DIAGONAL, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, offset);trace.append_arg(trace_idx, dim1);trace.append_arg(trace_idx, dim2);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_diagonal_Dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname outdim, at::Dimname dim1, at::Dimname dim2, int64_t offset) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::diagonal(dispatchKeySet, self, std::move(outdim), std::move(dim1), std::move(dim2), offset);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DIAGONAL_DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(outdim));trace.append_arg(trace_idx, std::move(dim1));trace.append_arg(trace_idx, std::move(dim2));trace.append_arg(trace_idx, offset);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_diagonal_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, at::IntArrayRef input_sizes, int64_t offset, int64_t dim1, int64_t dim2) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::diagonal_backward(dispatchKeySet, grad, std::move(input_sizes), offset, dim1, dim2);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DIAGONAL_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, std::move(input_sizes));trace.append_arg(trace_idx, offset);trace.append_arg(trace_idx, dim1);trace.append_arg(trace_idx, dim2);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fill_diagonal_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & fill_value, bool wrap) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fill_diagonal_(dispatchKeySet, self, fill_value, wrap);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FILL_DIAGONAL_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, fill_value);trace.append_arg(trace_idx, wrap);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_diff(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t n, int64_t dim, const c10::optional<at::Tensor> & prepend, const c10::optional<at::Tensor> & append) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::diff(dispatchKeySet, self, n, dim, prepend, append);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DIFF, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, prepend);trace.append_arg(trace_idx, append);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_diff_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t n, int64_t dim, const c10::optional<at::Tensor> & prepend, const c10::optional<at::Tensor> & append, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::diff_outf(dispatchKeySet, self, n, dim, prepend, append, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_DIFF_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, prepend);trace.append_arg(trace_idx, append);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

std::vector<at::Tensor> wrap_gradient_scalarint(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & spacing, c10::optional<int64_t> dim, int64_t edge_order) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::gradient(dispatchKeySet, self, spacing, dim, edge_order);
}

std::vector<at::Tensor> wrap_gradient_scalararray(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & spacing, at::IntArrayRef dim, int64_t edge_order) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::gradient(dispatchKeySet, self, spacing, std::move(dim), edge_order);
}

std::vector<at::Tensor> wrap_gradient_array(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, int64_t edge_order) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::gradient(dispatchKeySet, self, std::move(dim), edge_order);
}

std::vector<at::Tensor> wrap_gradient_scalarrayint(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::ArrayRef<at::Scalar> spacing, c10::optional<int64_t> dim, int64_t edge_order) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::gradient(dispatchKeySet, self, spacing, dim, edge_order);
}

std::vector<at::Tensor> wrap_gradient_scalarrayarray(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::ArrayRef<at::Scalar> spacing, at::IntArrayRef dim, int64_t edge_order) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::gradient(dispatchKeySet, self, spacing, std::move(dim), edge_order);
}

std::vector<at::Tensor> wrap_gradient_tensorarrayint(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::TensorList spacing, c10::optional<int64_t> dim, int64_t edge_order) {
  ensure_materialized(self);ensure_materialized(spacing);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::gradient(dispatchKeySet, self, std::move(spacing), dim, edge_order);
}

std::vector<at::Tensor> wrap_gradient_tensorarray(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::TensorList spacing, at::IntArrayRef dim, int64_t edge_order) {
  ensure_materialized(self);ensure_materialized(spacing);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::gradient(dispatchKeySet, self, std::move(spacing), std::move(dim), edge_order);
}

at::Tensor wrap_div_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::div(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DIV_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_div__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::div_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_DIV__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_div_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::div_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_DIV_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_div_Tensor_mode(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::div(dispatchKeySet, self, other, std::move(rounding_mode));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DIV_TENSOR_MODE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, std::move(rounding_mode));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_div__Tensor_mode(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::div_(dispatchKeySet, self, other, std::move(rounding_mode));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_DIV__TENSOR_MODE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, std::move(rounding_mode));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_div_out_mode(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::div_outf(dispatchKeySet, self, other, std::move(rounding_mode), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_DIV_OUT_MODE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, std::move(rounding_mode));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_div_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::div(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DIV_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_div__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::div_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_DIV__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_div_Scalar_mode(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::div(dispatchKeySet, self, other, std::move(rounding_mode));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DIV_SCALAR_MODE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, std::move(rounding_mode));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_div__Scalar_mode(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::div_(dispatchKeySet, self, other, std::move(rounding_mode));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_DIV__SCALAR_MODE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, std::move(rounding_mode));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_divide_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::divide(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DIVIDE_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_divide__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::divide_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_DIVIDE__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_divide_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::divide_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_DIVIDE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_divide_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::divide(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DIVIDE_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_divide__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::divide_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_DIVIDE__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_divide_Tensor_mode(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::divide(dispatchKeySet, self, other, std::move(rounding_mode));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DIVIDE_TENSOR_MODE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, std::move(rounding_mode));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_divide__Tensor_mode(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::divide_(dispatchKeySet, self, other, std::move(rounding_mode));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_DIVIDE__TENSOR_MODE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, std::move(rounding_mode));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_divide_out_mode(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::divide_outf(dispatchKeySet, self, other, std::move(rounding_mode), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_DIVIDE_OUT_MODE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, std::move(rounding_mode));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_divide_Scalar_mode(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::divide(dispatchKeySet, self, other, std::move(rounding_mode));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DIVIDE_SCALAR_MODE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, std::move(rounding_mode));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_divide__Scalar_mode(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::divide_(dispatchKeySet, self, other, std::move(rounding_mode));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_DIVIDE__SCALAR_MODE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, std::move(rounding_mode));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_true_divide_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::true_divide(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TRUE_DIVIDE_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_true_divide__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::true_divide_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_TRUE_DIVIDE__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_true_divide_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::true_divide_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_TRUE_DIVIDE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_true_divide_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::true_divide(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TRUE_DIVIDE_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_true_divide__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::true_divide_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_TRUE_DIVIDE__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_dot(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & tensor) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::dot(dispatchKeySet, self, tensor);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DOT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, tensor);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_dot_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & tensor, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::dot_outf(dispatchKeySet, self, tensor, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_DOT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, tensor);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_vdot(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::vdot(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_VDOT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_vdot_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::vdot_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_VDOT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_einsum(c10::DispatchKeySet dispatchKeySet, c10::string_view equation, at::TensorList tensors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::einsum(dispatchKeySet, std::move(equation), std::move(tensors));
  }
  auto defaults = compute_dtype(tensors);
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(default_dtype, default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_EINSUM, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(equation));trace.append_arg(trace_idx, std::move(tensors));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_embedding(c10::DispatchKeySet dispatchKeySet, const at::Tensor & weight, const at::Tensor & indices, int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::embedding(dispatchKeySet, weight, indices, padding_idx, scale_grad_by_freq, sparse);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(weight.dtype(), weight.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_EMBEDDING, dispatchKeySet);
  trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, padding_idx);trace.append_arg(trace_idx, scale_grad_by_freq);trace.append_arg(trace_idx, sparse);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_embedding_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::embedding_backward(dispatchKeySet, grad, indices, num_weights, padding_idx, scale_grad_by_freq, sparse);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_EMBEDDING_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, num_weights);trace.append_arg(trace_idx, padding_idx);trace.append_arg(trace_idx, scale_grad_by_freq);trace.append_arg(trace_idx, sparse);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_embedding_dense_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::embedding_dense_backward(dispatchKeySet, grad_output, indices, num_weights, padding_idx, scale_grad_by_freq);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_EMBEDDING_DENSE_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, num_weights);trace.append_arg(trace_idx, padding_idx);trace.append_arg(trace_idx, scale_grad_by_freq);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_embedding_renorm_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & indices, double max_norm, double norm_type) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::embedding_renorm_(dispatchKeySet, self, indices, max_norm, norm_type);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_EMBEDDING_RENORM_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, max_norm);trace.append_arg(trace_idx, norm_type);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_embedding_sparse_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::embedding_sparse_backward(dispatchKeySet, grad, indices, num_weights, padding_idx, scale_grad_by_freq);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_EMBEDDING_SPARSE_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, num_weights);trace.append_arg(trace_idx, padding_idx);trace.append_arg(trace_idx, scale_grad_by_freq);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrap__embedding_bag_forward_only(c10::DispatchKeySet dispatchKeySet, const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, int64_t padding_idx) {
  ensure_materialized(weight);ensure_materialized(indices);ensure_materialized(offsets);ensure_materialized(per_sample_weights);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_embedding_bag_forward_only(dispatchKeySet, weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx);
}

std::tuple<at::Tensor,at::Tensor> wrap__rowwise_prune(c10::DispatchKeySet dispatchKeySet, const at::Tensor & weight, const at::Tensor & mask, at::ScalarType compressed_indices_dtype) {
  ensure_materialized(weight);ensure_materialized(mask);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_rowwise_prune(dispatchKeySet, weight, mask, std::move(compressed_indices_dtype));
}

at::Tensor wrap_row_stack(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::row_stack(dispatchKeySet, std::move(tensors));
  }
  auto defaults = compute_dtype(tensors);
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(default_dtype, default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ROW_STACK, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_row_stack_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::row_stack_outf(dispatchKeySet, std::move(tensors), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ROW_STACK_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrap_embedding_bag(c10::DispatchKeySet dispatchKeySet, const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset) {
  ensure_materialized(weight);ensure_materialized(indices);ensure_materialized(offsets);ensure_materialized(per_sample_weights);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::embedding_bag(dispatchKeySet, weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrap_embedding_bag_padding_idx(c10::DispatchKeySet dispatchKeySet, const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, c10::optional<int64_t> padding_idx) {
  ensure_materialized(weight);ensure_materialized(indices);ensure_materialized(offsets);ensure_materialized(per_sample_weights);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::embedding_bag(dispatchKeySet, weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrap__embedding_bag(c10::DispatchKeySet dispatchKeySet, const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, int64_t padding_idx) {
  ensure_materialized(weight);ensure_materialized(indices);ensure_materialized(offsets);ensure_materialized(per_sample_weights);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_embedding_bag(dispatchKeySet, weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx);
}

at::Tensor wrap__embedding_bag_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offsets, const at::Tensor & offset2bag, const at::Tensor & bag_size, const at::Tensor & maximum_indices, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, int64_t padding_idx) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_embedding_bag_backward(dispatchKeySet, grad, indices, offsets, offset2bag, bag_size, maximum_indices, num_weights, scale_grad_by_freq, mode, sparse, per_sample_weights, padding_idx);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__EMBEDDING_BAG_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, offsets);trace.append_arg(trace_idx, offset2bag);trace.append_arg(trace_idx, bag_size);trace.append_arg(trace_idx, maximum_indices);trace.append_arg(trace_idx, num_weights);trace.append_arg(trace_idx, scale_grad_by_freq);trace.append_arg(trace_idx, mode);trace.append_arg(trace_idx, sparse);trace.append_arg(trace_idx, per_sample_weights);trace.append_arg(trace_idx, padding_idx);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__embedding_bag_sparse_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offsets, const at::Tensor & offset2bag, const at::Tensor & bag_size, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, const c10::optional<at::Tensor> & per_sample_weights, int64_t padding_idx) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_embedding_bag_sparse_backward(dispatchKeySet, grad, indices, offsets, offset2bag, bag_size, num_weights, scale_grad_by_freq, mode, per_sample_weights, padding_idx);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__EMBEDDING_BAG_SPARSE_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, offsets);trace.append_arg(trace_idx, offset2bag);trace.append_arg(trace_idx, bag_size);trace.append_arg(trace_idx, num_weights);trace.append_arg(trace_idx, scale_grad_by_freq);trace.append_arg(trace_idx, mode);trace.append_arg(trace_idx, per_sample_weights);trace.append_arg(trace_idx, padding_idx);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__embedding_bag_dense_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offset2bag, const at::Tensor & bag_size, const at::Tensor & maximum_indices, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, const c10::optional<at::Tensor> & per_sample_weights, int64_t padding_idx) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_embedding_bag_dense_backward(dispatchKeySet, grad, indices, offset2bag, bag_size, maximum_indices, num_weights, scale_grad_by_freq, mode, per_sample_weights, padding_idx);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__EMBEDDING_BAG_DENSE_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, offset2bag);trace.append_arg(trace_idx, bag_size);trace.append_arg(trace_idx, maximum_indices);trace.append_arg(trace_idx, num_weights);trace.append_arg(trace_idx, scale_grad_by_freq);trace.append_arg(trace_idx, mode);trace.append_arg(trace_idx, per_sample_weights);trace.append_arg(trace_idx, padding_idx);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__embedding_bag_per_sample_weights_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, const at::Tensor & offset2bag, int64_t mode, int64_t padding_idx) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_embedding_bag_per_sample_weights_backward(dispatchKeySet, grad, weight, indices, offsets, offset2bag, mode, padding_idx);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__EMBEDDING_BAG_PER_SAMPLE_WEIGHTS_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, offsets);trace.append_arg(trace_idx, offset2bag);trace.append_arg(trace_idx, mode);trace.append_arg(trace_idx, padding_idx);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_empty_names(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::empty(dispatchKeySet, std::move(size), std::move(names), std::move(dtype), std::move(layout), std::move(device), pin_memory, std::move(memory_format));
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_EMPTY_NAMES, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(names));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);trace.append_arg(trace_idx, std::move(memory_format));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_empty_memory_format(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::empty(dispatchKeySet, std::move(size), std::move(dtype), std::move(layout), std::move(device), pin_memory, std::move(memory_format));
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_EMPTY_MEMORY_FORMAT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);trace.append_arg(trace_idx, std::move(memory_format));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_new_empty(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::new_empty(dispatchKeySet, self, std::move(size), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), device ? *device : self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NEW_EMPTY, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_new_empty_strided(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, at::IntArrayRef stride, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::new_empty_strided(dispatchKeySet, self, std::move(size), std::move(stride), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), device ? *device : self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NEW_EMPTY_STRIDED, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_new_full(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, const at::Scalar & fill_value, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::new_full(dispatchKeySet, self, std::move(size), fill_value, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), device ? *device : self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NEW_FULL, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, fill_value);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_new_zeros(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::new_zeros(dispatchKeySet, self, std::move(size), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), device ? *device : self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NEW_ZEROS, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_new_ones(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::new_ones(dispatchKeySet, self, std::move(size), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), device ? *device : self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NEW_ONES, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__empty_affine_quantized(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, double scale, int64_t zero_point, c10::optional<at::MemoryFormat> memory_format) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_empty_affine_quantized(dispatchKeySet, std::move(size), std::move(dtype), std::move(layout), std::move(device), pin_memory, scale, zero_point, std::move(memory_format));
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__EMPTY_AFFINE_QUANTIZED, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);trace.append_arg(trace_idx, scale);trace.append_arg(trace_idx, zero_point);trace.append_arg(trace_idx, std::move(memory_format));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__empty_per_channel_affine_quantized(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_empty_per_channel_affine_quantized(dispatchKeySet, std::move(size), scales, zero_points, axis, std::move(dtype), std::move(layout), std::move(device), pin_memory, std::move(memory_format));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : scales.dtype(), device ? *device : scales.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__EMPTY_PER_CHANNEL_AFFINE_QUANTIZED, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, scales);trace.append_arg(trace_idx, zero_points);trace.append_arg(trace_idx, axis);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);trace.append_arg(trace_idx, std::move(memory_format));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

const at::Tensor & wrap_resize_(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, c10::optional<at::MemoryFormat> memory_format) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::resize_(dispatchKeySet, self, std::move(size), std::move(memory_format));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RESIZE_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(memory_format));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_empty_quantized(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, const at::Tensor & qtensor) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::empty_quantized(dispatchKeySet, std::move(size), qtensor);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(qtensor.dtype(), qtensor.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_EMPTY_QUANTIZED, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, qtensor);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_empty_out(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::empty_outf(dispatchKeySet, std::move(size), std::move(memory_format), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_EMPTY_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(memory_format));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_empty_like(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::empty_like(dispatchKeySet, self, std::move(dtype), std::move(layout), std::move(device), pin_memory, std::move(memory_format));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), device ? *device : self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_EMPTY_LIKE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);trace.append_arg(trace_idx, std::move(memory_format));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_empty_strided(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, at::IntArrayRef stride, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::empty_strided(dispatchKeySet, std::move(size), std::move(stride), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_EMPTY_STRIDED, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_erf_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::erf_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ERF_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_erfc_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::erfc_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ERFC_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_exp_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::exp_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_EXP_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_exp2_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::exp2_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_EXP2_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_expm1_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::expm1_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_EXPM1_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_expand(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, bool implicit) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::expand(dispatchKeySet, self, std::move(size), implicit);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_EXPAND, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, implicit);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_expand_as(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::expand_as(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_EXPAND_AS, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_eye(c10::DispatchKeySet dispatchKeySet, int64_t n, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::eye(dispatchKeySet, n, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_EYE, dispatchKeySet);
  trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_eye_m(c10::DispatchKeySet dispatchKeySet, int64_t n, int64_t m, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::eye(dispatchKeySet, n, m, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_EYE_M, dispatchKeySet);
  trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, m);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_eye_out(c10::DispatchKeySet dispatchKeySet, int64_t n, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::eye_outf(dispatchKeySet, n, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_EYE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_eye_m_out(c10::DispatchKeySet dispatchKeySet, int64_t n, int64_t m, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::eye_outf(dispatchKeySet, n, m, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_EYE_M_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, m);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_flatten_using_ints(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t start_dim, int64_t end_dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::flatten(dispatchKeySet, self, start_dim, end_dim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FLATTEN_USING_INTS, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, start_dim);trace.append_arg(trace_idx, end_dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_flatten_named_out_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t start_dim, int64_t end_dim, at::Dimname out_dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::flatten(dispatchKeySet, self, start_dim, end_dim, std::move(out_dim));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FLATTEN_NAMED_OUT_DIM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, start_dim);trace.append_arg(trace_idx, end_dim);trace.append_arg(trace_idx, std::move(out_dim));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_flatten_using_names(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname start_dim, at::Dimname end_dim, at::Dimname out_dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::flatten(dispatchKeySet, self, std::move(start_dim), std::move(end_dim), std::move(out_dim));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FLATTEN_USING_NAMES, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(start_dim));trace.append_arg(trace_idx, std::move(end_dim));trace.append_arg(trace_idx, std::move(out_dim));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_flatten_DimnameList(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dims, at::Dimname out_dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::flatten(dispatchKeySet, self, std::move(dims), std::move(out_dim));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FLATTEN_DIMNAMELIST, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dims));trace.append_arg(trace_idx, std::move(out_dim));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_unflatten_int(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::IntArrayRef sizes, c10::optional<at::DimnameList> names) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::unflatten(dispatchKeySet, self, dim, std::move(sizes), std::move(names));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_UNFLATTEN_INT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(sizes));trace.append_arg(trace_idx, std::move(names));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_unflatten_Dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, at::IntArrayRef sizes, at::DimnameList names) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::unflatten(dispatchKeySet, self, std::move(dim), std::move(sizes), std::move(names));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_UNFLATTEN_DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(sizes));trace.append_arg(trace_idx, std::move(names));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fill__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fill_(dispatchKeySet, self, value);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FILL__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, value);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_fill__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fill_(dispatchKeySet, self, value);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FILL__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, value);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_floor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::floor(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FLOOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_floor_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::floor_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FLOOR_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_floor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::floor_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FLOOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_floor_divide(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::floor_divide(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FLOOR_DIVIDE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_floor_divide__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::floor_divide_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FLOOR_DIVIDE__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_floor_divide_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::floor_divide_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FLOOR_DIVIDE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_floor_divide_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::floor_divide(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FLOOR_DIVIDE_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_floor_divide__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::floor_divide_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FLOOR_DIVIDE__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_frac_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::frac_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FRAC_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_full_names(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, const at::Scalar & fill_value, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::full(dispatchKeySet, std::move(size), fill_value, std::move(names), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FULL_NAMES, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, fill_value);trace.append_arg(trace_idx, std::move(names));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_full(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, const at::Scalar & fill_value, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::full(dispatchKeySet, std::move(size), fill_value, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FULL, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, fill_value);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_full_out(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, const at::Scalar & fill_value, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::full_outf(dispatchKeySet, std::move(size), fill_value, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FULL_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, fill_value);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_full_like(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & fill_value, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::full_like(dispatchKeySet, self, fill_value, std::move(dtype), std::move(layout), std::move(device), pin_memory, std::move(memory_format));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), device ? *device : self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FULL_LIKE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, fill_value);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);trace.append_arg(trace_idx, std::move(memory_format));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_from_file(c10::DispatchKeySet dispatchKeySet, c10::string_view filename, c10::optional<bool> shared, c10::optional<int64_t> size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::from_file(dispatchKeySet, std::move(filename), shared, size, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FROM_FILE, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(filename));trace.append_arg(trace_idx, shared);trace.append_arg(trace_idx, size);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_gcd_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::gcd_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GCD_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_lcm_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::lcm_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LCM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_grid_sampler(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::grid_sampler(dispatchKeySet, input, grid, interpolation_mode, padding_mode, align_corners);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_GRID_SAMPLER, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, grid);trace.append_arg(trace_idx, interpolation_mode);trace.append_arg(trace_idx, padding_mode);trace.append_arg(trace_idx, align_corners);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_grid_sampler_2d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::grid_sampler_2d(dispatchKeySet, input, grid, interpolation_mode, padding_mode, align_corners);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_GRID_SAMPLER_2D, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, grid);trace.append_arg(trace_idx, interpolation_mode);trace.append_arg(trace_idx, padding_mode);trace.append_arg(trace_idx, align_corners);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap_grid_sampler_2d_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
  ensure_materialized(grad_output);ensure_materialized(input);ensure_materialized(grid);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::grid_sampler_2d_backward(dispatchKeySet, grad_output, input, grid, interpolation_mode, padding_mode, align_corners);
}

at::Tensor wrap__grid_sampler_2d_cpu_fallback(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_grid_sampler_2d_cpu_fallback(dispatchKeySet, input, grid, interpolation_mode, padding_mode, align_corners);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__GRID_SAMPLER_2D_CPU_FALLBACK, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, grid);trace.append_arg(trace_idx, interpolation_mode);trace.append_arg(trace_idx, padding_mode);trace.append_arg(trace_idx, align_corners);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap__grid_sampler_2d_cpu_fallback_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
  ensure_materialized(grad_output);ensure_materialized(input);ensure_materialized(grid);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_grid_sampler_2d_cpu_fallback_backward(dispatchKeySet, grad_output, input, grid, interpolation_mode, padding_mode, align_corners);
}

at::Tensor wrap_grid_sampler_3d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::grid_sampler_3d(dispatchKeySet, input, grid, interpolation_mode, padding_mode, align_corners);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_GRID_SAMPLER_3D, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, grid);trace.append_arg(trace_idx, interpolation_mode);trace.append_arg(trace_idx, padding_mode);trace.append_arg(trace_idx, align_corners);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap_grid_sampler_3d_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
  ensure_materialized(grad_output);ensure_materialized(input);ensure_materialized(grid);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::grid_sampler_3d_backward(dispatchKeySet, grad_output, input, grid, interpolation_mode, padding_mode, align_corners);
}

at::Tensor wrap_hann_window(c10::DispatchKeySet dispatchKeySet, int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hann_window(dispatchKeySet, window_length, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_HANN_WINDOW, dispatchKeySet);
  trace.append_arg(trace_idx, window_length);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_hann_window_periodic(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hann_window(dispatchKeySet, window_length, periodic, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_HANN_WINDOW_PERIODIC, dispatchKeySet);
  trace.append_arg(trace_idx, window_length);trace.append_arg(trace_idx, periodic);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_hamming_window(c10::DispatchKeySet dispatchKeySet, int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hamming_window(dispatchKeySet, window_length, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_HAMMING_WINDOW, dispatchKeySet);
  trace.append_arg(trace_idx, window_length);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_hamming_window_periodic(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hamming_window(dispatchKeySet, window_length, periodic, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_HAMMING_WINDOW_PERIODIC, dispatchKeySet);
  trace.append_arg(trace_idx, window_length);trace.append_arg(trace_idx, periodic);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_hamming_window_periodic_alpha(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, double alpha, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hamming_window(dispatchKeySet, window_length, periodic, alpha, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_HAMMING_WINDOW_PERIODIC_ALPHA, dispatchKeySet);
  trace.append_arg(trace_idx, window_length);trace.append_arg(trace_idx, periodic);trace.append_arg(trace_idx, alpha);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_hamming_window_periodic_alpha_beta(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, double alpha, double beta, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hamming_window(dispatchKeySet, window_length, periodic, alpha, beta, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_HAMMING_WINDOW_PERIODIC_ALPHA_BETA, dispatchKeySet);
  trace.append_arg(trace_idx, window_length);trace.append_arg(trace_idx, periodic);trace.append_arg(trace_idx, alpha);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_kaiser_window(c10::DispatchKeySet dispatchKeySet, int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::kaiser_window(dispatchKeySet, window_length, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_KAISER_WINDOW, dispatchKeySet);
  trace.append_arg(trace_idx, window_length);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_kaiser_window_periodic(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::kaiser_window(dispatchKeySet, window_length, periodic, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_KAISER_WINDOW_PERIODIC, dispatchKeySet);
  trace.append_arg(trace_idx, window_length);trace.append_arg(trace_idx, periodic);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_kaiser_window_beta(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, double beta, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::kaiser_window(dispatchKeySet, window_length, periodic, beta, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_KAISER_WINDOW_BETA, dispatchKeySet);
  trace.append_arg(trace_idx, window_length);trace.append_arg(trace_idx, periodic);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_hinge_embedding_loss(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, double margin, int64_t reduction) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hinge_embedding_loss(dispatchKeySet, self, target, margin, reduction);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_HINGE_EMBEDDING_LOSS, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, margin);trace.append_arg(trace_idx, reduction);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_group_norm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, int64_t num_groups, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps, bool cudnn_enabled) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::group_norm(dispatchKeySet, input, num_groups, weight, bias, eps, cudnn_enabled);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_GROUP_NORM, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, num_groups);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, eps);trace.append_arg(trace_idx, cudnn_enabled);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_native_group_norm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, int64_t N, int64_t C, int64_t HxW, int64_t group, double eps) {
  ensure_materialized(input);ensure_materialized(weight);ensure_materialized(bias);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::native_group_norm(dispatchKeySet, input, weight, bias, N, C, HxW, group, eps);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_native_group_norm_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & rstd, const c10::optional<at::Tensor> & weight, int64_t N, int64_t C, int64_t HxW, int64_t group, std::array<bool,3> output_mask) {
  ensure_materialized(grad_out);ensure_materialized(input);ensure_materialized(mean);ensure_materialized(rstd);ensure_materialized(weight);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::native_group_norm_backward(dispatchKeySet, grad_out, input, mean, rstd, weight, N, C, HxW, group, std::move(output_mask));
}

at::Tensor wrap__fft_r2c(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool onesided) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_fft_r2c(dispatchKeySet, self, std::move(dim), normalization, onesided);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__FFT_R2C, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, normalization);trace.append_arg(trace_idx, onesided);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap__fft_r2c_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool onesided, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_fft_r2c_outf(dispatchKeySet, self, std::move(dim), normalization, onesided, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__FFT_R2C_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, normalization);trace.append_arg(trace_idx, onesided);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap__fft_c2r(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, int64_t last_dim_size) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_fft_c2r(dispatchKeySet, self, std::move(dim), normalization, last_dim_size);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__FFT_C2R, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, normalization);trace.append_arg(trace_idx, last_dim_size);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap__fft_c2r_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, int64_t last_dim_size, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_fft_c2r_outf(dispatchKeySet, self, std::move(dim), normalization, last_dim_size, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__FFT_C2R_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, normalization);trace.append_arg(trace_idx, last_dim_size);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap__fft_c2c(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool forward) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_fft_c2c(dispatchKeySet, self, std::move(dim), normalization, forward);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__FFT_C2C, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, normalization);trace.append_arg(trace_idx, forward);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap__fft_c2c_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool forward, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_fft_c2c_outf(dispatchKeySet, self, std::move(dim), normalization, forward, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__FFT_C2C_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, normalization);trace.append_arg(trace_idx, forward);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

int64_t wrap__cufft_get_plan_cache_size(c10::DispatchKeySet dispatchKeySet, int64_t device_index) {
  
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_cufft_get_plan_cache_size(dispatchKeySet, device_index);
}

int64_t wrap__cufft_get_plan_cache_max_size(c10::DispatchKeySet dispatchKeySet, int64_t device_index) {
  
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_cufft_get_plan_cache_max_size(dispatchKeySet, device_index);
}

void wrap__cufft_set_plan_cache_max_size(c10::DispatchKeySet dispatchKeySet, int64_t device_index, int64_t max_size) {
  
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_cufft_set_plan_cache_max_size(dispatchKeySet, device_index, max_size);
}

void wrap__cufft_clear_plan_cache(c10::DispatchKeySet dispatchKeySet, int64_t device_index) {
  
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_cufft_clear_plan_cache(dispatchKeySet, device_index);
}

at::Tensor wrap_index_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index(dispatchKeySet, self, indices);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_INDEX_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, indices);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_index_copy_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_copy_(dispatchKeySet, self, dim, index, source);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_INDEX_COPY_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, source);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_index_copy(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_copy(dispatchKeySet, self, dim, index, source);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_INDEX_COPY, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, source);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_index_copy__dimname(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & source) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_copy_(dispatchKeySet, self, std::move(dim), index, source);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_INDEX_COPY__DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, source);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_index_copy_dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & source) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_copy(dispatchKeySet, self, std::move(dim), index, source);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_INDEX_COPY_DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, source);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_index_put_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_put_(dispatchKeySet, self, indices, values, accumulate);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_INDEX_PUT_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, values);trace.append_arg(trace_idx, accumulate);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_index_put(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_put(dispatchKeySet, self, indices, values, accumulate);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_INDEX_PUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, values);trace.append_arg(trace_idx, accumulate);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap__index_put_impl_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate, bool unsafe) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_index_put_impl_(dispatchKeySet, self, indices, values, accumulate, unsafe);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__INDEX_PUT_IMPL_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, values);trace.append_arg(trace_idx, accumulate);trace.append_arg(trace_idx, unsafe);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_instance_norm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool use_input_stats, double momentum, double eps, bool cudnn_enabled) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::instance_norm(dispatchKeySet, input, weight, bias, running_mean, running_var, use_input_stats, momentum, eps, cudnn_enabled);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_INSTANCE_NORM, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, running_mean);trace.append_arg(trace_idx, running_var);trace.append_arg(trace_idx, use_input_stats);trace.append_arg(trace_idx, momentum);trace.append_arg(trace_idx, eps);trace.append_arg(trace_idx, cudnn_enabled);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_inverse(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::inverse(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_INVERSE, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_inverse_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::inverse_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_INVERSE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap__inverse_helper(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_inverse_helper(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__INVERSE_HELPER, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_isclose(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, double rtol, double atol, bool equal_nan) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::isclose(dispatchKeySet, self, other, rtol, atol, equal_nan);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ISCLOSE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, rtol);trace.append_arg(trace_idx, atol);trace.append_arg(trace_idx, equal_nan);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_isnan(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::isnan(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(kBool), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ISNAN, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

bool wrap_is_distributed(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::is_distributed(dispatchKeySet, self);
}

bool wrap_is_floating_point(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::__dispatch_is_floating_point(dispatchKeySet, self);
}

bool wrap_is_complex(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::__dispatch_is_complex(dispatchKeySet, self);
}

at::Tensor wrap_isreal(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::isreal(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ISREAL, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

bool wrap_is_nonzero(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::is_nonzero(dispatchKeySet, self);
}

bool wrap_is_same_size(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  ensure_materialized(self);ensure_materialized(other);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::is_same_size(dispatchKeySet, self, other);
}

bool wrap_is_signed(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::__dispatch_is_signed(dispatchKeySet, self);
}

at::Tensor wrap_kl_div(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, bool log_target) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::kl_div(dispatchKeySet, self, target, reduction, log_target);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_KL_DIV, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, log_target);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_kl_div_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, bool log_target) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::kl_div_backward(dispatchKeySet, grad_output, self, target, reduction, log_target);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_KL_DIV_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, log_target);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_kron(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::kron(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_KRON, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_kron_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::kron_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_KRON_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

std::tuple<at::Tensor,at::Tensor> wrap_kthvalue(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t k, int64_t dim, bool keepdim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::kthvalue(dispatchKeySet, self, k, dim, keepdim);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_kthvalue_values(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t k, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(values);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::kthvalue_outf(dispatchKeySet, self, k, dim, keepdim, values, indices);
}

std::tuple<at::Tensor,at::Tensor> wrap_kthvalue_dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t k, at::Dimname dim, bool keepdim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::kthvalue(dispatchKeySet, self, k, std::move(dim), keepdim);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_kthvalue_dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t k, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(values);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::kthvalue_outf(dispatchKeySet, self, k, std::move(dim), keepdim, values, indices);
}

at::Tensor wrap_layer_norm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::IntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps, bool cudnn_enable) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::layer_norm(dispatchKeySet, input, std::move(normalized_shape), weight, bias, eps, cudnn_enable);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LAYER_NORM, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, std::move(normalized_shape));trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, eps);trace.append_arg(trace_idx, cudnn_enable);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_native_layer_norm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::IntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps) {
  ensure_materialized(input);ensure_materialized(weight);ensure_materialized(bias);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::native_layer_norm(dispatchKeySet, input, std::move(normalized_shape), weight, bias, eps);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_native_layer_norm_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & input, at::IntArrayRef normalized_shape, const at::Tensor & mean, const at::Tensor & rstd, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, std::array<bool,3> output_mask) {
  ensure_materialized(grad_out);ensure_materialized(input);ensure_materialized(mean);ensure_materialized(rstd);ensure_materialized(weight);ensure_materialized(bias);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::native_layer_norm_backward(dispatchKeySet, grad_out, input, std::move(normalized_shape), mean, rstd, weight, bias, std::move(output_mask));
}

at::Tensor wrap_nan_to_num(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nan_to_num(dispatchKeySet, self, nan, posinf, neginf);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NAN_TO_NUM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, nan);trace.append_arg(trace_idx, posinf);trace.append_arg(trace_idx, neginf);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_nan_to_num_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nan_to_num_(dispatchKeySet, self, nan, posinf, neginf);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NAN_TO_NUM_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, nan);trace.append_arg(trace_idx, posinf);trace.append_arg(trace_idx, neginf);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_nan_to_num_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nan_to_num_outf(dispatchKeySet, self, nan, posinf, neginf, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NAN_TO_NUM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, nan);trace.append_arg(trace_idx, posinf);trace.append_arg(trace_idx, neginf);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_linear(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linear(dispatchKeySet, input, weight, bias);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINEAR, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_mkldnn_linear(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mkldnn_linear(dispatchKeySet, self, weight, bias);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MKLDNN_LINEAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_mkldnn_linear_backward_input(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef input_size, const at::Tensor & grad_output, const at::Tensor & weight) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mkldnn_linear_backward_input(dispatchKeySet, std::move(input_size), grad_output, weight);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MKLDNN_LINEAR_BACKWARD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(input_size));trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, weight);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap_mkldnn_linear_backward_weights(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, bool bias_defined) {
  ensure_materialized(grad_output);ensure_materialized(input);ensure_materialized(weight);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::mkldnn_linear_backward_weights(dispatchKeySet, grad_output, input, weight, bias_defined);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_mkldnn_linear_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, std::array<bool,3> output_mask) {
  ensure_materialized(self);ensure_materialized(grad_output);ensure_materialized(weight);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::mkldnn_linear_backward(dispatchKeySet, self, grad_output, weight, std::move(output_mask));
}

at::Tensor wrap_fbgemm_linear_int8_weight_fp32_activation(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const at::Tensor & packed, const at::Tensor & col_offsets, const at::Scalar & weight_scale, const at::Scalar & weight_zero_point, const at::Tensor & bias) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fbgemm_linear_int8_weight_fp32_activation(dispatchKeySet, input, weight, packed, col_offsets, weight_scale, weight_zero_point, bias);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FBGEMM_LINEAR_INT8_WEIGHT_FP32_ACTIVATION, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, packed);trace.append_arg(trace_idx, col_offsets);trace.append_arg(trace_idx, weight_scale);trace.append_arg(trace_idx, weight_zero_point);trace.append_arg(trace_idx, bias);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_fbgemm_linear_int8_weight(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const at::Tensor & packed, const at::Tensor & col_offsets, const at::Scalar & weight_scale, const at::Scalar & weight_zero_point, const at::Tensor & bias) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fbgemm_linear_int8_weight(dispatchKeySet, input, weight, packed, col_offsets, weight_scale, weight_zero_point, bias);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FBGEMM_LINEAR_INT8_WEIGHT, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, packed);trace.append_arg(trace_idx, col_offsets);trace.append_arg(trace_idx, weight_scale);trace.append_arg(trace_idx, weight_zero_point);trace.append_arg(trace_idx, bias);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,double,int64_t> wrap_fbgemm_linear_quantize_weight(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input) {
  ensure_materialized(input);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::fbgemm_linear_quantize_weight(dispatchKeySet, input);
}

at::Tensor wrap_fbgemm_pack_gemm_matrix_fp16(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fbgemm_pack_gemm_matrix_fp16(dispatchKeySet, input);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FBGEMM_PACK_GEMM_MATRIX_FP16, dispatchKeySet);
  trace.append_arg(trace_idx, input);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_fbgemm_linear_fp16_weight_fp32_activation(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & packed_weight, const at::Tensor & bias) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fbgemm_linear_fp16_weight_fp32_activation(dispatchKeySet, input, packed_weight, bias);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FBGEMM_LINEAR_FP16_WEIGHT_FP32_ACTIVATION, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, packed_weight);trace.append_arg(trace_idx, bias);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_fbgemm_linear_fp16_weight(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & packed_weight, const at::Tensor & bias) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fbgemm_linear_fp16_weight(dispatchKeySet, input, packed_weight, bias);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FBGEMM_LINEAR_FP16_WEIGHT, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, packed_weight);trace.append_arg(trace_idx, bias);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_fbgemm_pack_quantized_matrix(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fbgemm_pack_quantized_matrix(dispatchKeySet, input);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FBGEMM_PACK_QUANTIZED_MATRIX, dispatchKeySet);
  trace.append_arg(trace_idx, input);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_fbgemm_pack_quantized_matrix_KN(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, int64_t K, int64_t N) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fbgemm_pack_quantized_matrix(dispatchKeySet, input, K, N);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FBGEMM_PACK_QUANTIZED_MATRIX_KN, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, K);trace.append_arg(trace_idx, N);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_ldexp_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ldexp(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LDEXP_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_ldexp_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ldexp_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LDEXP_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_ldexp_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ldexp_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LDEXP_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_linspace(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, c10::optional<int64_t> steps, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linspace(dispatchKeySet, start, end, steps, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINSPACE, dispatchKeySet);
  trace.append_arg(trace_idx, start);trace.append_arg(trace_idx, end);trace.append_arg(trace_idx, steps);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_linspace_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, c10::optional<int64_t> steps, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linspace_outf(dispatchKeySet, start, end, steps, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINSPACE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, start);trace.append_arg(trace_idx, end);trace.append_arg(trace_idx, steps);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_log_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::log_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOG_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_log10_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::log10_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOG10_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_log1p(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::log1p(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LOG1P, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_log1p_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::log1p_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOG1P_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_log1p_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::log1p_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOG1P_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_log2_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::log2_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOG2_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_logaddexp_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logaddexp_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOGADDEXP_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_logaddexp(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logaddexp(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LOGADDEXP, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_logaddexp2_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logaddexp2_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOGADDEXP2_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_logaddexp2(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logaddexp2(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LOGADDEXP2, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_xlogy_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::xlogy(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_XLOGY_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_xlogy_Scalar_Self(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::xlogy(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(other.dtype(), other.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_XLOGY_SCALAR_SELF, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_xlogy_Scalar_Other(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::xlogy(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_XLOGY_SCALAR_OTHER, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_xlogy__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::xlogy_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_XLOGY__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_xlogy__Scalar_Other(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::xlogy_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_XLOGY__SCALAR_OTHER, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_xlogy_OutTensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::xlogy_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_XLOGY_OUTTENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_xlogy_OutScalar_Self(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::xlogy_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_XLOGY_OUTSCALAR_SELF, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_xlogy_OutScalar_Other(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::xlogy_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_XLOGY_OUTSCALAR_OTHER, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_logdet(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logdet(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LOGDET, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_logspace(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, c10::optional<int64_t> steps, double base, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logspace(dispatchKeySet, start, end, steps, base, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LOGSPACE, dispatchKeySet);
  trace.append_arg(trace_idx, start);trace.append_arg(trace_idx, end);trace.append_arg(trace_idx, steps);trace.append_arg(trace_idx, base);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_logspace_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, c10::optional<int64_t> steps, double base, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logspace_outf(dispatchKeySet, start, end, steps, base, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOGSPACE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, start);trace.append_arg(trace_idx, end);trace.append_arg(trace_idx, steps);trace.append_arg(trace_idx, base);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_log_softmax_int(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::log_softmax(dispatchKeySet, self, dim, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LOG_SOFTMAX_INT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_log_softmax_Dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::log_softmax(dispatchKeySet, self, std::move(dim), std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LOG_SOFTMAX_DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__log_softmax(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_log_softmax(dispatchKeySet, self, dim, half_to_float);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__LOG_SOFTMAX, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, half_to_float);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__log_softmax_backward_data(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_log_softmax_backward_data(dispatchKeySet, grad_output, output, dim, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__LOG_SOFTMAX_BACKWARD_DATA, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, output);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__logcumsumexp(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_logcumsumexp(dispatchKeySet, self, dim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__LOGCUMSUMEXP, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap__logcumsumexp_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_logcumsumexp_outf(dispatchKeySet, self, dim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__LOGCUMSUMEXP_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_logcumsumexp(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logcumsumexp(dispatchKeySet, self, dim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LOGCUMSUMEXP, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_logcumsumexp_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logcumsumexp_outf(dispatchKeySet, self, dim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOGCUMSUMEXP_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_logcumsumexp_dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logcumsumexp(dispatchKeySet, self, std::move(dim));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LOGCUMSUMEXP_DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_logcumsumexp_dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logcumsumexp_outf(dispatchKeySet, self, std::move(dim), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOGCUMSUMEXP_DIMNAME_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_logsumexp(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logsumexp(dispatchKeySet, self, std::move(dim), keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LOGSUMEXP, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_logsumexp_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logsumexp_outf(dispatchKeySet, self, std::move(dim), keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOGSUMEXP_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_logsumexp_names(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logsumexp(dispatchKeySet, self, std::move(dim), keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LOGSUMEXP_NAMES, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_logsumexp_names_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logsumexp_outf(dispatchKeySet, self, std::move(dim), keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOGSUMEXP_NAMES_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_margin_ranking_loss(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & target, double margin, int64_t reduction) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::margin_ranking_loss(dispatchKeySet, input1, input2, target, margin, reduction);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input1.dtype(), input1.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MARGIN_RANKING_LOSS, dispatchKeySet);
  trace.append_arg(trace_idx, input1);trace.append_arg(trace_idx, input2);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, margin);trace.append_arg(trace_idx, reduction);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_matmul(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::matmul(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MATMUL, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_matmul_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::matmul_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MATMUL_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_matrix_rank_tol(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double tol, bool symmetric) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::matrix_rank(dispatchKeySet, self, tol, symmetric);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MATRIX_RANK_TOL, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, tol);trace.append_arg(trace_idx, symmetric);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_matrix_rank(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool symmetric) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::matrix_rank(dispatchKeySet, self, symmetric);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MATRIX_RANK, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, symmetric);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_matrix_power(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t n) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::matrix_power(dispatchKeySet, self, n);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MATRIX_POWER, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, n);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_matrix_power_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t n, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::matrix_power_outf(dispatchKeySet, self, n, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MATRIX_POWER_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_matrix_exp(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::matrix_exp(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MATRIX_EXP, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_matrix_exp_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::matrix_exp_backward(dispatchKeySet, self, grad);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MATRIX_EXP_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, grad);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap__aminmax(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_aminmax(dispatchKeySet, self);
}

std::tuple<at::Tensor,at::Tensor> wrap__aminmax_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_aminmax(dispatchKeySet, self, dim, keepdim);
}

at::Tensor wrap__compute_linear_combination(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & coefficients) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_compute_linear_combination(dispatchKeySet, input, coefficients);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__COMPUTE_LINEAR_COMBINATION, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, coefficients);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap__compute_linear_combination_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & coefficients, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_compute_linear_combination_outf(dispatchKeySet, input, coefficients, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__COMPUTE_LINEAR_COMBINATION_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, coefficients);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

std::tuple<at::Tensor,at::Tensor> wrap_max_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::max(dispatchKeySet, self, dim, keepdim);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_max_dim_max(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & max, at::Tensor & max_values) {
  ensure_materialized(self);ensure_materialized(max);ensure_materialized(max_values);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::max_outf(dispatchKeySet, self, dim, keepdim, max, max_values);
}

std::tuple<at::Tensor,at::Tensor> wrap_max_names_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::max(dispatchKeySet, self, std::move(dim), keepdim);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_max_names_dim_max(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & max, at::Tensor & max_values) {
  ensure_materialized(self);ensure_materialized(max);ensure_materialized(max_values);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::max_outf(dispatchKeySet, self, std::move(dim), keepdim, max, max_values);
}

at::Tensor wrap_value_selecting_reduction_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, int64_t dim, const at::Tensor & indices, at::IntArrayRef sizes, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::value_selecting_reduction_backward(dispatchKeySet, grad, dim, indices, std::move(sizes), keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_VALUE_SELECTING_REDUCTION_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, std::move(sizes));trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_amax(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::amax(dispatchKeySet, self, std::move(dim), keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_AMAX, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_amax_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::amax_outf(dispatchKeySet, self, std::move(dim), keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_AMAX_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

std::tuple<at::Tensor,at::Tensor> wrap_max_pool1d_with_indices(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::max_pool1d_with_indices(dispatchKeySet, self, std::move(kernel_size), std::move(stride), std::move(padding), std::move(dilation), ceil_mode);
}

at::Tensor wrap_max_pool1d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::max_pool1d(dispatchKeySet, self, std::move(kernel_size), std::move(stride), std::move(padding), std::move(dilation), ceil_mode);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MAX_POOL1D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, ceil_mode);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_max_pool2d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::max_pool2d(dispatchKeySet, self, std::move(kernel_size), std::move(stride), std::move(padding), std::move(dilation), ceil_mode);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MAX_POOL2D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, ceil_mode);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_mkldnn_max_pool2d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mkldnn_max_pool2d(dispatchKeySet, self, std::move(kernel_size), std::move(stride), std::move(padding), std::move(dilation), ceil_mode);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MKLDNN_MAX_POOL2D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, ceil_mode);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_mkldnn_max_pool2d_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, const at::Tensor & input, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mkldnn_max_pool2d_backward(dispatchKeySet, grad_output, output, input, std::move(kernel_size), std::move(stride), std::move(padding), std::move(dilation), ceil_mode);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MKLDNN_MAX_POOL2D_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, output);trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, ceil_mode);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_mkldnn_max_pool3d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mkldnn_max_pool3d(dispatchKeySet, self, std::move(kernel_size), std::move(stride), std::move(padding), std::move(dilation), ceil_mode);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MKLDNN_MAX_POOL3D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, ceil_mode);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_mkldnn_max_pool3d_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, const at::Tensor & input, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mkldnn_max_pool3d_backward(dispatchKeySet, grad_output, output, input, std::move(kernel_size), std::move(stride), std::move(padding), std::move(dilation), ceil_mode);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MKLDNN_MAX_POOL3D_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, output);trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, ceil_mode);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_quantized_max_pool1d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::quantized_max_pool1d(dispatchKeySet, self, std::move(kernel_size), std::move(stride), std::move(padding), std::move(dilation), ceil_mode);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_QUANTIZED_MAX_POOL1D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, ceil_mode);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_quantized_max_pool2d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::quantized_max_pool2d(dispatchKeySet, self, std::move(kernel_size), std::move(stride), std::move(padding), std::move(dilation), ceil_mode);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_QUANTIZED_MAX_POOL2D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, ceil_mode);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_max_pool3d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::max_pool3d(dispatchKeySet, self, std::move(kernel_size), std::move(stride), std::move(padding), std::move(dilation), ceil_mode);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MAX_POOL3D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, ceil_mode);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_mean(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mean(dispatchKeySet, self, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MEAN, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_mean_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mean(dispatchKeySet, self, std::move(dim), keepdim, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MEAN_DIM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_mean_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mean_outf(dispatchKeySet, self, std::move(dim), keepdim, std::move(dtype), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MEAN_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_mean_names_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mean(dispatchKeySet, self, std::move(dim), keepdim, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MEAN_NAMES_DIM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_mean_names_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mean_outf(dispatchKeySet, self, std::move(dim), keepdim, std::move(dtype), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MEAN_NAMES_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_median(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::median(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MEDIAN, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap_median_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::median(dispatchKeySet, self, dim, keepdim);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_median_dim_values(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(values);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::median_outf(dispatchKeySet, self, dim, keepdim, values, indices);
}

std::tuple<at::Tensor,at::Tensor> wrap_median_names_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::median(dispatchKeySet, self, std::move(dim), keepdim);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_median_names_dim_values(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(values);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::median_outf(dispatchKeySet, self, std::move(dim), keepdim, values, indices);
}

at::Tensor wrap_nanmedian(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nanmedian(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NANMEDIAN, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap_nanmedian_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::nanmedian(dispatchKeySet, self, dim, keepdim);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_nanmedian_dim_values(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(values);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::nanmedian_outf(dispatchKeySet, self, dim, keepdim, values, indices);
}

std::tuple<at::Tensor,at::Tensor> wrap_nanmedian_names_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::nanmedian(dispatchKeySet, self, std::move(dim), keepdim);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_nanmedian_names_dim_values(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(values);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::nanmedian_outf(dispatchKeySet, self, std::move(dim), keepdim, values, indices);
}

std::tuple<at::Tensor,at::Tensor> wrap_min_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::min(dispatchKeySet, self, dim, keepdim);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_min_dim_min(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & min, at::Tensor & min_indices) {
  ensure_materialized(self);ensure_materialized(min);ensure_materialized(min_indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::min_outf(dispatchKeySet, self, dim, keepdim, min, min_indices);
}

std::tuple<at::Tensor,at::Tensor> wrap_min_names_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::min(dispatchKeySet, self, std::move(dim), keepdim);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_min_names_dim_min(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & min, at::Tensor & min_indices) {
  ensure_materialized(self);ensure_materialized(min);ensure_materialized(min_indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::min_outf(dispatchKeySet, self, std::move(dim), keepdim, min, min_indices);
}

at::Tensor wrap_amin(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::amin(dispatchKeySet, self, std::move(dim), keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_AMIN, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_amin_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::amin_outf(dispatchKeySet, self, std::move(dim), keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_AMIN_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_mkldnn_convolution(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mkldnn_convolution(dispatchKeySet, self, weight, bias, std::move(padding), std::move(stride), std::move(dilation), groups);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MKLDNN_CONVOLUTION, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_mkldnn_convolution_backward_input(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef self_size, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool bias_defined) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mkldnn_convolution_backward_input(dispatchKeySet, std::move(self_size), grad_output, weight, std::move(padding), std::move(stride), std::move(dilation), groups, bias_defined);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MKLDNN_CONVOLUTION_BACKWARD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(self_size));trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, bias_defined);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap_mkldnn_convolution_backward_weights(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool bias_defined) {
  ensure_materialized(grad_output);ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::mkldnn_convolution_backward_weights(dispatchKeySet, std::move(weight_size), grad_output, self, std::move(padding), std::move(stride), std::move(dilation), groups, bias_defined);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_mkldnn_convolution_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, std::array<bool,3> output_mask) {
  ensure_materialized(self);ensure_materialized(grad_output);ensure_materialized(weight);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::mkldnn_convolution_backward(dispatchKeySet, self, grad_output, weight, std::move(padding), std::move(stride), std::move(dilation), groups, std::move(output_mask));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_miopen_batch_norm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double exponential_average_factor, double epsilon) {
  ensure_materialized(input);ensure_materialized(weight);ensure_materialized(bias);ensure_materialized(running_mean);ensure_materialized(running_var);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::miopen_batch_norm(dispatchKeySet, input, weight, bias, running_mean, running_var, training, exponential_average_factor, epsilon);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_miopen_batch_norm_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grad_output, const at::Tensor & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_var, double epsilon) {
  ensure_materialized(input);ensure_materialized(grad_output);ensure_materialized(weight);ensure_materialized(running_mean);ensure_materialized(running_var);ensure_materialized(save_mean);ensure_materialized(save_var);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::miopen_batch_norm_backward(dispatchKeySet, input, grad_output, weight, running_mean, running_var, save_mean, save_var, epsilon);
}

at::Tensor wrap_miopen_convolution(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::miopen_convolution(dispatchKeySet, self, weight, bias, std::move(padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MIOPEN_CONVOLUTION, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, benchmark);trace.append_arg(trace_idx, deterministic);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_miopen_convolution_backward_input(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef self_size, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::miopen_convolution_backward_input(dispatchKeySet, std::move(self_size), grad_output, weight, std::move(padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MIOPEN_CONVOLUTION_BACKWARD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(self_size));trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, benchmark);trace.append_arg(trace_idx, deterministic);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_miopen_convolution_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) {
  ensure_materialized(self);ensure_materialized(grad_output);ensure_materialized(weight);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::miopen_convolution_backward(dispatchKeySet, self, grad_output, weight, std::move(padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic, std::move(output_mask));
}

at::Tensor wrap_miopen_convolution_backward_bias(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::miopen_convolution_backward_bias(dispatchKeySet, grad_output);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MIOPEN_CONVOLUTION_BACKWARD_BIAS, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_miopen_convolution_backward_weight(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::miopen_convolution_backward_weight(dispatchKeySet, std::move(weight_size), grad_output, self, std::move(padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MIOPEN_CONVOLUTION_BACKWARD_WEIGHT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(weight_size));trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, benchmark);trace.append_arg(trace_idx, deterministic);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_miopen_convolution_transpose(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::miopen_convolution_transpose(dispatchKeySet, self, weight, bias, std::move(padding), std::move(output_padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MIOPEN_CONVOLUTION_TRANSPOSE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(output_padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, benchmark);trace.append_arg(trace_idx, deterministic);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_miopen_convolution_transpose_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) {
  ensure_materialized(self);ensure_materialized(grad_output);ensure_materialized(weight);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::miopen_convolution_transpose_backward(dispatchKeySet, self, grad_output, weight, std::move(padding), std::move(output_padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic, std::move(output_mask));
}

at::Tensor wrap_miopen_convolution_transpose_backward_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::miopen_convolution_transpose_backward_input(dispatchKeySet, grad_output, weight, std::move(padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MIOPEN_CONVOLUTION_TRANSPOSE_BACKWARD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, benchmark);trace.append_arg(trace_idx, deterministic);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_miopen_convolution_transpose_backward_weight(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::miopen_convolution_transpose_backward_weight(dispatchKeySet, std::move(weight_size), grad_output, self, std::move(padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MIOPEN_CONVOLUTION_TRANSPOSE_BACKWARD_WEIGHT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(weight_size));trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, benchmark);trace.append_arg(trace_idx, deterministic);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_miopen_depthwise_convolution(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::miopen_depthwise_convolution(dispatchKeySet, self, weight, bias, std::move(padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MIOPEN_DEPTHWISE_CONVOLUTION, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, benchmark);trace.append_arg(trace_idx, deterministic);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_miopen_depthwise_convolution_backward_input(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef self_size, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::miopen_depthwise_convolution_backward_input(dispatchKeySet, std::move(self_size), grad_output, weight, std::move(padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MIOPEN_DEPTHWISE_CONVOLUTION_BACKWARD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(self_size));trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, benchmark);trace.append_arg(trace_idx, deterministic);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_miopen_depthwise_convolution_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) {
  ensure_materialized(self);ensure_materialized(grad_output);ensure_materialized(weight);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::miopen_depthwise_convolution_backward(dispatchKeySet, self, grad_output, weight, std::move(padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic, std::move(output_mask));
}

at::Tensor wrap_miopen_depthwise_convolution_backward_weight(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::miopen_depthwise_convolution_backward_weight(dispatchKeySet, std::move(weight_size), grad_output, self, std::move(padding), std::move(stride), std::move(dilation), groups, benchmark, deterministic);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MIOPEN_DEPTHWISE_CONVOLUTION_BACKWARD_WEIGHT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(weight_size));trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);trace.append_arg(trace_idx, benchmark);trace.append_arg(trace_idx, deterministic);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrap_miopen_rnn(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, at::IntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state) {
  ensure_materialized(input);ensure_materialized(weight);ensure_materialized(hx);ensure_materialized(cx);ensure_materialized(dropout_state);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::miopen_rnn(dispatchKeySet, input, std::move(weight), weight_stride0, hx, cx, mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, std::move(batch_sizes), dropout_state);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,std::vector<at::Tensor>> wrap_miopen_rnn_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const at::Tensor & weight_buf, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, const at::Tensor & output, const c10::optional<at::Tensor> & grad_output, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, at::IntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state, const at::Tensor & reserve, std::array<bool,4> output_mask) {
  ensure_materialized(input);ensure_materialized(weight);ensure_materialized(weight_buf);ensure_materialized(hx);ensure_materialized(cx);ensure_materialized(output);ensure_materialized(grad_output);ensure_materialized(grad_hy);ensure_materialized(grad_cy);ensure_materialized(dropout_state);ensure_materialized(reserve);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::miopen_rnn_backward(dispatchKeySet, input, std::move(weight), weight_stride0, weight_buf, hx, cx, output, grad_output, grad_hy, grad_cy, mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, std::move(batch_sizes), dropout_state, reserve, std::move(output_mask));
}

at::Tensor wrap_mm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mm(dispatchKeySet, self, mat2);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mat2);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_mm_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mm_outf(dispatchKeySet, self, mat2, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mat2);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap__sparse_mm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & sparse, const at::Tensor & dense) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sparse_mm(dispatchKeySet, sparse, dense);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(sparse.dtype(), sparse.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SPARSE_MM, dispatchKeySet);
  trace.append_arg(trace_idx, sparse);trace.append_arg(trace_idx, dense);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__sparse_sparse_matmul(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sparse_sparse_matmul(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SPARSE_SPARSE_MATMUL, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__sparse_mask_helper(c10::DispatchKeySet dispatchKeySet, const at::Tensor & t, const at::Tensor & mask_indices) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sparse_mask_helper(dispatchKeySet, t, mask_indices);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(t.dtype(), t.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SPARSE_MASK_HELPER, dispatchKeySet);
  trace.append_arg(trace_idx, t);trace.append_arg(trace_idx, mask_indices);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap_mode(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::mode(dispatchKeySet, self, dim, keepdim);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_mode_values(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(values);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::mode_outf(dispatchKeySet, self, dim, keepdim, values, indices);
}

std::tuple<at::Tensor,at::Tensor> wrap_mode_dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::mode(dispatchKeySet, self, std::move(dim), keepdim);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_mode_dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(values);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::mode_outf(dispatchKeySet, self, std::move(dim), keepdim, values, indices);
}

at::Tensor wrap_mul_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mul(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MUL_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_mul__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mul_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MUL__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_mul_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mul_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MUL_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_mul_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mul(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MUL_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_mul__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mul_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MUL__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_multiply_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::multiply(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MULTIPLY_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_multiply__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::multiply_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MULTIPLY__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_multiply_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::multiply_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MULTIPLY_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_multiply_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::multiply(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MULTIPLY_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_multiply__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::multiply_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MULTIPLY__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_mv(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mv(dispatchKeySet, self, vec);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MV, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, vec);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_mv_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mv_outf(dispatchKeySet, self, vec, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MV_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, vec);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_mvlgamma(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t p) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mvlgamma(dispatchKeySet, self, p);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MVLGAMMA, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_mvlgamma_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t p) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mvlgamma_(dispatchKeySet, self, p);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MVLGAMMA_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_narrow_copy(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, int64_t start, int64_t length) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::narrow_copy(dispatchKeySet, self, dim, start, length);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NARROW_COPY, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, start);trace.append_arg(trace_idx, length);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_narrow_copy_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, int64_t start, int64_t length, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::narrow_copy_outf(dispatchKeySet, self, dim, start, length, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NARROW_COPY_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, start);trace.append_arg(trace_idx, length);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_narrow(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, int64_t start, int64_t length) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::narrow(dispatchKeySet, self, dim, start, length);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NARROW, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, start);trace.append_arg(trace_idx, length);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_narrow_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & start, int64_t length) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::narrow(dispatchKeySet, self, dim, start, length);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NARROW_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, start);trace.append_arg(trace_idx, length);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_native_batch_norm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps) {
  ensure_materialized(input);ensure_materialized(weight);ensure_materialized(bias);ensure_materialized(running_mean);ensure_materialized(running_var);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::native_batch_norm(dispatchKeySet, input, weight, bias, running_mean, running_var, training, momentum, eps);
}

std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrap_native_batch_norm_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, at::Tensor & out, at::Tensor & save_mean, at::Tensor & save_invstd) {
  ensure_materialized(input);ensure_materialized(weight);ensure_materialized(bias);ensure_materialized(running_mean);ensure_materialized(running_var);ensure_materialized(out);ensure_materialized(save_mean);ensure_materialized(save_invstd);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::native_batch_norm_outf(dispatchKeySet, input, weight, bias, running_mean, running_var, training, momentum, eps, out, save_mean, save_invstd);
}

std::tuple<at::Tensor,at::Tensor> wrap_batch_norm_stats(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, double eps) {
  ensure_materialized(input);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::batch_norm_stats(dispatchKeySet, input, eps);
}

at::Tensor wrap_batch_norm_elemt(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & mean, const at::Tensor & invstd, double eps) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::batch_norm_elemt(dispatchKeySet, input, weight, bias, mean, invstd, eps);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BATCH_NORM_ELEMT, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, mean);trace.append_arg(trace_idx, invstd);trace.append_arg(trace_idx, eps);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_batch_norm_elemt_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & mean, const at::Tensor & invstd, double eps, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::batch_norm_elemt_outf(dispatchKeySet, input, weight, bias, mean, invstd, eps, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BATCH_NORM_ELEMT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, mean);trace.append_arg(trace_idx, invstd);trace.append_arg(trace_idx, eps);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

std::tuple<at::Tensor,at::Tensor> wrap_batch_norm_gather_stats(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum, double eps, int64_t count) {
  ensure_materialized(input);ensure_materialized(mean);ensure_materialized(invstd);ensure_materialized(running_mean);ensure_materialized(running_var);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::batch_norm_gather_stats(dispatchKeySet, input, mean, invstd, running_mean, running_var, momentum, eps, count);
}

std::tuple<at::Tensor,at::Tensor> wrap_batch_norm_gather_stats_with_counts(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum, double eps, const at::Tensor & counts) {
  ensure_materialized(input);ensure_materialized(mean);ensure_materialized(invstd);ensure_materialized(running_mean);ensure_materialized(running_var);ensure_materialized(counts);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::batch_norm_gather_stats_with_counts(dispatchKeySet, input, mean, invstd, running_mean, running_var, momentum, eps, counts);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_native_batch_norm_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_invstd, bool train, double eps, std::array<bool,3> output_mask) {
  ensure_materialized(grad_out);ensure_materialized(input);ensure_materialized(weight);ensure_materialized(running_mean);ensure_materialized(running_var);ensure_materialized(save_mean);ensure_materialized(save_invstd);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::native_batch_norm_backward(dispatchKeySet, grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, std::move(output_mask));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrap_batch_norm_backward_reduce(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & weight, bool input_g, bool weight_g, bool bias_g) {
  ensure_materialized(grad_out);ensure_materialized(input);ensure_materialized(mean);ensure_materialized(invstd);ensure_materialized(weight);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::batch_norm_backward_reduce(dispatchKeySet, grad_out, input, mean, invstd, weight, input_g, weight_g, bias_g);
}

at::Tensor wrap_batch_norm_backward_elemt(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & weight, const at::Tensor & mean_dy, const at::Tensor & mean_dy_xmu, const at::Tensor & count) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::batch_norm_backward_elemt(dispatchKeySet, grad_out, input, mean, invstd, weight, mean_dy, mean_dy_xmu, count);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_out.dtype(), grad_out.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BATCH_NORM_BACKWARD_ELEMT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_out);trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, mean);trace.append_arg(trace_idx, invstd);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, mean_dy);trace.append_arg(trace_idx, mean_dy_xmu);trace.append_arg(trace_idx, count);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap_batch_norm_update_stats(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum) {
  ensure_materialized(input);ensure_materialized(running_mean);ensure_materialized(running_var);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::batch_norm_update_stats(dispatchKeySet, input, running_mean, running_var, momentum);
}

bool wrap_is_vulkan_available(c10::DispatchKeySet dispatchKeySet) {
  
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::is_vulkan_available(dispatchKeySet);
}

bool wrap__nnpack_available(c10::DispatchKeySet dispatchKeySet) {
  
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_nnpack_available(dispatchKeySet);
}

at::Tensor wrap__nnpack_spatial_convolution(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef stride) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_nnpack_spatial_convolution(dispatchKeySet, input, weight, bias, std::move(padding), std::move(stride));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__NNPACK_SPATIAL_CONVOLUTION, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap__nnpack_spatial_convolution_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, std::array<bool,3> output_mask) {
  ensure_materialized(input);ensure_materialized(grad_output);ensure_materialized(weight);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_nnpack_spatial_convolution_backward(dispatchKeySet, input, grad_output, weight, std::move(padding), std::move(output_mask));
}

at::Tensor wrap__nnpack_spatial_convolution_backward_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_nnpack_spatial_convolution_backward_input(dispatchKeySet, input, grad_output, weight, std::move(padding));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__NNPACK_SPATIAL_CONVOLUTION_BACKWARD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(padding));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__nnpack_spatial_convolution_backward_weight(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::IntArrayRef weightsize, const at::Tensor & grad_output, at::IntArrayRef padding) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_nnpack_spatial_convolution_backward_weight(dispatchKeySet, input, std::move(weightsize), grad_output, std::move(padding));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__NNPACK_SPATIAL_CONVOLUTION_BACKWARD_WEIGHT, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, std::move(weightsize));trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, std::move(padding));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_ones_names(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ones(dispatchKeySet, std::move(size), std::move(names), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ONES_NAMES, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(names));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_ones(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ones(dispatchKeySet, std::move(size), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ONES, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_ones_out(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ones_outf(dispatchKeySet, std::move(size), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ONES_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_ones_like(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ones_like(dispatchKeySet, self, std::move(dtype), std::move(layout), std::move(device), pin_memory, std::move(memory_format));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), device ? *device : self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ONES_LIKE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);trace.append_arg(trace_idx, std::move(memory_format));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_pairwise_distance(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x1, const at::Tensor & x2, double p, double eps, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::pairwise_distance(dispatchKeySet, x1, x2, p, eps, keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(x1.dtype(), x1.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_PAIRWISE_DISTANCE, dispatchKeySet);
  trace.append_arg(trace_idx, x1);trace.append_arg(trace_idx, x2);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, eps);trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_cdist(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x1, const at::Tensor & x2, double p, c10::optional<int64_t> compute_mode) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cdist(dispatchKeySet, x1, x2, p, compute_mode);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(x1.dtype(), x1.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CDIST, dispatchKeySet);
  trace.append_arg(trace_idx, x1);trace.append_arg(trace_idx, x2);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, compute_mode);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__euclidean_dist(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x1, const at::Tensor & x2) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_euclidean_dist(dispatchKeySet, x1, x2);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(x1.dtype(), x1.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__EUCLIDEAN_DIST, dispatchKeySet);
  trace.append_arg(trace_idx, x1);trace.append_arg(trace_idx, x2);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__cdist_forward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x1, const at::Tensor & x2, double p, c10::optional<int64_t> compute_mode) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_cdist_forward(dispatchKeySet, x1, x2, p, compute_mode);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(x1.dtype(), x1.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__CDIST_FORWARD, dispatchKeySet);
  trace.append_arg(trace_idx, x1);trace.append_arg(trace_idx, x2);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, compute_mode);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__cdist_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & x1, const at::Tensor & x2, double p, const at::Tensor & cdist) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_cdist_backward(dispatchKeySet, grad, x1, x2, p, cdist);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__CDIST_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, x1);trace.append_arg(trace_idx, x2);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, cdist);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_pdist(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double p) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::pdist(dispatchKeySet, self, p);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_PDIST, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__pdist_forward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double p) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_pdist_forward(dispatchKeySet, self, p);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__PDIST_FORWARD, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__pdist_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self, double p, const at::Tensor & pdist) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_pdist_backward(dispatchKeySet, grad, self, p, pdist);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__PDIST_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, pdist);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_cosine_similarity(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x1, const at::Tensor & x2, int64_t dim, double eps) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cosine_similarity(dispatchKeySet, x1, x2, dim, eps);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(x1.dtype(), x1.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_COSINE_SIMILARITY, dispatchKeySet);
  trace.append_arg(trace_idx, x1);trace.append_arg(trace_idx, x2);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, eps);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_permute(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dims) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::permute(dispatchKeySet, self, std::move(dims));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_PERMUTE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dims));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_movedim_intlist(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef source, at::IntArrayRef destination) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::movedim(dispatchKeySet, self, std::move(source), std::move(destination));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MOVEDIM_INTLIST, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(source));trace.append_arg(trace_idx, std::move(destination));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_movedim_int(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t source, int64_t destination) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::movedim(dispatchKeySet, self, source, destination);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MOVEDIM_INT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, source);trace.append_arg(trace_idx, destination);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_moveaxis_intlist(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef source, at::IntArrayRef destination) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::moveaxis(dispatchKeySet, self, std::move(source), std::move(destination));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MOVEAXIS_INTLIST, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(source));trace.append_arg(trace_idx, std::move(destination));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_moveaxis_int(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t source, int64_t destination) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::moveaxis(dispatchKeySet, self, source, destination);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MOVEAXIS_INT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, source);trace.append_arg(trace_idx, destination);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_numpy_T(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::numpy_T(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NUMPY_T, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_pixel_shuffle(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t upscale_factor) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::pixel_shuffle(dispatchKeySet, self, upscale_factor);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_PIXEL_SHUFFLE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, upscale_factor);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_pixel_unshuffle(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t downscale_factor) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::pixel_unshuffle(dispatchKeySet, self, downscale_factor);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_PIXEL_UNSHUFFLE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, downscale_factor);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_channel_shuffle(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t groups) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::channel_shuffle(dispatchKeySet, self, groups);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CHANNEL_SHUFFLE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, groups);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

bool wrap_is_pinned(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::is_pinned(dispatchKeySet, self);
}

at::Tensor wrap_pin_memory(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::pin_memory(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_PIN_MEMORY, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_pinverse(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double rcond) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::pinverse(dispatchKeySet, self, rcond);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_PINVERSE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, rcond);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_poisson_nll_loss(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & target, bool log_input, bool full, double eps, int64_t reduction) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::poisson_nll_loss(dispatchKeySet, input, target, log_input, full, eps, reduction);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_POISSON_NLL_LOSS, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, log_input);trace.append_arg(trace_idx, full);trace.append_arg(trace_idx, eps);trace.append_arg(trace_idx, reduction);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_rad2deg(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rad2deg(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RAD2DEG, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_rad2deg_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rad2deg_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RAD2DEG_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_rad2deg_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rad2deg_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RAD2DEG_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_deg2rad(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::deg2rad(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DEG2RAD, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_deg2rad_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::deg2rad_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_DEG2RAD_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_deg2rad_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::deg2rad_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_DEG2RAD_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_scalar_tensor(c10::DispatchKeySet dispatchKeySet, const at::Scalar & s, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::scalar_tensor(dispatchKeySet, s, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SCALAR_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, s);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_rand_names(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rand(dispatchKeySet, std::move(size), std::move(names), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RAND_NAMES, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(names));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_rand_generator_with_names(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rand(dispatchKeySet, std::move(size), std::move(generator), std::move(names), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RAND_GENERATOR_WITH_NAMES, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(generator));trace.append_arg(trace_idx, std::move(names));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_rand(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rand(dispatchKeySet, std::move(size), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RAND, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_rand_generator(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rand(dispatchKeySet, std::move(size), std::move(generator), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RAND_GENERATOR, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(generator));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_rand_out(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rand_outf(dispatchKeySet, std::move(size), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RAND_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_rand_generator_out(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rand_outf(dispatchKeySet, std::move(size), std::move(generator), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RAND_GENERATOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(generator));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_rand_like(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rand_like(dispatchKeySet, self, std::move(dtype), std::move(layout), std::move(device), pin_memory, std::move(memory_format));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), device ? *device : self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RAND_LIKE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);trace.append_arg(trace_idx, std::move(memory_format));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_randint(c10::DispatchKeySet dispatchKeySet, int64_t high, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::randint(dispatchKeySet, high, std::move(size), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RANDINT, dispatchKeySet);
  trace.append_arg(trace_idx, high);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_randint_generator(c10::DispatchKeySet dispatchKeySet, int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::randint(dispatchKeySet, high, std::move(size), std::move(generator), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RANDINT_GENERATOR, dispatchKeySet);
  trace.append_arg(trace_idx, high);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(generator));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_randint_low(c10::DispatchKeySet dispatchKeySet, int64_t low, int64_t high, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::randint(dispatchKeySet, low, high, std::move(size), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RANDINT_LOW, dispatchKeySet);
  trace.append_arg(trace_idx, low);trace.append_arg(trace_idx, high);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_randint_low_generator(c10::DispatchKeySet dispatchKeySet, int64_t low, int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::randint(dispatchKeySet, low, high, std::move(size), std::move(generator), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RANDINT_LOW_GENERATOR, dispatchKeySet);
  trace.append_arg(trace_idx, low);trace.append_arg(trace_idx, high);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(generator));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_randint_out(c10::DispatchKeySet dispatchKeySet, int64_t high, at::IntArrayRef size, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::randint_outf(dispatchKeySet, high, std::move(size), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RANDINT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, high);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_randint_generator_out(c10::DispatchKeySet dispatchKeySet, int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::randint_outf(dispatchKeySet, high, std::move(size), std::move(generator), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RANDINT_GENERATOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, high);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(generator));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_randint_low_out(c10::DispatchKeySet dispatchKeySet, int64_t low, int64_t high, at::IntArrayRef size, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::randint_outf(dispatchKeySet, low, high, std::move(size), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RANDINT_LOW_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, low);trace.append_arg(trace_idx, high);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_randint_low_generator_out(c10::DispatchKeySet dispatchKeySet, int64_t low, int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::randint_outf(dispatchKeySet, low, high, std::move(size), std::move(generator), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RANDINT_LOW_GENERATOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, low);trace.append_arg(trace_idx, high);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(generator));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_randint_like(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t high, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::randint_like(dispatchKeySet, self, high, std::move(dtype), std::move(layout), std::move(device), pin_memory, std::move(memory_format));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), device ? *device : self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RANDINT_LIKE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, high);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);trace.append_arg(trace_idx, std::move(memory_format));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_randint_like_low_dtype(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t low, int64_t high, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::randint_like(dispatchKeySet, self, low, high, std::move(dtype), std::move(layout), std::move(device), pin_memory, std::move(memory_format));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), device ? *device : self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RANDINT_LIKE_LOW_DTYPE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, low);trace.append_arg(trace_idx, high);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);trace.append_arg(trace_idx, std::move(memory_format));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_randn(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::randn(dispatchKeySet, std::move(size), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RANDN, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_randn_generator(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::randn(dispatchKeySet, std::move(size), std::move(generator), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RANDN_GENERATOR, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(generator));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_randn_names(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::randn(dispatchKeySet, std::move(size), std::move(names), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RANDN_NAMES, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(names));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_randn_generator_with_names(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::randn(dispatchKeySet, std::move(size), std::move(generator), std::move(names), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RANDN_GENERATOR_WITH_NAMES, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(generator));trace.append_arg(trace_idx, std::move(names));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_randn_out(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::randn_outf(dispatchKeySet, std::move(size), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RANDN_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_randn_generator_out(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::randn_outf(dispatchKeySet, std::move(size), std::move(generator), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RANDN_GENERATOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(generator));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_randn_like(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::randn_like(dispatchKeySet, self, std::move(dtype), std::move(layout), std::move(device), pin_memory, std::move(memory_format));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), device ? *device : self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RANDN_LIKE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);trace.append_arg(trace_idx, std::move(memory_format));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_randperm(c10::DispatchKeySet dispatchKeySet, int64_t n, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::randperm(dispatchKeySet, n, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RANDPERM, dispatchKeySet);
  trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_randperm_generator(c10::DispatchKeySet dispatchKeySet, int64_t n, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::randperm(dispatchKeySet, n, std::move(generator), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RANDPERM_GENERATOR, dispatchKeySet);
  trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, std::move(generator));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_randperm_out(c10::DispatchKeySet dispatchKeySet, int64_t n, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::randperm_outf(dispatchKeySet, n, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RANDPERM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_randperm_generator_out(c10::DispatchKeySet dispatchKeySet, int64_t n, c10::optional<at::Generator> generator, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::randperm_outf(dispatchKeySet, n, std::move(generator), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RANDPERM_GENERATOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, std::move(generator));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_range_step(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::range(dispatchKeySet, start, end, step, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RANGE_STEP, dispatchKeySet);
  trace.append_arg(trace_idx, start);trace.append_arg(trace_idx, end);trace.append_arg(trace_idx, step);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_range(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::range(dispatchKeySet, start, end, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RANGE, dispatchKeySet);
  trace.append_arg(trace_idx, start);trace.append_arg(trace_idx, end);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_range_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::range_outf(dispatchKeySet, start, end, step, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RANGE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, start);trace.append_arg(trace_idx, end);trace.append_arg(trace_idx, step);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_ravel(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ravel(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RAVEL, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_reciprocal_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::reciprocal_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RECIPROCAL_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_neg(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::neg(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NEG, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_neg_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::neg_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NEG_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_neg_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::neg_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NEG_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_negative(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::negative(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NEGATIVE, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_negative_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::negative_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NEGATIVE_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_negative_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::negative_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NEGATIVE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_repeat(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef repeats) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::repeat(dispatchKeySet, self, std::move(repeats));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_REPEAT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(repeats));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_repeat_interleave_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & repeats, c10::optional<int64_t> output_size) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::repeat_interleave(dispatchKeySet, repeats, output_size);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(repeats.dtype(), repeats.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_REPEAT_INTERLEAVE_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, repeats);trace.append_arg(trace_idx, output_size);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_repeat_interleave_self_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & repeats, c10::optional<int64_t> dim, c10::optional<int64_t> output_size) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::repeat_interleave(dispatchKeySet, self, repeats, dim, output_size);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_REPEAT_INTERLEAVE_SELF_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, repeats);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, output_size);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_repeat_interleave_self_int(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t repeats, c10::optional<int64_t> dim, c10::optional<int64_t> output_size) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::repeat_interleave(dispatchKeySet, self, repeats, dim, output_size);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_REPEAT_INTERLEAVE_SELF_INT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, repeats);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, output_size);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_reshape(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef shape) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::reshape(dispatchKeySet, self, std::move(shape));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RESHAPE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(shape));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__mkldnn_reshape(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef shape) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_mkldnn_reshape(dispatchKeySet, self, std::move(shape));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__MKLDNN_RESHAPE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(shape));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_reshape_as(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::reshape_as(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RESHAPE_AS, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_round_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::round_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ROUND_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_rrelu(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rrelu(dispatchKeySet, self, lower, upper, training, std::move(generator));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RRELU, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, lower);trace.append_arg(trace_idx, upper);trace.append_arg(trace_idx, training);trace.append_arg(trace_idx, std::move(generator));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_rrelu_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rrelu_(dispatchKeySet, self, lower, upper, training, std::move(generator));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RRELU_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, lower);trace.append_arg(trace_idx, upper);trace.append_arg(trace_idx, training);trace.append_arg(trace_idx, std::move(generator));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_relu(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::relu(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RELU, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_relu_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::relu_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RELU_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_relu6(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::relu6(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RELU6, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_relu6_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::relu6_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RELU6_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_prelu(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::prelu(dispatchKeySet, self, weight);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_PRELU, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap_prelu_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight) {
  ensure_materialized(grad_output);ensure_materialized(self);ensure_materialized(weight);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::prelu_backward(dispatchKeySet, grad_output, self, weight);
}

at::Tensor wrap_gelu(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::gelu(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_GELU, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_gelu_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::gelu_backward(dispatchKeySet, grad, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_GELU_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_infinitely_differentiable_gelu_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::infinitely_differentiable_gelu_backward(dispatchKeySet, grad, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_INFINITELY_DIFFERENTIABLE_GELU_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_hardshrink(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & lambd) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hardshrink(dispatchKeySet, self, lambd);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_HARDSHRINK, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, lambd);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_hardshrink_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & self, const at::Scalar & lambd) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hardshrink_backward(dispatchKeySet, grad_out, self, lambd);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_out.dtype(), grad_out.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_HARDSHRINK_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_out);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, lambd);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_rsqrt_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rsqrt_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RSQRT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_select_Dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, int64_t index) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::select(dispatchKeySet, self, std::move(dim), index);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SELECT_DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, index);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_select_int(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, int64_t index) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::select(dispatchKeySet, self, dim, index);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SELECT_INT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_select_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, at::IntArrayRef input_sizes, int64_t dim, int64_t index) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::select_backward(dispatchKeySet, grad, std::move(input_sizes), dim, index);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SELECT_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, std::move(input_sizes));trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_selu(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::selu(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SELU, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_selu_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::selu_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SELU_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_celu(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::celu(dispatchKeySet, self, alpha);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CELU, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, alpha);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_celu_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::celu_(dispatchKeySet, self, alpha);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CELU_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, alpha);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_silu(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::silu(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SILU, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_silu_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::silu_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SILU_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_silu_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::silu_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SILU_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_silu_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::silu_backward(dispatchKeySet, grad_output, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SILU_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_mish(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mish(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MISH, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_mish_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mish_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MISH_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_mish_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mish_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MISH_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_mish_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mish_backward(dispatchKeySet, grad_output, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MISH_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_sigmoid(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sigmoid(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SIGMOID, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_sigmoid_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sigmoid_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SIGMOID_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_sigmoid_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sigmoid_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SIGMOID_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_logit(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> eps) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logit(dispatchKeySet, self, eps);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LOGIT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, eps);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_logit_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, c10::optional<double> eps) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logit_(dispatchKeySet, self, eps);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOGIT_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, eps);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_logit_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> eps, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logit_outf(dispatchKeySet, self, eps, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOGIT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, eps);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_sin_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sin_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SIN_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_sinc_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sinc_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SINC_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_sinh_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sinh_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SINH_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_detach(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::detach(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DETACH, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_detach_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::detach_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_DETACH_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

int64_t wrap_size_int(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::__dispatch_size(dispatchKeySet, self, dim);
}

int64_t wrap_size_Dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::size(dispatchKeySet, self, std::move(dim));
}

at::Tensor wrap_slice_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<int64_t> start, c10::optional<int64_t> end, int64_t step) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::slice(dispatchKeySet, self, dim, start, end, step);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SLICE_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, start);trace.append_arg(trace_idx, end);trace.append_arg(trace_idx, step);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_slice_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, at::IntArrayRef input_sizes, int64_t dim, int64_t start, int64_t end, int64_t step) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::slice_backward(dispatchKeySet, grad, std::move(input_sizes), dim, start, end, step);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SLICE_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, std::move(input_sizes));trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, start);trace.append_arg(trace_idx, end);trace.append_arg(trace_idx, step);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap_slogdet(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::slogdet(dispatchKeySet, self);
}

at::Tensor wrap_smm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::smm(dispatchKeySet, self, mat2);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SMM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mat2);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_softmax_int(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::softmax(dispatchKeySet, self, dim, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SOFTMAX_INT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_softmax_Dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::softmax(dispatchKeySet, self, std::move(dim), std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SOFTMAX_DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__softmax(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_softmax(dispatchKeySet, self, dim, half_to_float);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SOFTMAX, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, half_to_float);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__softmax_backward_data(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_softmax_backward_data(dispatchKeySet, grad_output, output, dim, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SOFTMAX_BACKWARD_DATA, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, output);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::vector<at::Tensor> wrap_unsafe_split_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t split_size, int64_t dim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::unsafe_split(dispatchKeySet, self, split_size, dim);
}

std::vector<at::Tensor> wrap_split_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t split_size, int64_t dim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::split(dispatchKeySet, self, split_size, dim);
}

std::vector<at::Tensor> wrap_unsafe_split_with_sizes(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef split_sizes, int64_t dim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::unsafe_split_with_sizes(dispatchKeySet, self, std::move(split_sizes), dim);
}

std::vector<at::Tensor> wrap_split_with_sizes(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef split_sizes, int64_t dim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::split_with_sizes(dispatchKeySet, self, std::move(split_sizes), dim);
}

std::vector<at::Tensor> wrap_hsplit_int(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t sections) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::hsplit(dispatchKeySet, self, sections);
}

std::vector<at::Tensor> wrap_hsplit_array(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef indices) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::hsplit(dispatchKeySet, self, std::move(indices));
}

std::vector<at::Tensor> wrap_vsplit_int(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t sections) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::vsplit(dispatchKeySet, self, sections);
}

std::vector<at::Tensor> wrap_vsplit_array(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef indices) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::vsplit(dispatchKeySet, self, std::move(indices));
}

std::vector<at::Tensor> wrap_dsplit_int(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t sections) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::dsplit(dispatchKeySet, self, sections);
}

std::vector<at::Tensor> wrap_dsplit_array(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef indices) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::dsplit(dispatchKeySet, self, std::move(indices));
}

at::Tensor wrap_squeeze(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::squeeze(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SQUEEZE, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_squeeze_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::squeeze(dispatchKeySet, self, dim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SQUEEZE_DIM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_squeeze_dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::squeeze(dispatchKeySet, self, std::move(dim));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SQUEEZE_DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_squeeze_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::squeeze_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SQUEEZE_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_squeeze__dim(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::squeeze_(dispatchKeySet, self, dim);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SQUEEZE__DIM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_squeeze__dimname(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Dimname dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::squeeze_(dispatchKeySet, self, std::move(dim));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SQUEEZE__DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_sspaddmm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sspaddmm(dispatchKeySet, self, mat1, mat2, beta, alpha);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SSPADDMM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mat1);trace.append_arg(trace_idx, mat2);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, alpha);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_sspaddmm_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sspaddmm_outf(dispatchKeySet, self, mat1, mat2, beta, alpha, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SSPADDMM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mat1);trace.append_arg(trace_idx, mat2);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, alpha);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_stack(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::stack(dispatchKeySet, std::move(tensors), dim);
  }
  auto defaults = compute_dtype(tensors);
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(default_dtype, default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_STACK, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));trace.append_arg(trace_idx, dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_stack_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::stack_outf(dispatchKeySet, std::move(tensors), dim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_STACK_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap__stack(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_stack(dispatchKeySet, std::move(tensors), dim);
  }
  auto defaults = compute_dtype(tensors);
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(default_dtype, default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__STACK, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));trace.append_arg(trace_idx, dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap__stack_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_stack_outf(dispatchKeySet, std::move(tensors), dim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__STACK_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_hstack(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hstack(dispatchKeySet, std::move(tensors));
  }
  auto defaults = compute_dtype(tensors);
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(default_dtype, default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_HSTACK, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_hstack_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hstack_outf(dispatchKeySet, std::move(tensors), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_HSTACK_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_vstack(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::vstack(dispatchKeySet, std::move(tensors));
  }
  auto defaults = compute_dtype(tensors);
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(default_dtype, default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_VSTACK, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_vstack_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::vstack_outf(dispatchKeySet, std::move(tensors), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_VSTACK_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_dstack(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::dstack(dispatchKeySet, std::move(tensors));
  }
  auto defaults = compute_dtype(tensors);
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(default_dtype, default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DSTACK, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_dstack_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::dstack_outf(dispatchKeySet, std::move(tensors), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_DSTACK_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_stft(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const c10::optional<at::Tensor> & window, bool normalized, c10::optional<bool> onesided, c10::optional<bool> return_complex) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::stft(dispatchKeySet, self, n_fft, hop_length, win_length, window, normalized, onesided, return_complex);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_STFT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, n_fft);trace.append_arg(trace_idx, hop_length);trace.append_arg(trace_idx, win_length);trace.append_arg(trace_idx, window);trace.append_arg(trace_idx, normalized);trace.append_arg(trace_idx, onesided);trace.append_arg(trace_idx, return_complex);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_istft(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const c10::optional<at::Tensor> & window, bool center, bool normalized, c10::optional<bool> onesided, c10::optional<int64_t> length, bool return_complex) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::istft(dispatchKeySet, self, n_fft, hop_length, win_length, window, center, normalized, onesided, length, return_complex);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ISTFT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, n_fft);trace.append_arg(trace_idx, hop_length);trace.append_arg(trace_idx, win_length);trace.append_arg(trace_idx, window);trace.append_arg(trace_idx, center);trace.append_arg(trace_idx, normalized);trace.append_arg(trace_idx, onesided);trace.append_arg(trace_idx, length);trace.append_arg(trace_idx, return_complex);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

int64_t wrap_stride_int(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::__dispatch_stride(dispatchKeySet, self, dim);
}

int64_t wrap_stride_Dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::stride(dispatchKeySet, self, std::move(dim));
}

at::Tensor wrap_sum(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sum(dispatchKeySet, self, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SUM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_sum_dim_IntList(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sum(dispatchKeySet, self, std::move(dim), keepdim, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SUM_DIM_INTLIST, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_sum_dim_DimnameList(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sum(dispatchKeySet, self, std::move(dim), keepdim, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SUM_DIM_DIMNAMELIST, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_sum_IntList_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sum_outf(dispatchKeySet, self, std::move(dim), keepdim, std::move(dtype), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SUM_INTLIST_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_sum_DimnameList_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sum_outf(dispatchKeySet, self, std::move(dim), keepdim, std::move(dtype), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SUM_DIMNAMELIST_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_nansum(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nansum(dispatchKeySet, self, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NANSUM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_nansum_dim_IntList(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nansum(dispatchKeySet, self, std::move(dim), keepdim, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NANSUM_DIM_INTLIST, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_nansum_IntList_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nansum_outf(dispatchKeySet, self, std::move(dim), keepdim, std::move(dtype), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NANSUM_INTLIST_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_sum_to_size(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sum_to_size(dispatchKeySet, self, std::move(size));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SUM_TO_SIZE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(size));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_sqrt(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sqrt(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SQRT, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_sqrt_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sqrt_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SQRT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_square(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::square(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SQUARE, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_square_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::square_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SQUARE_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_square_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::square_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SQUARE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_std(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool unbiased) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::std(dispatchKeySet, self, unbiased);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_STD, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, unbiased);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_std_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::std(dispatchKeySet, self, std::move(dim), unbiased, keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_STD_DIM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, unbiased);trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_std_correction(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::std(dispatchKeySet, self, std::move(dim), correction, keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_STD_CORRECTION, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, correction);trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap_std_mean(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool unbiased) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::std_mean(dispatchKeySet, self, unbiased);
}

std::tuple<at::Tensor,at::Tensor> wrap_std_mean_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::std_mean(dispatchKeySet, self, std::move(dim), unbiased, keepdim);
}

std::tuple<at::Tensor,at::Tensor> wrap_std_mean_correction(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::std_mean(dispatchKeySet, self, std::move(dim), correction, keepdim);
}

std::tuple<at::Tensor,at::Tensor> wrap_std_mean_names_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::std_mean(dispatchKeySet, self, std::move(dim), unbiased, keepdim);
}

std::tuple<at::Tensor,at::Tensor> wrap_std_mean_correction_names(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::std_mean(dispatchKeySet, self, std::move(dim), correction, keepdim);
}

at::Tensor & wrap_std_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::std_outf(dispatchKeySet, self, std::move(dim), unbiased, keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_STD_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, unbiased);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_std_correction_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::std_outf(dispatchKeySet, self, std::move(dim), correction, keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_STD_CORRECTION_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, correction);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_std_names_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::std(dispatchKeySet, self, std::move(dim), unbiased, keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_STD_NAMES_DIM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, unbiased);trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_std_names_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::std_outf(dispatchKeySet, self, std::move(dim), unbiased, keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_STD_NAMES_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, unbiased);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_std_correction_names(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::std(dispatchKeySet, self, std::move(dim), correction, keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_STD_CORRECTION_NAMES, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, correction);trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_std_correction_names_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::std_outf(dispatchKeySet, self, std::move(dim), correction, keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_STD_CORRECTION_NAMES_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, correction);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_prod(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::prod(dispatchKeySet, self, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_PROD, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_prod_dim_int(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::prod(dispatchKeySet, self, dim, keepdim, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_PROD_DIM_INT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_prod_int_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::prod_outf(dispatchKeySet, self, dim, keepdim, std::move(dtype), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_PROD_INT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_prod_dim_Dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::prod(dispatchKeySet, self, std::move(dim), keepdim, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_PROD_DIM_DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_prod_Dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::prod_outf(dispatchKeySet, self, std::move(dim), keepdim, std::move(dtype), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_PROD_DIMNAME_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_t(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::t(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_T, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_t_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::t_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_T_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_tan_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::tan_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_TAN_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_tanh(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::tanh(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TANH, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_tanh_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::tanh_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_TANH_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_tanh_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::tanh_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_TANH_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_tensordot(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::IntArrayRef dims_self, at::IntArrayRef dims_other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::tensordot(dispatchKeySet, self, other, std::move(dims_self), std::move(dims_other));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TENSORDOT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, std::move(dims_self));trace.append_arg(trace_idx, std::move(dims_other));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_tensordot_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::IntArrayRef dims_self, at::IntArrayRef dims_other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::tensordot_outf(dispatchKeySet, self, other, std::move(dims_self), std::move(dims_other), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_TENSORDOT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, std::move(dims_self));trace.append_arg(trace_idx, std::move(dims_other));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_threshold(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::threshold(dispatchKeySet, self, threshold, value);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_THRESHOLD, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, threshold);trace.append_arg(trace_idx, value);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_threshold_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::threshold_outf(dispatchKeySet, self, threshold, value, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_THRESHOLD_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, threshold);trace.append_arg(trace_idx, value);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_threshold_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::threshold_backward_outf(dispatchKeySet, grad_output, self, threshold, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_THRESHOLD_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, threshold);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_threshold_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::threshold_backward(dispatchKeySet, grad_output, self, threshold);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_THRESHOLD_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, threshold);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_tile(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dims) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::tile(dispatchKeySet, self, std::move(dims));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TILE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dims));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_transpose_int(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim0, int64_t dim1) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::transpose(dispatchKeySet, self, dim0, dim1);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TRANSPOSE_INT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim0);trace.append_arg(trace_idx, dim1);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_transpose_Dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim0, at::Dimname dim1) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::transpose(dispatchKeySet, self, std::move(dim0), std::move(dim1));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TRANSPOSE_DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim0));trace.append_arg(trace_idx, std::move(dim1));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__mkldnn_transpose(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim0, int64_t dim1) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_mkldnn_transpose(dispatchKeySet, self, dim0, dim1);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__MKLDNN_TRANSPOSE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim0);trace.append_arg(trace_idx, dim1);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_transpose_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim0, int64_t dim1) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::transpose_(dispatchKeySet, self, dim0, dim1);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_TRANSPOSE_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim0);trace.append_arg(trace_idx, dim1);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap__mkldnn_transpose_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim0, int64_t dim1) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_mkldnn_transpose_(dispatchKeySet, self, dim0, dim1);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__MKLDNN_TRANSPOSE_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim0);trace.append_arg(trace_idx, dim1);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_one_hot(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t num_classes, at::ScalarType dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::one_hot(dispatchKeySet, self, num_classes, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(dtype), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ONE_HOT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, num_classes);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_flip(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dims) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::flip(dispatchKeySet, self, std::move(dims));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FLIP, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dims));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_fliplr(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fliplr(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FLIPLR, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_flipud(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::flipud(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FLIPUD, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_roll(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef shifts, at::IntArrayRef dims) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::roll(dispatchKeySet, self, std::move(shifts), std::move(dims));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ROLL, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(shifts));trace.append_arg(trace_idx, std::move(dims));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_rot90(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t k, at::IntArrayRef dims) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rot90(dispatchKeySet, self, k, std::move(dims));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ROT90, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, k);trace.append_arg(trace_idx, std::move(dims));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_trapz_x(c10::DispatchKeySet dispatchKeySet, const at::Tensor & y, const at::Tensor & x, int64_t dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::trapz(dispatchKeySet, y, x, dim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(y.dtype(), y.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TRAPZ_X, dispatchKeySet);
  trace.append_arg(trace_idx, y);trace.append_arg(trace_idx, x);trace.append_arg(trace_idx, dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_trapz_dx(c10::DispatchKeySet dispatchKeySet, const at::Tensor & y, double dx, int64_t dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::trapz(dispatchKeySet, y, dx, dim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(y.dtype(), y.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TRAPZ_DX, dispatchKeySet);
  trace.append_arg(trace_idx, y);trace.append_arg(trace_idx, dx);trace.append_arg(trace_idx, dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__trilinear(c10::DispatchKeySet dispatchKeySet, const at::Tensor & i1, const at::Tensor & i2, const at::Tensor & i3, at::IntArrayRef expand1, at::IntArrayRef expand2, at::IntArrayRef expand3, at::IntArrayRef sumdim, int64_t unroll_dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_trilinear(dispatchKeySet, i1, i2, i3, std::move(expand1), std::move(expand2), std::move(expand3), std::move(sumdim), unroll_dim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(i1.dtype(), i1.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__TRILINEAR, dispatchKeySet);
  trace.append_arg(trace_idx, i1);trace.append_arg(trace_idx, i2);trace.append_arg(trace_idx, i3);trace.append_arg(trace_idx, std::move(expand1));trace.append_arg(trace_idx, std::move(expand2));trace.append_arg(trace_idx, std::move(expand3));trace.append_arg(trace_idx, std::move(sumdim));trace.append_arg(trace_idx, unroll_dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_triplet_margin_loss(c10::DispatchKeySet dispatchKeySet, const at::Tensor & anchor, const at::Tensor & positive, const at::Tensor & negative, double margin, double p, double eps, bool swap, int64_t reduction) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::triplet_margin_loss(dispatchKeySet, anchor, positive, negative, margin, p, eps, swap, reduction);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(anchor.dtype(), anchor.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TRIPLET_MARGIN_LOSS, dispatchKeySet);
  trace.append_arg(trace_idx, anchor);trace.append_arg(trace_idx, positive);trace.append_arg(trace_idx, negative);trace.append_arg(trace_idx, margin);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, eps);trace.append_arg(trace_idx, swap);trace.append_arg(trace_idx, reduction);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_trunc(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::trunc(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TRUNC, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_trunc_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::trunc_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_TRUNC_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_trunc_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::trunc_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_TRUNC_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_fix(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fix(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FIX, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fix_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fix_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FIX_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_fix_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fix_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FIX_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_type_as(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::type_as(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TYPE_AS, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

bool wrap__has_compatible_shallow_copy_type(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & from) {
  ensure_materialized(self);ensure_materialized(from);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_has_compatible_shallow_copy_type(dispatchKeySet, self, from);
}

std::tuple<at::Tensor,at::Tensor> wrap__unique(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool sorted, bool return_inverse) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_unique(dispatchKeySet, self, sorted, return_inverse);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_unique_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool sorted, bool return_inverse, bool return_counts) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::unique_dim(dispatchKeySet, self, dim, sorted, return_inverse, return_counts);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_unique_consecutive(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool return_inverse, bool return_counts, c10::optional<int64_t> dim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::unique_consecutive(dispatchKeySet, self, return_inverse, return_counts, dim);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_unique_dim_consecutive(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool return_inverse, bool return_counts) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::unique_dim_consecutive(dispatchKeySet, self, dim, return_inverse, return_counts);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap__unique2(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool sorted, bool return_inverse, bool return_counts) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_unique2(dispatchKeySet, self, sorted, return_inverse, return_counts);
}

at::Tensor wrap__unsafe_view(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_unsafe_view(dispatchKeySet, self, std::move(size));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__UNSAFE_VIEW, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(size));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_unsqueeze(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::unsqueeze(dispatchKeySet, self, dim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_UNSQUEEZE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_unsqueeze_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::unsqueeze_(dispatchKeySet, self, dim);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_UNSQUEEZE_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_vander(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, c10::optional<int64_t> N, bool increasing) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::vander(dispatchKeySet, x, N, increasing);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(x.dtype(), x.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_VANDER, dispatchKeySet);
  trace.append_arg(trace_idx, x);trace.append_arg(trace_idx, N);trace.append_arg(trace_idx, increasing);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_var(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool unbiased) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::var(dispatchKeySet, self, unbiased);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_VAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, unbiased);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_var_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::var(dispatchKeySet, self, std::move(dim), unbiased, keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_VAR_DIM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, unbiased);trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_var_correction(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::var(dispatchKeySet, self, std::move(dim), correction, keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_VAR_CORRECTION, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, correction);trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_var_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::var_outf(dispatchKeySet, self, std::move(dim), unbiased, keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_VAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, unbiased);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_var_correction_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::var_outf(dispatchKeySet, self, std::move(dim), correction, keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_VAR_CORRECTION_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, correction);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_var_names_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::var(dispatchKeySet, self, std::move(dim), unbiased, keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_VAR_NAMES_DIM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, unbiased);trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_var_names_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::var_outf(dispatchKeySet, self, std::move(dim), unbiased, keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_VAR_NAMES_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, unbiased);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_var_correction_names(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::var(dispatchKeySet, self, std::move(dim), correction, keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_VAR_CORRECTION_NAMES, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, correction);trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_var_correction_names_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::var_outf(dispatchKeySet, self, std::move(dim), correction, keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_VAR_CORRECTION_NAMES_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, correction);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

std::tuple<at::Tensor,at::Tensor> wrap_var_mean(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool unbiased) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::var_mean(dispatchKeySet, self, unbiased);
}

std::tuple<at::Tensor,at::Tensor> wrap_var_mean_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::var_mean(dispatchKeySet, self, std::move(dim), unbiased, keepdim);
}

std::tuple<at::Tensor,at::Tensor> wrap_var_mean_correction(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::var_mean(dispatchKeySet, self, std::move(dim), correction, keepdim);
}

std::tuple<at::Tensor,at::Tensor> wrap_var_mean_names_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::var_mean(dispatchKeySet, self, std::move(dim), unbiased, keepdim);
}

std::tuple<at::Tensor,at::Tensor> wrap_var_mean_correction_names(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::var_mean(dispatchKeySet, self, std::move(dim), correction, keepdim);
}

at::Tensor wrap_view_as(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::view_as(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_VIEW_AS, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_where_self(c10::DispatchKeySet dispatchKeySet, const at::Tensor & condition, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::where(dispatchKeySet, condition, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(condition.dtype(), condition.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_WHERE_SELF, dispatchKeySet);
  trace.append_arg(trace_idx, condition);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_where_ScalarSelf(c10::DispatchKeySet dispatchKeySet, const at::Tensor & condition, const at::Scalar & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::where(dispatchKeySet, condition, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(condition.dtype(), condition.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_WHERE_SCALARSELF, dispatchKeySet);
  trace.append_arg(trace_idx, condition);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_where_ScalarOther(c10::DispatchKeySet dispatchKeySet, const at::Tensor & condition, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::where(dispatchKeySet, condition, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(condition.dtype(), condition.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_WHERE_SCALAROTHER, dispatchKeySet);
  trace.append_arg(trace_idx, condition);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_where_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & condition, const at::Scalar & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::where(dispatchKeySet, condition, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(condition.dtype(), condition.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_WHERE_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, condition);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::vector<at::Tensor> wrap_where(c10::DispatchKeySet dispatchKeySet, const at::Tensor & condition) {
  ensure_materialized(condition);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::where(dispatchKeySet, condition);
}

at::Tensor wrap__s_where(c10::DispatchKeySet dispatchKeySet, const at::Tensor & condition, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_s_where(dispatchKeySet, condition, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(condition.dtype(), condition.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__S_WHERE, dispatchKeySet);
  trace.append_arg(trace_idx, condition);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_norm_except_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & v, int64_t pow, int64_t dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::norm_except_dim(dispatchKeySet, v, pow, dim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(v.dtype(), v.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NORM_EXCEPT_DIM, dispatchKeySet);
  trace.append_arg(trace_idx, v);trace.append_arg(trace_idx, pow);trace.append_arg(trace_idx, dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__weight_norm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & v, const at::Tensor & g, int64_t dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_weight_norm(dispatchKeySet, v, g, dim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(v.dtype(), v.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__WEIGHT_NORM, dispatchKeySet);
  trace.append_arg(trace_idx, v);trace.append_arg(trace_idx, g);trace.append_arg(trace_idx, dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap__weight_norm_cuda_interface(c10::DispatchKeySet dispatchKeySet, const at::Tensor & v, const at::Tensor & g, int64_t dim) {
  ensure_materialized(v);ensure_materialized(g);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_weight_norm_cuda_interface(dispatchKeySet, v, g, dim);
}

std::tuple<at::Tensor,at::Tensor> wrap__weight_norm_cuda_interface_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_w, const at::Tensor & saved_v, const at::Tensor & saved_g, const at::Tensor & saved_norms, int64_t dim) {
  ensure_materialized(grad_w);ensure_materialized(saved_v);ensure_materialized(saved_g);ensure_materialized(saved_norms);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_weight_norm_cuda_interface_backward(dispatchKeySet, grad_w, saved_v, saved_g, saved_norms, dim);
}

std::tuple<at::Tensor,at::Tensor> wrap__weight_norm_differentiable_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_w, const at::Tensor & saved_v, const at::Tensor & saved_g, const at::Tensor & saved_norms, int64_t dim) {
  ensure_materialized(grad_w);ensure_materialized(saved_v);ensure_materialized(saved_g);ensure_materialized(saved_norms);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_weight_norm_differentiable_backward(dispatchKeySet, grad_w, saved_v, saved_g, saved_norms, dim);
}

at::Tensor wrap_zeros_names(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::zeros(dispatchKeySet, std::move(size), std::move(names), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ZEROS_NAMES, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(names));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_zeros(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::zeros(dispatchKeySet, std::move(size), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ZEROS, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_zeros_out(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::zeros_outf(dispatchKeySet, std::move(size), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ZEROS_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_zeros_like(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::zeros_like(dispatchKeySet, self, std::move(dtype), std::move(layout), std::move(device), pin_memory, std::move(memory_format));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), device ? *device : self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ZEROS_LIKE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);trace.append_arg(trace_idx, std::move(memory_format));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__standard_gamma_grad(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & output) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_standard_gamma_grad(dispatchKeySet, self, output);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__STANDARD_GAMMA_GRAD, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, output);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__standard_gamma(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_standard_gamma(dispatchKeySet, self, std::move(generator));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__STANDARD_GAMMA, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(generator));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__dirichlet_grad(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & alpha, const at::Tensor & total) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_dirichlet_grad(dispatchKeySet, x, alpha, total);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(x.dtype(), x.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__DIRICHLET_GRAD, dispatchKeySet);
  trace.append_arg(trace_idx, x);trace.append_arg(trace_idx, alpha);trace.append_arg(trace_idx, total);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__sample_dirichlet(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sample_dirichlet(dispatchKeySet, self, std::move(generator));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SAMPLE_DIRICHLET, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(generator));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_poisson(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::poisson(dispatchKeySet, self, std::move(generator));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_POISSON, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(generator));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_binomial(c10::DispatchKeySet dispatchKeySet, const at::Tensor & count, const at::Tensor & prob, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::binomial(dispatchKeySet, count, prob, std::move(generator));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(count.dtype(), count.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BINOMIAL, dispatchKeySet);
  trace.append_arg(trace_idx, count);trace.append_arg(trace_idx, prob);trace.append_arg(trace_idx, std::move(generator));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_native_norm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & p) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::native_norm(dispatchKeySet, self, p);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NATIVE_NORM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_native_norm_ScalarOpt_dim_dtype(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::native_norm(dispatchKeySet, self, p, std::move(dim), keepdim, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NATIVE_NORM_SCALAROPT_DIM_DTYPE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__sparse_sum(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sparse_sum(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SPARSE_SUM, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__sparse_sum_dtype(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::ScalarType dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sparse_sum(dispatchKeySet, self, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(dtype), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SPARSE_SUM_DTYPE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__sparse_sum_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sparse_sum(dispatchKeySet, self, std::move(dim));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SPARSE_SUM_DIM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__sparse_sum_dim_dtype(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, at::ScalarType dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sparse_sum(dispatchKeySet, self, std::move(dim), std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(dtype), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SPARSE_SUM_DIM_DTYPE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__sparse_sum_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self, at::IntArrayRef dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sparse_sum_backward(dispatchKeySet, grad, self, std::move(dim));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SPARSE_SUM_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__sparse_softmax_int(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sparse_softmax(dispatchKeySet, self, dim, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SPARSE_SOFTMAX_INT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__sparse_softmax_Dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sparse_softmax(dispatchKeySet, self, std::move(dim), std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SPARSE_SOFTMAX_DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__sparse_softmax(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sparse_softmax(dispatchKeySet, self, dim, half_to_float);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SPARSE_SOFTMAX, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, half_to_float);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__sparse_softmax_backward_data(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sparse_softmax_backward_data(dispatchKeySet, grad_output, output, dim, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SPARSE_SOFTMAX_BACKWARD_DATA, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, output);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__sparse_log_softmax_int(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sparse_log_softmax(dispatchKeySet, self, dim, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SPARSE_LOG_SOFTMAX_INT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__sparse_log_softmax_Dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sparse_log_softmax(dispatchKeySet, self, std::move(dim), std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SPARSE_LOG_SOFTMAX_DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__sparse_log_softmax(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sparse_log_softmax(dispatchKeySet, self, dim, half_to_float);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SPARSE_LOG_SOFTMAX, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, half_to_float);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__sparse_log_softmax_backward_data(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sparse_log_softmax_backward_data(dispatchKeySet, grad_output, output, dim, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SPARSE_LOG_SOFTMAX_BACKWARD_DATA, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, output);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_norm_ScalarOpt_dtype(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::ScalarType dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::norm(dispatchKeySet, self, p, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(dtype), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NORM_SCALAROPT_DTYPE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_norm_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & p) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::norm(dispatchKeySet, self, p);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NORM_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_norm_ScalarOpt_dim_dtype(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::ScalarType dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::norm(dispatchKeySet, self, p, std::move(dim), keepdim, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(dtype), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NORM_SCALAROPT_DIM_DTYPE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_norm_ScalarOpt_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::norm(dispatchKeySet, self, p, std::move(dim), keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NORM_SCALAROPT_DIM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_norm_dtype_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::ScalarType dtype, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::norm_outf(dispatchKeySet, self, p, std::move(dim), keepdim, std::move(dtype), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NORM_DTYPE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_norm_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::norm_outf(dispatchKeySet, self, p, std::move(dim), keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NORM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_norm_names_ScalarOpt_dim_dtype(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::ScalarType dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::norm(dispatchKeySet, self, p, std::move(dim), keepdim, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(dtype), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NORM_NAMES_SCALAROPT_DIM_DTYPE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_norm_names_ScalarOpt_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::norm(dispatchKeySet, self, p, std::move(dim), keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NORM_NAMES_SCALAROPT_DIM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_norm_names_dtype_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::ScalarType dtype, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::norm_outf(dispatchKeySet, self, p, std::move(dim), keepdim, std::move(dtype), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NORM_NAMES_DTYPE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_norm_names_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::norm_outf(dispatchKeySet, self, p, std::move(dim), keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NORM_NAMES_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

std::tuple<at::Tensor,at::Tensor> wrap_frexp_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::frexp(dispatchKeySet, self);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_frexp_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & mantissa, at::Tensor & exponent) {
  ensure_materialized(self);ensure_materialized(mantissa);ensure_materialized(exponent);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::frexp_outf(dispatchKeySet, self, mantissa, exponent);
}

at::Tensor wrap_frobenius_norm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::frobenius_norm(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FROBENIUS_NORM, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_frobenius_norm_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::frobenius_norm(dispatchKeySet, self, std::move(dim), keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FROBENIUS_NORM_DIM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_frobenius_norm_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::frobenius_norm_outf(dispatchKeySet, self, std::move(dim), keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FROBENIUS_NORM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_nuclear_norm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nuclear_norm(dispatchKeySet, self, keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NUCLEAR_NORM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_nuclear_norm_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nuclear_norm_outf(dispatchKeySet, self, keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NUCLEAR_NORM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_nuclear_norm_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nuclear_norm(dispatchKeySet, self, std::move(dim), keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NUCLEAR_NORM_DIM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_nuclear_norm_dim_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nuclear_norm_outf(dispatchKeySet, self, std::move(dim), keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NUCLEAR_NORM_DIM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_clone(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::clone(dispatchKeySet, self, std::move(memory_format));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CLONE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(memory_format));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_positive(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::positive(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_POSITIVE, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

const at::Tensor & wrap_resize_as_(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & the_template, c10::optional<at::MemoryFormat> memory_format) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::resize_as_(dispatchKeySet, self, the_template, std::move(memory_format));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RESIZE_AS_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, the_template);trace.append_arg(trace_idx, std::move(memory_format));
  finish_in_place(tt, trace_idx);
  return self;
}

const at::Tensor & wrap_resize_as_sparse_(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & the_template) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::resize_as_sparse_(dispatchKeySet, self, the_template);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RESIZE_AS_SPARSE_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, the_template);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_zero_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::zero_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ZERO_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_sub_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sub_outf(dispatchKeySet, self, other, alpha, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SUB_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, alpha);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_sub_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sub(dispatchKeySet, self, other, alpha);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SUB_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, alpha);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_sub__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sub_(dispatchKeySet, self, other, alpha);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SUB__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, alpha);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_sub_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sub(dispatchKeySet, self, other, alpha);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SUB_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, alpha);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_sub__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sub_(dispatchKeySet, self, other, alpha);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SUB__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, alpha);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_subtract_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::subtract_outf(dispatchKeySet, self, other, alpha, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SUBTRACT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, alpha);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_subtract_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::subtract(dispatchKeySet, self, other, alpha);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SUBTRACT_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, alpha);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_subtract__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::subtract_(dispatchKeySet, self, other, alpha);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SUBTRACT__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, alpha);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_subtract_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::subtract(dispatchKeySet, self, other, alpha);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SUBTRACT_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, alpha);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_subtract__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::subtract_(dispatchKeySet, self, other, alpha);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SUBTRACT__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, alpha);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_rsub_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rsub(dispatchKeySet, self, other, alpha);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RSUB_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, alpha);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_heaviside_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & values, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::heaviside_outf(dispatchKeySet, self, values, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_HEAVISIDE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, values);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_rsub_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rsub(dispatchKeySet, self, other, alpha);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RSUB_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, alpha);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__sparse_addmm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & sparse, const at::Tensor & dense, const at::Scalar & beta, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sparse_addmm(dispatchKeySet, self, sparse, dense, beta, alpha);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SPARSE_ADDMM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, sparse);trace.append_arg(trace_idx, dense);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, alpha);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_addmm_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::addmm_outf(dispatchKeySet, self, mat1, mat2, beta, alpha, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ADDMM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mat1);trace.append_arg(trace_idx, mat2);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, alpha);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_addmm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::addmm(dispatchKeySet, self, mat1, mat2, beta, alpha);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ADDMM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mat1);trace.append_arg(trace_idx, mat2);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, alpha);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_addmm_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::addmm_(dispatchKeySet, self, mat1, mat2, beta, alpha);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ADDMM_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mat1);trace.append_arg(trace_idx, mat2);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, alpha);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_sparse_csr_tensor_crow_col_value_size(c10::DispatchKeySet dispatchKeySet, const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sparse_csr_tensor(dispatchKeySet, crow_indices, col_indices, values, std::move(size), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : crow_indices.dtype(), device ? *device : crow_indices.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SPARSE_CSR_TENSOR_CROW_COL_VALUE_SIZE, dispatchKeySet);
  trace.append_arg(trace_idx, crow_indices);trace.append_arg(trace_idx, col_indices);trace.append_arg(trace_idx, values);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_sparse_csr_tensor_crow_col_value(c10::DispatchKeySet dispatchKeySet, const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sparse_csr_tensor(dispatchKeySet, crow_indices, col_indices, values, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : crow_indices.dtype(), device ? *device : crow_indices.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SPARSE_CSR_TENSOR_CROW_COL_VALUE, dispatchKeySet);
  trace.append_arg(trace_idx, crow_indices);trace.append_arg(trace_idx, col_indices);trace.append_arg(trace_idx, values);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__sparse_csr_tensor_unsafe(c10::DispatchKeySet dispatchKeySet, const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sparse_csr_tensor_unsafe(dispatchKeySet, crow_indices, col_indices, values, std::move(size), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : crow_indices.dtype(), device ? *device : crow_indices.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SPARSE_CSR_TENSOR_UNSAFE, dispatchKeySet);
  trace.append_arg(trace_idx, crow_indices);trace.append_arg(trace_idx, col_indices);trace.append_arg(trace_idx, values);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_sparse_coo_tensor_size(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sparse_coo_tensor(dispatchKeySet, std::move(size), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SPARSE_COO_TENSOR_SIZE, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_sparse_coo_tensor_indices(c10::DispatchKeySet dispatchKeySet, const at::Tensor & indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sparse_coo_tensor(dispatchKeySet, indices, values, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : indices.dtype(), device ? *device : indices.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SPARSE_COO_TENSOR_INDICES, dispatchKeySet);
  trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, values);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_sparse_coo_tensor_indices_size(c10::DispatchKeySet dispatchKeySet, const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sparse_coo_tensor(dispatchKeySet, indices, values, std::move(size), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : indices.dtype(), device ? *device : indices.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SPARSE_COO_TENSOR_INDICES_SIZE, dispatchKeySet);
  trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, values);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__sparse_coo_tensor_unsafe(c10::DispatchKeySet dispatchKeySet, const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sparse_coo_tensor_unsafe(dispatchKeySet, indices, values, std::move(size), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : indices.dtype(), device ? *device : indices.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SPARSE_COO_TENSOR_UNSAFE, dispatchKeySet);
  trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, values);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

void wrap__validate_sparse_coo_tensor_args(c10::DispatchKeySet dispatchKeySet, const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size) {
  ensure_materialized(indices);ensure_materialized(values);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_validate_sparse_coo_tensor_args(dispatchKeySet, indices, values, std::move(size));
}

void wrap__validate_sparse_csr_tensor_args(c10::DispatchKeySet dispatchKeySet, const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size) {
  ensure_materialized(crow_indices);ensure_materialized(col_indices);ensure_materialized(values);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_validate_sparse_csr_tensor_args(dispatchKeySet, crow_indices, col_indices, values, std::move(size));
}

at::Tensor wrap__sparse_coo_tensor_with_dims(c10::DispatchKeySet dispatchKeySet, int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sparse_coo_tensor_with_dims(dispatchKeySet, sparse_dim, dense_dim, std::move(size), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SPARSE_COO_TENSOR_WITH_DIMS, dispatchKeySet);
  trace.append_arg(trace_idx, sparse_dim);trace.append_arg(trace_idx, dense_dim);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__sparse_coo_tensor_with_dims_and_tensors(c10::DispatchKeySet dispatchKeySet, int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, const at::Tensor & indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_sparse_coo_tensor_with_dims_and_tensors(dispatchKeySet, sparse_dim, dense_dim, std::move(size), indices, values, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : indices.dtype(), device ? *device : indices.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SPARSE_COO_TENSOR_WITH_DIMS_AND_TENSORS, dispatchKeySet);
  trace.append_arg(trace_idx, sparse_dim);trace.append_arg(trace_idx, dense_dim);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, values);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

const at::Tensor & wrap_sparse_resize_(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sparse_resize_(dispatchKeySet, self, std::move(size), sparse_dim, dense_dim);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SPARSE_RESIZE_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, sparse_dim);trace.append_arg(trace_idx, dense_dim);
  finish_in_place(tt, trace_idx);
  return self;
}

const at::Tensor & wrap_sparse_resize_and_clear_(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sparse_resize_and_clear_(dispatchKeySet, self, std::move(size), sparse_dim, dense_dim);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SPARSE_RESIZE_AND_CLEAR_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, sparse_dim);trace.append_arg(trace_idx, dense_dim);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_sparse_mask(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sparse_mask(dispatchKeySet, self, mask);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SPARSE_MASK, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mask);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::vector<at::Tensor> wrap__to_cpu(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_to_cpu(dispatchKeySet, std::move(tensors));
}

at::Tensor wrap_to_dense(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::to_dense(dispatchKeySet, self, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TO_DENSE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_to_dense_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::to_dense_backward(dispatchKeySet, grad, input);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TO_DENSE_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, input);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

int64_t wrap_sparse_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::sparse_dim(dispatchKeySet, self);
}

int64_t wrap__dimI(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_dimI(dispatchKeySet, self);
}

int64_t wrap_dense_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::dense_dim(dispatchKeySet, self);
}

int64_t wrap__dimV(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_dimV(dispatchKeySet, self);
}

int64_t wrap__nnz(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_nnz(dispatchKeySet, self);
}

at::Tensor wrap_coalesce(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::coalesce(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_COALESCE, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__coalesce(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_coalesce(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__COALESCE, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

bool wrap_is_coalesced(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::is_coalesced(dispatchKeySet, self);
}

at::Tensor wrap__indices(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_indices(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__INDICES, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__values(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_values(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__VALUES, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap__coalesced_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, bool coalesced) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_coalesced_(dispatchKeySet, self, coalesced);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__COALESCED_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, coalesced);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_indices(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::indices(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_INDICES, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_values(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::values(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_VALUES, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_crow_indices(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::crow_indices(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CROW_INDICES, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_col_indices(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::col_indices(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_COL_INDICES, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_hspmm_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & mat1, const at::Tensor & mat2, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hspmm_outf(dispatchKeySet, mat1, mat2, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_HSPMM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, mat1);trace.append_arg(trace_idx, mat2);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_hspmm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & mat1, const at::Tensor & mat2) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hspmm(dispatchKeySet, mat1, mat2);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(mat1.dtype(), mat1.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_HSPMM, dispatchKeySet);
  trace.append_arg(trace_idx, mat1);trace.append_arg(trace_idx, mat2);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_copy_sparse_to_sparse_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & src, bool non_blocking) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::copy_sparse_to_sparse_(dispatchKeySet, self, src, non_blocking);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_COPY_SPARSE_TO_SPARSE_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, src);trace.append_arg(trace_idx, non_blocking);
  finish_in_place(tt, trace_idx);
  return self;
}

std::vector<at::Tensor> wrap_unbind_int(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::unbind(dispatchKeySet, self, dim);
}

std::vector<at::Tensor> wrap_unbind_Dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::unbind(dispatchKeySet, self, std::move(dim));
}

at::Tensor wrap_to_sparse_sparse_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t sparse_dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::to_sparse(dispatchKeySet, self, sparse_dim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TO_SPARSE_SPARSE_DIM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, sparse_dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_to_sparse(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::to_sparse(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TO_SPARSE, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_to_mkldnn(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::to_mkldnn(dispatchKeySet, self, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TO_MKLDNN, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_mkldnn_reorder_conv2d_weight(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mkldnn_reorder_conv2d_weight(dispatchKeySet, self, std::move(padding), std::move(stride), std::move(dilation), groups);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MKLDNN_REORDER_CONV2D_WEIGHT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_mkldnn_reorder_conv3d_weight(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mkldnn_reorder_conv3d_weight(dispatchKeySet, self, std::move(padding), std::move(stride), std::move(dilation), groups);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MKLDNN_REORDER_CONV3D_WEIGHT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, groups);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_to_mkldnn_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::to_mkldnn_backward(dispatchKeySet, grad, input);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TO_MKLDNN_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, input);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_quantize_per_tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double scale, int64_t zero_point, at::ScalarType dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::quantize_per_tensor(dispatchKeySet, self, scale, zero_point, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(dtype), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_QUANTIZE_PER_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, scale);trace.append_arg(trace_idx, zero_point);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::vector<at::Tensor> wrap_quantize_per_tensor_tensors(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, const at::Tensor & scales, const at::Tensor & zero_points, at::ScalarType dtype) {
  ensure_materialized(tensors);ensure_materialized(scales);ensure_materialized(zero_points);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::quantize_per_tensor(dispatchKeySet, std::move(tensors), scales, zero_points, std::move(dtype));
}

at::Tensor wrap_quantize_per_channel(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, at::ScalarType dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::quantize_per_channel(dispatchKeySet, self, scales, zero_points, axis, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(dtype), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_QUANTIZE_PER_CHANNEL, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, scales);trace.append_arg(trace_idx, zero_points);trace.append_arg(trace_idx, axis);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_dequantize_self(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::dequantize(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DEQUANTIZE_SELF, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::vector<at::Tensor> wrap_dequantize_tensors(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::dequantize(dispatchKeySet, std::move(tensors));
}

double wrap_q_scale(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::q_scale(dispatchKeySet, self);
}

int64_t wrap_q_zero_point(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::q_zero_point(dispatchKeySet, self);
}

at::Tensor wrap_q_per_channel_scales(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::q_per_channel_scales(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_Q_PER_CHANNEL_SCALES, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_q_per_channel_zero_points(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::q_per_channel_zero_points(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_Q_PER_CHANNEL_ZERO_POINTS, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

int64_t wrap_q_per_channel_axis(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::q_per_channel_axis(dispatchKeySet, self);
}

at::Tensor wrap_int_repr(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::int_repr(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_INT_REPR, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__make_per_tensor_quantized_tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double scale, int64_t zero_point) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_make_per_tensor_quantized_tensor(dispatchKeySet, self, scale, zero_point);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__MAKE_PER_TENSOR_QUANTIZED_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, scale);trace.append_arg(trace_idx, zero_point);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__make_per_channel_quantized_tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_make_per_channel_quantized_tensor(dispatchKeySet, self, scale, zero_point, axis);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__MAKE_PER_CHANNEL_QUANTIZED_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, scale);trace.append_arg(trace_idx, zero_point);trace.append_arg(trace_idx, axis);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::QScheme wrap_qscheme(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::qscheme(dispatchKeySet, self);
}

at::Tensor wrap_fake_quantize_per_tensor_affine(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fake_quantize_per_tensor_affine(dispatchKeySet, self, scale, zero_point, quant_min, quant_max);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FAKE_QUANTIZE_PER_TENSOR_AFFINE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, scale);trace.append_arg(trace_idx, zero_point);trace.append_arg(trace_idx, quant_min);trace.append_arg(trace_idx, quant_max);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap_fake_quantize_per_tensor_affine_cachemask(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::fake_quantize_per_tensor_affine_cachemask(dispatchKeySet, self, scale, zero_point, quant_min, quant_max);
}

at::Tensor wrap_fake_quantize_per_tensor_affine_cachemask_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & mask) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fake_quantize_per_tensor_affine_cachemask_backward(dispatchKeySet, grad, mask);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FAKE_QUANTIZE_PER_TENSOR_AFFINE_CACHEMASK_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, mask);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__fake_quantize_learnable_per_tensor_affine(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t quant_min, int64_t quant_max, double grad_factor) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_fake_quantize_learnable_per_tensor_affine(dispatchKeySet, self, scale, zero_point, quant_min, quant_max, grad_factor);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__FAKE_QUANTIZE_LEARNABLE_PER_TENSOR_AFFINE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, scale);trace.append_arg(trace_idx, zero_point);trace.append_arg(trace_idx, quant_min);trace.append_arg(trace_idx, quant_max);trace.append_arg(trace_idx, grad_factor);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap__fake_quantize_learnable_per_tensor_affine_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t quant_min, int64_t quant_max, double grad_factor) {
  ensure_materialized(grad);ensure_materialized(self);ensure_materialized(scale);ensure_materialized(zero_point);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_fake_quantize_learnable_per_tensor_affine_backward(dispatchKeySet, grad, self, scale, zero_point, quant_min, quant_max, grad_factor);
}

at::Tensor wrap_fake_quantize_per_channel_affine(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fake_quantize_per_channel_affine(dispatchKeySet, self, scale, zero_point, axis, quant_min, quant_max);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FAKE_QUANTIZE_PER_CHANNEL_AFFINE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, scale);trace.append_arg(trace_idx, zero_point);trace.append_arg(trace_idx, axis);trace.append_arg(trace_idx, quant_min);trace.append_arg(trace_idx, quant_max);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap_fake_quantize_per_channel_affine_cachemask(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max) {
  ensure_materialized(self);ensure_materialized(scale);ensure_materialized(zero_point);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::fake_quantize_per_channel_affine_cachemask(dispatchKeySet, self, scale, zero_point, axis, quant_min, quant_max);
}

at::Tensor wrap_fake_quantize_per_channel_affine_cachemask_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & mask) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fake_quantize_per_channel_affine_cachemask_backward(dispatchKeySet, grad, mask);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FAKE_QUANTIZE_PER_CHANNEL_AFFINE_CACHEMASK_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, mask);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__fake_quantize_learnable_per_channel_affine(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_fake_quantize_learnable_per_channel_affine(dispatchKeySet, self, scale, zero_point, axis, quant_min, quant_max, grad_factor);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__FAKE_QUANTIZE_LEARNABLE_PER_CHANNEL_AFFINE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, scale);trace.append_arg(trace_idx, zero_point);trace.append_arg(trace_idx, axis);trace.append_arg(trace_idx, quant_min);trace.append_arg(trace_idx, quant_max);trace.append_arg(trace_idx, grad_factor);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap__fake_quantize_learnable_per_channel_affine_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor) {
  ensure_materialized(grad);ensure_materialized(self);ensure_materialized(scale);ensure_materialized(zero_point);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_fake_quantize_learnable_per_channel_affine_backward(dispatchKeySet, grad, self, scale, zero_point, axis, quant_min, quant_max, grad_factor);
}

std::tuple<double,int64_t> wrap__choose_qparams_per_tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool reduce_range) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_choose_qparams_per_tensor(dispatchKeySet, self, reduce_range);
}

at::Tensor wrap__saturate_weight_to_fp16(c10::DispatchKeySet dispatchKeySet, const at::Tensor & weight) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_saturate_weight_to_fp16(dispatchKeySet, weight);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(weight.dtype(), weight.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__SATURATE_WEIGHT_TO_FP16, dispatchKeySet);
  trace.append_arg(trace_idx, weight);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap_choose_qparams_optimized(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, int64_t numel, int64_t n_bins, double ratio, int64_t bit_width) {
  ensure_materialized(input);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::choose_qparams_optimized(dispatchKeySet, input, numel, n_bins, ratio, bit_width);
}

at::Tensor wrap_to_dtype_layout(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::to(dispatchKeySet, self, std::move(dtype), std::move(layout), std::move(device), pin_memory, non_blocking, copy, std::move(memory_format));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), device ? *device : self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TO_DTYPE_LAYOUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);trace.append_arg(trace_idx, non_blocking);trace.append_arg(trace_idx, copy);trace.append_arg(trace_idx, std::move(memory_format));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_to_device(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Device device, at::ScalarType dtype, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::to(dispatchKeySet, self, std::move(device), std::move(dtype), non_blocking, copy, std::move(memory_format));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(dtype), device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TO_DEVICE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, non_blocking);trace.append_arg(trace_idx, copy);trace.append_arg(trace_idx, std::move(memory_format));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_to_dtype(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::ScalarType dtype, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::to(dispatchKeySet, self, std::move(dtype), non_blocking, copy, std::move(memory_format));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(dtype), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TO_DTYPE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, non_blocking);trace.append_arg(trace_idx, copy);trace.append_arg(trace_idx, std::move(memory_format));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_to_other(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::to(dispatchKeySet, self, other, non_blocking, copy, std::move(memory_format));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TO_OTHER, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, non_blocking);trace.append_arg(trace_idx, copy);trace.append_arg(trace_idx, std::move(memory_format));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::vector<at::Tensor> wrap_meshgrid(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::meshgrid(dispatchKeySet, std::move(tensors));
}

at::Tensor wrap_cartesian_prod(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cartesian_prod(dispatchKeySet, std::move(tensors));
  }
  auto defaults = compute_dtype(tensors);
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(default_dtype, default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CARTESIAN_PROD, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_combinations(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t r, bool with_replacement) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::combinations(dispatchKeySet, self, r, with_replacement);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_COMBINATIONS, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, r);trace.append_arg(trace_idx, with_replacement);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Scalar wrap_item(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::item(dispatchKeySet, self);
}

at::ScalarType wrap_result_type_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & tensor, const at::Tensor & other) {
  ensure_materialized(tensor);ensure_materialized(other);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::result_type(dispatchKeySet, tensor, other);
}

at::ScalarType wrap_result_type_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & tensor, const at::Scalar & other) {
  ensure_materialized(tensor);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::result_type(dispatchKeySet, tensor, other);
}

at::ScalarType wrap_result_type_Scalar_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Scalar & scalar, const at::Tensor & tensor) {
  ensure_materialized(tensor);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::result_type(dispatchKeySet, scalar, tensor);
}

at::ScalarType wrap_result_type_Scalar_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Scalar & scalar1, const at::Scalar & scalar2) {
  
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::result_type(dispatchKeySet, scalar1, scalar2);
}

bool wrap_can_cast(c10::DispatchKeySet dispatchKeySet, at::ScalarType from, at::ScalarType to) {
  
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::can_cast(dispatchKeySet, std::move(from), std::move(to));
}

at::ScalarType wrap_promote_types(c10::DispatchKeySet dispatchKeySet, at::ScalarType type1, at::ScalarType type2) {
  
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::promote_types(dispatchKeySet, std::move(type1), std::move(type2));
}

at::Scalar wrap__local_scalar_dense(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_local_scalar_dense(dispatchKeySet, self);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap__thnn_fused_lstm_cell(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & cx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias) {
  ensure_materialized(input_gates);ensure_materialized(hidden_gates);ensure_materialized(cx);ensure_materialized(input_bias);ensure_materialized(hidden_bias);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_thnn_fused_lstm_cell(dispatchKeySet, input_gates, hidden_gates, cx, input_bias, hidden_bias);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrap__thnn_fused_lstm_cell_backward(c10::DispatchKeySet dispatchKeySet, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & cx, const at::Tensor & cy, const at::Tensor & workspace, bool has_bias) {
  ensure_materialized(grad_hy);ensure_materialized(grad_cy);ensure_materialized(cx);ensure_materialized(cy);ensure_materialized(workspace);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_thnn_fused_lstm_cell_backward(dispatchKeySet, grad_hy, grad_cy, cx, cy, workspace, has_bias);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrap__thnn_differentiable_lstm_cell_backward(c10::DispatchKeySet dispatchKeySet, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias, const at::Tensor & cx, const at::Tensor & cy) {
  ensure_materialized(grad_hy);ensure_materialized(grad_cy);ensure_materialized(input_gates);ensure_materialized(hidden_gates);ensure_materialized(input_bias);ensure_materialized(hidden_bias);ensure_materialized(cx);ensure_materialized(cy);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_thnn_differentiable_lstm_cell_backward(dispatchKeySet, grad_hy, grad_cy, input_gates, hidden_gates, input_bias, hidden_bias, cx, cy);
}

std::tuple<at::Tensor,at::Tensor> wrap__thnn_fused_gru_cell(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & hx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias) {
  ensure_materialized(input_gates);ensure_materialized(hidden_gates);ensure_materialized(hx);ensure_materialized(input_bias);ensure_materialized(hidden_bias);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_thnn_fused_gru_cell(dispatchKeySet, input_gates, hidden_gates, hx, input_bias, hidden_bias);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrap__thnn_fused_gru_cell_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_hy, const at::Tensor & workspace, bool has_bias) {
  ensure_materialized(grad_hy);ensure_materialized(workspace);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_thnn_fused_gru_cell_backward(dispatchKeySet, grad_hy, workspace, has_bias);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrap__thnn_differentiable_gru_cell_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_hy, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & hx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias) {
  ensure_materialized(grad_hy);ensure_materialized(input_gates);ensure_materialized(hidden_gates);ensure_materialized(hx);ensure_materialized(input_bias);ensure_materialized(hidden_bias);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_thnn_differentiable_gru_cell_backward(dispatchKeySet, grad_hy, input_gates, hidden_gates, hx, input_bias, hidden_bias);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_lstm_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
  ensure_materialized(input);ensure_materialized(hx);ensure_materialized(params);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::lstm(dispatchKeySet, input, std::move(hx), std::move(params), has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_lstm_data(c10::DispatchKeySet dispatchKeySet, const at::Tensor & data, const at::Tensor & batch_sizes, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
  ensure_materialized(data);ensure_materialized(batch_sizes);ensure_materialized(hx);ensure_materialized(params);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::lstm(dispatchKeySet, data, batch_sizes, std::move(hx), std::move(params), has_biases, num_layers, dropout, train, bidirectional);
}

std::tuple<at::Tensor,at::Tensor> wrap_gru_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
  ensure_materialized(input);ensure_materialized(hx);ensure_materialized(params);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::gru(dispatchKeySet, input, hx, std::move(params), has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

std::tuple<at::Tensor,at::Tensor> wrap_gru_data(c10::DispatchKeySet dispatchKeySet, const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
  ensure_materialized(data);ensure_materialized(batch_sizes);ensure_materialized(hx);ensure_materialized(params);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::gru(dispatchKeySet, data, batch_sizes, hx, std::move(params), has_biases, num_layers, dropout, train, bidirectional);
}

std::tuple<at::Tensor,at::Tensor> wrap_rnn_tanh_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
  ensure_materialized(input);ensure_materialized(hx);ensure_materialized(params);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::rnn_tanh(dispatchKeySet, input, hx, std::move(params), has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

std::tuple<at::Tensor,at::Tensor> wrap_rnn_tanh_data(c10::DispatchKeySet dispatchKeySet, const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
  ensure_materialized(data);ensure_materialized(batch_sizes);ensure_materialized(hx);ensure_materialized(params);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::rnn_tanh(dispatchKeySet, data, batch_sizes, hx, std::move(params), has_biases, num_layers, dropout, train, bidirectional);
}

std::tuple<at::Tensor,at::Tensor> wrap_rnn_relu_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
  ensure_materialized(input);ensure_materialized(hx);ensure_materialized(params);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::rnn_relu(dispatchKeySet, input, hx, std::move(params), has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

std::tuple<at::Tensor,at::Tensor> wrap_rnn_relu_data(c10::DispatchKeySet dispatchKeySet, const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
  ensure_materialized(data);ensure_materialized(batch_sizes);ensure_materialized(hx);ensure_materialized(params);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::rnn_relu(dispatchKeySet, data, batch_sizes, hx, std::move(params), has_biases, num_layers, dropout, train, bidirectional);
}

std::tuple<at::Tensor,at::Tensor> wrap_lstm_cell(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
  ensure_materialized(input);ensure_materialized(hx);ensure_materialized(w_ih);ensure_materialized(w_hh);ensure_materialized(b_ih);ensure_materialized(b_hh);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::lstm_cell(dispatchKeySet, input, std::move(hx), w_ih, w_hh, b_ih, b_hh);
}

at::Tensor wrap_gru_cell(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::gru_cell(dispatchKeySet, input, hx, w_ih, w_hh, b_ih, b_hh);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_GRU_CELL, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, hx);trace.append_arg(trace_idx, w_ih);trace.append_arg(trace_idx, w_hh);trace.append_arg(trace_idx, b_ih);trace.append_arg(trace_idx, b_hh);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_rnn_tanh_cell(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rnn_tanh_cell(dispatchKeySet, input, hx, w_ih, w_hh, b_ih, b_hh);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RNN_TANH_CELL, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, hx);trace.append_arg(trace_idx, w_ih);trace.append_arg(trace_idx, w_hh);trace.append_arg(trace_idx, b_ih);trace.append_arg(trace_idx, b_hh);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_rnn_relu_cell(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rnn_relu_cell(dispatchKeySet, input, hx, w_ih, w_hh, b_ih, b_hh);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RNN_RELU_CELL, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, hx);trace.append_arg(trace_idx, w_ih);trace.append_arg(trace_idx, w_hh);trace.append_arg(trace_idx, b_ih);trace.append_arg(trace_idx, b_hh);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap_quantized_lstm_cell(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
  ensure_materialized(input);ensure_materialized(hx);ensure_materialized(w_ih);ensure_materialized(w_hh);ensure_materialized(b_ih);ensure_materialized(b_hh);ensure_materialized(packed_ih);ensure_materialized(packed_hh);ensure_materialized(col_offsets_ih);ensure_materialized(col_offsets_hh);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::quantized_lstm_cell(dispatchKeySet, input, std::move(hx), w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}

at::Tensor wrap_quantized_gru_cell(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::quantized_gru_cell(dispatchKeySet, input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_QUANTIZED_GRU_CELL, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, hx);trace.append_arg(trace_idx, w_ih);trace.append_arg(trace_idx, w_hh);trace.append_arg(trace_idx, b_ih);trace.append_arg(trace_idx, b_hh);trace.append_arg(trace_idx, packed_ih);trace.append_arg(trace_idx, packed_hh);trace.append_arg(trace_idx, col_offsets_ih);trace.append_arg(trace_idx, col_offsets_hh);trace.append_arg(trace_idx, scale_ih);trace.append_arg(trace_idx, scale_hh);trace.append_arg(trace_idx, zero_point_ih);trace.append_arg(trace_idx, zero_point_hh);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_quantized_rnn_relu_cell(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::quantized_rnn_relu_cell(dispatchKeySet, input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_QUANTIZED_RNN_RELU_CELL, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, hx);trace.append_arg(trace_idx, w_ih);trace.append_arg(trace_idx, w_hh);trace.append_arg(trace_idx, b_ih);trace.append_arg(trace_idx, b_hh);trace.append_arg(trace_idx, packed_ih);trace.append_arg(trace_idx, packed_hh);trace.append_arg(trace_idx, col_offsets_ih);trace.append_arg(trace_idx, col_offsets_hh);trace.append_arg(trace_idx, scale_ih);trace.append_arg(trace_idx, scale_hh);trace.append_arg(trace_idx, zero_point_ih);trace.append_arg(trace_idx, zero_point_hh);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_quantized_rnn_tanh_cell(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::quantized_rnn_tanh_cell(dispatchKeySet, input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_QUANTIZED_RNN_TANH_CELL, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, hx);trace.append_arg(trace_idx, w_ih);trace.append_arg(trace_idx, w_hh);trace.append_arg(trace_idx, b_ih);trace.append_arg(trace_idx, b_hh);trace.append_arg(trace_idx, packed_ih);trace.append_arg(trace_idx, packed_hh);trace.append_arg(trace_idx, col_offsets_ih);trace.append_arg(trace_idx, col_offsets_hh);trace.append_arg(trace_idx, scale_ih);trace.append_arg(trace_idx, scale_hh);trace.append_arg(trace_idx, zero_point_ih);trace.append_arg(trace_idx, zero_point_hh);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap__pack_padded_sequence(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & lengths, bool batch_first) {
  ensure_materialized(input);ensure_materialized(lengths);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_pack_padded_sequence(dispatchKeySet, input, lengths, batch_first);
}

at::Tensor wrap__pack_padded_sequence_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, at::IntArrayRef input_size, const at::Tensor & batch_sizes, bool batch_first) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_pack_padded_sequence_backward(dispatchKeySet, grad, std::move(input_size), batch_sizes, batch_first);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__PACK_PADDED_SEQUENCE_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, std::move(input_size));trace.append_arg(trace_idx, batch_sizes);trace.append_arg(trace_idx, batch_first);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap__pad_packed_sequence(c10::DispatchKeySet dispatchKeySet, const at::Tensor & data, const at::Tensor & batch_sizes, bool batch_first, const at::Scalar & padding_value, int64_t total_length) {
  ensure_materialized(data);ensure_materialized(batch_sizes);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_pad_packed_sequence(dispatchKeySet, data, batch_sizes, batch_first, padding_value, total_length);
}

at::Tensor & wrap_set__source_Storage(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Storage source) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::set_(dispatchKeySet, self, std::move(source));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SET__SOURCE_STORAGE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(source));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_set__source_Storage_storage_offset(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Storage source, int64_t storage_offset, at::IntArrayRef size, at::IntArrayRef stride) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::set_(dispatchKeySet, self, std::move(source), storage_offset, std::move(size), std::move(stride));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SET__SOURCE_STORAGE_STORAGE_OFFSET, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(source));trace.append_arg(trace_idx, storage_offset);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(stride));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_set__source_Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & source) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::set_(dispatchKeySet, self, source);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SET__SOURCE_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, source);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_set_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::set_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SET_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

bool wrap_is_set_to(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & tensor) {
  ensure_materialized(self);ensure_materialized(tensor);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::is_set_to(dispatchKeySet, self, tensor);
}

at::Tensor & wrap_masked_fill__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & mask, const at::Scalar & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::masked_fill_(dispatchKeySet, self, mask, value);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MASKED_FILL__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mask);trace.append_arg(trace_idx, value);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_masked_fill_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask, const at::Scalar & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::masked_fill(dispatchKeySet, self, mask, value);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MASKED_FILL_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mask);trace.append_arg(trace_idx, value);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_masked_fill__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & mask, const at::Tensor & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::masked_fill_(dispatchKeySet, self, mask, value);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MASKED_FILL__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mask);trace.append_arg(trace_idx, value);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_masked_fill_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask, const at::Tensor & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::masked_fill(dispatchKeySet, self, mask, value);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MASKED_FILL_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mask);trace.append_arg(trace_idx, value);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_masked_scatter_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & mask, const at::Tensor & source) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::masked_scatter_(dispatchKeySet, self, mask, source);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MASKED_SCATTER_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mask);trace.append_arg(trace_idx, source);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_masked_scatter(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask, const at::Tensor & source) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::masked_scatter(dispatchKeySet, self, mask, source);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MASKED_SCATTER, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mask);trace.append_arg(trace_idx, source);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_view(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::view(dispatchKeySet, self, std::move(size));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_VIEW, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(size));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_view_dtype(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::ScalarType dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::view(dispatchKeySet, self, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(dtype), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_VIEW_DTYPE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_put_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & index, const at::Tensor & source, bool accumulate) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::put_(dispatchKeySet, self, index, source, accumulate);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_PUT_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, source);trace.append_arg(trace_idx, accumulate);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_put(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & index, const at::Tensor & source, bool accumulate) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::put(dispatchKeySet, self, index, source, accumulate);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_PUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, source);trace.append_arg(trace_idx, accumulate);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_index_add_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_add_(dispatchKeySet, self, dim, index, source);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_INDEX_ADD_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, source);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_index_add__alpha(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_add_(dispatchKeySet, self, dim, index, source, alpha);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_INDEX_ADD__ALPHA, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, source);trace.append_arg(trace_idx, alpha);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_index_add(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_add(dispatchKeySet, self, dim, index, source);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_INDEX_ADD, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, source);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_index_add_alpha(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_add(dispatchKeySet, self, dim, index, source, alpha);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_INDEX_ADD_ALPHA, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, source);trace.append_arg(trace_idx, alpha);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_index_add_dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_add(dispatchKeySet, self, std::move(dim), index, source, alpha);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_INDEX_ADD_DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, source);trace.append_arg(trace_idx, alpha);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_index_fill__int_Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_fill_(dispatchKeySet, self, dim, index, value);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_INDEX_FILL__INT_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, value);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_index_fill_int_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_fill(dispatchKeySet, self, dim, index, value);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_INDEX_FILL_INT_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, value);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_index_fill__int_Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_fill_(dispatchKeySet, self, dim, index, value);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_INDEX_FILL__INT_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, value);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_index_fill_int_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_fill(dispatchKeySet, self, dim, index, value);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_INDEX_FILL_INT_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, value);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_index_fill__Dimname_Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_fill_(dispatchKeySet, self, std::move(dim), index, value);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_INDEX_FILL__DIMNAME_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, value);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_index_fill__Dimname_Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_fill_(dispatchKeySet, self, std::move(dim), index, value);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_INDEX_FILL__DIMNAME_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, value);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_index_fill_Dimname_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_fill(dispatchKeySet, self, std::move(dim), index, value);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_INDEX_FILL_DIMNAME_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, value);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_index_fill_Dimname_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_fill(dispatchKeySet, self, std::move(dim), index, value);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_INDEX_FILL_DIMNAME_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, value);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_scatter__src(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::scatter_(dispatchKeySet, self, dim, index, src);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SCATTER__SRC, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, src);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_scatter_src(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::scatter(dispatchKeySet, self, dim, index, src);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SCATTER_SRC, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, src);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_scatter__value(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::scatter_(dispatchKeySet, self, dim, index, value);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SCATTER__VALUE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, value);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_scatter_value(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::scatter(dispatchKeySet, self, dim, index, value);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SCATTER_VALUE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, value);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_scatter_dimname_src(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & src) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::scatter(dispatchKeySet, self, std::move(dim), index, src);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SCATTER_DIMNAME_SRC, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, src);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_scatter_dimname_value(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::scatter(dispatchKeySet, self, std::move(dim), index, value);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SCATTER_DIMNAME_VALUE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, value);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_scatter__reduce(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::scatter_(dispatchKeySet, self, dim, index, src, std::move(reduce));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SCATTER__REDUCE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, src);trace.append_arg(trace_idx, std::move(reduce));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_scatter__value_reduce(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, c10::string_view reduce) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::scatter_(dispatchKeySet, self, dim, index, value, std::move(reduce));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SCATTER__VALUE_REDUCE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, value);trace.append_arg(trace_idx, std::move(reduce));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_scatter_add_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::scatter_add_(dispatchKeySet, self, dim, index, src);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SCATTER_ADD_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, src);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_scatter_add(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::scatter_add(dispatchKeySet, self, dim, index, src);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SCATTER_ADD, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, src);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_scatter_add_dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & src) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::scatter_add(dispatchKeySet, self, std::move(dim), index, src);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SCATTER_ADD_DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, src);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_eq__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::eq_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_EQ__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_eq__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::eq_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_EQ__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_bitwise_and_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bitwise_and_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BITWISE_AND_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_bitwise_and_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bitwise_and_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BITWISE_AND_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_bitwise_and_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bitwise_and(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BITWISE_AND_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_bitwise_and_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bitwise_and(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BITWISE_AND_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_bitwise_and__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bitwise_and_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BITWISE_AND__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_bitwise_and__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bitwise_and_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BITWISE_AND__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap___and___Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__and__(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H___AND___SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap___and___Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__and__(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H___AND___TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap___iand___Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__iand__(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H___IAND___SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap___iand___Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__iand__(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H___IAND___TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_bitwise_or_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bitwise_or_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BITWISE_OR_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_bitwise_or_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bitwise_or_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BITWISE_OR_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_bitwise_or_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bitwise_or(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BITWISE_OR_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_bitwise_or_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bitwise_or(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BITWISE_OR_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_bitwise_or__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bitwise_or_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BITWISE_OR__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_bitwise_or__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bitwise_or_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BITWISE_OR__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap___or___Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__or__(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H___OR___SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap___or___Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__or__(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H___OR___TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap___ior___Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__ior__(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H___IOR___SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap___ior___Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__ior__(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H___IOR___TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_bitwise_xor_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bitwise_xor_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BITWISE_XOR_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_bitwise_xor_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bitwise_xor_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BITWISE_XOR_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_bitwise_xor_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bitwise_xor(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BITWISE_XOR_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_bitwise_xor_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bitwise_xor(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BITWISE_XOR_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_bitwise_xor__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bitwise_xor_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BITWISE_XOR__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_bitwise_xor__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bitwise_xor_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BITWISE_XOR__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap___xor___Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__xor__(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H___XOR___SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap___xor___Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__xor__(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H___XOR___TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap___ixor___Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__ixor__(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H___IXOR___SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap___ixor___Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__ixor__(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H___IXOR___TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap___lshift___Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__lshift__(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H___LSHIFT___SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap___lshift___Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__lshift__(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H___LSHIFT___TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap___ilshift___Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__ilshift__(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H___ILSHIFT___SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap___ilshift___Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__ilshift__(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H___ILSHIFT___TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap___rshift___Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__rshift__(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H___RSHIFT___SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap___rshift___Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__rshift__(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H___RSHIFT___TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap___irshift___Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__irshift__(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H___IRSHIFT___SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap___irshift___Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::__irshift__(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H___IRSHIFT___TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_tril_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t diagonal) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::tril_(dispatchKeySet, self, diagonal);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_TRIL_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, diagonal);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_triu_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t diagonal) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::triu_(dispatchKeySet, self, diagonal);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_TRIU_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, diagonal);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_renorm_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & p, int64_t dim, const at::Scalar & maxnorm) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::renorm_(dispatchKeySet, self, p, dim, maxnorm);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RENORM_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, maxnorm);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_lerp__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & end, const at::Scalar & weight) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::lerp_(dispatchKeySet, self, end, weight);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LERP__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, end);trace.append_arg(trace_idx, weight);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_lerp__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & end, const at::Tensor & weight) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::lerp_(dispatchKeySet, self, end, weight);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LERP__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, end);trace.append_arg(trace_idx, weight);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_fmod__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fmod_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FMOD__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_fmod__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fmod_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FMOD__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_addbmm_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::addbmm_(dispatchKeySet, self, batch1, batch2, beta, alpha);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ADDBMM_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, batch1);trace.append_arg(trace_idx, batch2);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, alpha);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_addbmm_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::addbmm_outf(dispatchKeySet, self, batch1, batch2, beta, alpha, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ADDBMM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, batch1);trace.append_arg(trace_idx, batch2);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, alpha);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_addbmm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::addbmm(dispatchKeySet, self, batch1, batch2, beta, alpha);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ADDBMM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, batch1);trace.append_arg(trace_idx, batch2);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, alpha);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_addcdiv_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::addcdiv_(dispatchKeySet, self, tensor1, tensor2, value);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ADDCDIV_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, tensor1);trace.append_arg(trace_idx, tensor2);trace.append_arg(trace_idx, value);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_random__from(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t from, c10::optional<int64_t> to, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::random_(dispatchKeySet, self, from, to, std::move(generator));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RANDOM__FROM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, from);trace.append_arg(trace_idx, to);trace.append_arg(trace_idx, std::move(generator));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_random__to(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t to, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::random_(dispatchKeySet, self, to, std::move(generator));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RANDOM__TO, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, to);trace.append_arg(trace_idx, std::move(generator));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_random_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::random_(dispatchKeySet, self, std::move(generator));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RANDOM_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(generator));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_uniform_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double from, double to, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::uniform_(dispatchKeySet, self, from, to, std::move(generator));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_UNIFORM_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, from);trace.append_arg(trace_idx, to);trace.append_arg(trace_idx, std::move(generator));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_cauchy_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double median, double sigma, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cauchy_(dispatchKeySet, self, median, sigma, std::move(generator));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CAUCHY_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, median);trace.append_arg(trace_idx, sigma);trace.append_arg(trace_idx, std::move(generator));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_log_normal_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::log_normal_(dispatchKeySet, self, mean, std, std::move(generator));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOG_NORMAL_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mean);trace.append_arg(trace_idx, std);trace.append_arg(trace_idx, std::move(generator));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_exponential_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double lambd, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::exponential_(dispatchKeySet, self, lambd, std::move(generator));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_EXPONENTIAL_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, lambd);trace.append_arg(trace_idx, std::move(generator));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_geometric_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double p, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::geometric_(dispatchKeySet, self, p, std::move(generator));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GEOMETRIC_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, std::move(generator));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_diag_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::diag_outf(dispatchKeySet, self, diagonal, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_DIAG_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, diagonal);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_diag(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t diagonal) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::diag(dispatchKeySet, self, diagonal);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DIAG, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, diagonal);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_diag_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, at::IntArrayRef input_sizes, int64_t diagonal) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::diag_backward(dispatchKeySet, grad, std::move(input_sizes), diagonal);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DIAG_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, std::move(input_sizes));trace.append_arg(trace_idx, diagonal);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_cross_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<int64_t> dim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cross_outf(dispatchKeySet, self, other, dim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CROSS_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_cross(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<int64_t> dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cross(dispatchKeySet, self, other, dim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CROSS, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_triu_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::triu_outf(dispatchKeySet, self, diagonal, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_TRIU_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, diagonal);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_triu(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t diagonal) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::triu(dispatchKeySet, self, diagonal);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TRIU, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, diagonal);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_tril_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::tril_outf(dispatchKeySet, self, diagonal, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_TRIL_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, diagonal);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_tril(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t diagonal) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::tril(dispatchKeySet, self, diagonal);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TRIL, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, diagonal);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_tril_indices(c10::DispatchKeySet dispatchKeySet, int64_t row, int64_t col, int64_t offset, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::tril_indices(dispatchKeySet, row, col, offset, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TRIL_INDICES, dispatchKeySet);
  trace.append_arg(trace_idx, row);trace.append_arg(trace_idx, col);trace.append_arg(trace_idx, offset);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_triu_indices(c10::DispatchKeySet dispatchKeySet, int64_t row, int64_t col, int64_t offset, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::triu_indices(dispatchKeySet, row, col, offset, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TRIU_INDICES, dispatchKeySet);
  trace.append_arg(trace_idx, row);trace.append_arg(trace_idx, col);trace.append_arg(trace_idx, offset);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_trace(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::trace(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TRACE, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_trace_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, at::IntArrayRef sizes) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::trace_backward(dispatchKeySet, grad, std::move(sizes));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TRACE_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, std::move(sizes));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_ne_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ne_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NE_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_ne_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ne(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(kBool), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NE_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_ne_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ne_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NE_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_ne_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ne(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(kBool), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NE_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_ne__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ne_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NE__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_ne__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ne_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NE__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_not_equal_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::not_equal_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NOT_EQUAL_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_not_equal_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::not_equal(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NOT_EQUAL_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_not_equal_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::not_equal_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NOT_EQUAL_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_not_equal_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::not_equal(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NOT_EQUAL_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_not_equal__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::not_equal_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NOT_EQUAL__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_not_equal__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::not_equal_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NOT_EQUAL__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_eq_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::eq_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_EQ_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_eq_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::eq(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(kBool), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_EQ_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_eq_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::eq_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_EQ_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_eq_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::eq(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(kBool), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_EQ_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_ge_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ge_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GE_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_ge_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ge(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_GE_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_ge_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ge_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GE_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_ge_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ge(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_GE_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_ge__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ge_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GE__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_ge__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ge_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GE__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_greater_equal_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::greater_equal_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GREATER_EQUAL_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_greater_equal_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::greater_equal(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_GREATER_EQUAL_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_greater_equal_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::greater_equal_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GREATER_EQUAL_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_greater_equal_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::greater_equal(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_GREATER_EQUAL_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_greater_equal__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::greater_equal_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GREATER_EQUAL__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_greater_equal__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::greater_equal_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GREATER_EQUAL__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_le_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::le_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LE_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_le_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::le(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LE_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_le_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::le_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LE_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_le_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::le(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LE_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_le__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::le_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LE__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_le__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::le_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LE__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_less_equal_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::less_equal_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LESS_EQUAL_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_less_equal_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::less_equal(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LESS_EQUAL_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_less_equal_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::less_equal_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LESS_EQUAL_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_less_equal_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::less_equal(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LESS_EQUAL_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_less_equal__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::less_equal_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LESS_EQUAL__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_less_equal__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::less_equal_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LESS_EQUAL__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_gt_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::gt_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GT_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_gt_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::gt(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(kBool), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_GT_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_gt_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::gt_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GT_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_gt_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::gt(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(kBool), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_GT_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_gt__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::gt_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GT__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_gt__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::gt_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GT__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_greater_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::greater_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GREATER_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_greater_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::greater(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(kBool), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_GREATER_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_greater_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::greater_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GREATER_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_greater_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::greater(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(kBool), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_GREATER_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_greater__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::greater_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GREATER__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_greater__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::greater_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GREATER__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_lt_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::lt_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LT_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_lt_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::lt(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(kBool), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LT_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_lt_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::lt_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LT_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_lt_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::lt(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(kBool), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LT_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_lt__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::lt_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LT__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_lt__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::lt_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LT__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_less_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::less_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LESS_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_less_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::less(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(kBool), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LESS_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_less_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::less_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LESS_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_less_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::less(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(kBool), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LESS_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_less__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::less_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LESS__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_less__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::less_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LESS__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_take_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & index, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::take_outf(dispatchKeySet, self, index, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_TAKE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_take(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & index) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::take(dispatchKeySet, self, index);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TAKE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, index);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_take_along_dim_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & indices, c10::optional<int64_t> dim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::take_along_dim_outf(dispatchKeySet, self, indices, dim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_TAKE_ALONG_DIM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_take_along_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & indices, c10::optional<int64_t> dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::take_along_dim(dispatchKeySet, self, indices, dim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TAKE_ALONG_DIM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_index_select_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_select_outf(dispatchKeySet, self, dim, index, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_INDEX_SELECT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_index_select(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_select(dispatchKeySet, self, dim, index);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_INDEX_SELECT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_index_select_dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_select_outf(dispatchKeySet, self, std::move(dim), index, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_INDEX_SELECT_DIMNAME_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_index_select_dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_select(dispatchKeySet, self, std::move(dim), index);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_INDEX_SELECT_DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, index);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_index_select_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, at::IntArrayRef self_sizes, int64_t dim, const at::Tensor & index) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::index_select_backward(dispatchKeySet, grad, std::move(self_sizes), dim, index);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_INDEX_SELECT_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, std::move(self_sizes));trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_masked_select_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::masked_select_outf(dispatchKeySet, self, mask, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MASKED_SELECT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mask);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_masked_select(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::masked_select(dispatchKeySet, self, mask);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MASKED_SELECT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mask);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_masked_select_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & input, const at::Tensor & mask) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::masked_select_backward(dispatchKeySet, grad, input, mask);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MASKED_SELECT_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, mask);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_nonzero_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nonzero_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NONZERO_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_nonzero(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nonzero(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NONZERO, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::vector<at::Tensor> wrap_nonzero_numpy(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::nonzero_numpy(dispatchKeySet, self);
}

at::Tensor & wrap_gather_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::gather_outf(dispatchKeySet, self, dim, index, sparse_grad, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GATHER_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, sparse_grad);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_gather(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::gather(dispatchKeySet, self, dim, index, sparse_grad);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_GATHER, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, sparse_grad);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_gather_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::gather_backward(dispatchKeySet, grad, self, dim, index, sparse_grad);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_GATHER_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, sparse_grad);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_gather_dimname_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, bool sparse_grad, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::gather_outf(dispatchKeySet, self, std::move(dim), index, sparse_grad, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GATHER_DIMNAME_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, sparse_grad);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_gather_dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, bool sparse_grad) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::gather(dispatchKeySet, self, std::move(dim), index, sparse_grad);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_GATHER_DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, sparse_grad);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__gather_sparse_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & grad) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_gather_sparse_backward(dispatchKeySet, self, dim, index, grad);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__GATHER_SPARSE_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, grad);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_addcmul_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::addcmul_outf(dispatchKeySet, self, tensor1, tensor2, value, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ADDCMUL_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, tensor1);trace.append_arg(trace_idx, tensor2);trace.append_arg(trace_idx, value);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_addcmul(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::addcmul(dispatchKeySet, self, tensor1, tensor2, value);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ADDCMUL, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, tensor1);trace.append_arg(trace_idx, tensor2);trace.append_arg(trace_idx, value);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_addcmul_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::addcmul_(dispatchKeySet, self, tensor1, tensor2, value);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ADDCMUL_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, tensor1);trace.append_arg(trace_idx, tensor2);trace.append_arg(trace_idx, value);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_addcdiv_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::addcdiv_outf(dispatchKeySet, self, tensor1, tensor2, value, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ADDCDIV_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, tensor1);trace.append_arg(trace_idx, tensor2);trace.append_arg(trace_idx, value);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_addcdiv(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::addcdiv(dispatchKeySet, self, tensor1, tensor2, value);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ADDCDIV, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, tensor1);trace.append_arg(trace_idx, tensor2);trace.append_arg(trace_idx, value);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_cross_entropy_loss(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cross_entropy_loss(dispatchKeySet, self, target, weight, reduction, ignore_index);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CROSS_ENTROPY_LOSS, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, ignore_index);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor &,at::Tensor &> wrap_lstsq_X(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & A, at::Tensor & X, at::Tensor & qr) {
  ensure_materialized(self);ensure_materialized(A);ensure_materialized(X);ensure_materialized(qr);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::lstsq_outf(dispatchKeySet, self, A, X, qr);
}

std::tuple<at::Tensor,at::Tensor> wrap_lstsq(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & A) {
  ensure_materialized(self);ensure_materialized(A);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::lstsq(dispatchKeySet, self, A);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_triangular_solve_X(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & A, bool upper, bool transpose, bool unitriangular, at::Tensor & X, at::Tensor & M) {
  ensure_materialized(self);ensure_materialized(A);ensure_materialized(X);ensure_materialized(M);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::triangular_solve_outf(dispatchKeySet, self, A, upper, transpose, unitriangular, X, M);
}

std::tuple<at::Tensor,at::Tensor> wrap_triangular_solve(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & A, bool upper, bool transpose, bool unitriangular) {
  ensure_materialized(self);ensure_materialized(A);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::triangular_solve(dispatchKeySet, self, A, upper, transpose, unitriangular);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_symeig_e(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool eigenvectors, bool upper, at::Tensor & e, at::Tensor & V) {
  ensure_materialized(self);ensure_materialized(e);ensure_materialized(V);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::symeig_outf(dispatchKeySet, self, eigenvectors, upper, e, V);
}

std::tuple<at::Tensor,at::Tensor> wrap_symeig(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool eigenvectors, bool upper) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::symeig(dispatchKeySet, self, eigenvectors, upper);
}

std::tuple<at::Tensor,at::Tensor> wrap__symeig_helper(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool eigenvectors, bool upper) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_symeig_helper(dispatchKeySet, self, eigenvectors, upper);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_eig_e(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool eigenvectors, at::Tensor & e, at::Tensor & v) {
  ensure_materialized(self);ensure_materialized(e);ensure_materialized(v);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::eig_outf(dispatchKeySet, self, eigenvectors, e, v);
}

std::tuple<at::Tensor,at::Tensor> wrap_eig(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool eigenvectors) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::eig(dispatchKeySet, self, eigenvectors);
}

std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrap_svd_U(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool some, bool compute_uv, at::Tensor & U, at::Tensor & S, at::Tensor & V) {
  ensure_materialized(self);ensure_materialized(U);ensure_materialized(S);ensure_materialized(V);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::svd_outf(dispatchKeySet, self, some, compute_uv, U, S, V);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_svd(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool some, bool compute_uv) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::svd(dispatchKeySet, self, some, compute_uv);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap__svd_helper(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool some, bool compute_uv) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_svd_helper(dispatchKeySet, self, some, compute_uv);
}

at::Tensor wrap_swapaxes(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t axis0, int64_t axis1) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::swapaxes(dispatchKeySet, self, axis0, axis1);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SWAPAXES, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, axis0);trace.append_arg(trace_idx, axis1);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_swapaxes_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t axis0, int64_t axis1) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::swapaxes_(dispatchKeySet, self, axis0, axis1);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SWAPAXES_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, axis0);trace.append_arg(trace_idx, axis1);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_swapdims(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim0, int64_t dim1) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::swapdims(dispatchKeySet, self, dim0, dim1);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SWAPDIMS, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim0);trace.append_arg(trace_idx, dim1);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_swapdims_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim0, int64_t dim1) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::swapdims_(dispatchKeySet, self, dim0, dim1);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SWAPDIMS_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim0);trace.append_arg(trace_idx, dim1);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_cholesky_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool upper, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cholesky_outf(dispatchKeySet, self, upper, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CHOLESKY_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, upper);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_cholesky(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool upper) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cholesky(dispatchKeySet, self, upper);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CHOLESKY, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, upper);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_cholesky_solve_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & input2, bool upper, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cholesky_solve_outf(dispatchKeySet, self, input2, upper, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CHOLESKY_SOLVE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, input2);trace.append_arg(trace_idx, upper);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_cholesky_solve(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & input2, bool upper) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cholesky_solve(dispatchKeySet, self, input2, upper);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CHOLESKY_SOLVE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, input2);trace.append_arg(trace_idx, upper);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__cholesky_solve_helper(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & A, bool upper) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_cholesky_solve_helper(dispatchKeySet, self, A, upper);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__CHOLESKY_SOLVE_HELPER, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, A);trace.append_arg(trace_idx, upper);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap_solve(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & A) {
  ensure_materialized(self);ensure_materialized(A);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::solve(dispatchKeySet, self, A);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_solve_solution(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & A, at::Tensor & solution, at::Tensor & lu) {
  ensure_materialized(self);ensure_materialized(A);ensure_materialized(solution);ensure_materialized(lu);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::solve_outf(dispatchKeySet, self, A, solution, lu);
}

std::tuple<at::Tensor,at::Tensor> wrap__solve_helper(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & A) {
  ensure_materialized(self);ensure_materialized(A);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_solve_helper(dispatchKeySet, self, A);
}

at::Tensor wrap_cholesky_inverse(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool upper) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cholesky_inverse(dispatchKeySet, self, upper);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CHOLESKY_INVERSE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, upper);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_cholesky_inverse_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool upper, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::cholesky_inverse_outf(dispatchKeySet, self, upper, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_CHOLESKY_INVERSE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, upper);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

std::tuple<at::Tensor &,at::Tensor &> wrap_qr_Q(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool some, at::Tensor & Q, at::Tensor & R) {
  ensure_materialized(self);ensure_materialized(Q);ensure_materialized(R);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::qr_outf(dispatchKeySet, self, some, Q, R);
}

std::tuple<at::Tensor,at::Tensor> wrap_qr(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool some) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::qr(dispatchKeySet, self, some);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_geqrf_a(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & a, at::Tensor & tau) {
  ensure_materialized(self);ensure_materialized(a);ensure_materialized(tau);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::geqrf_outf(dispatchKeySet, self, a, tau);
}

std::tuple<at::Tensor,at::Tensor> wrap_geqrf(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::geqrf(dispatchKeySet, self);
}

at::Tensor wrap_orgqr(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & input2) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::orgqr(dispatchKeySet, self, input2);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ORGQR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, input2);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_orgqr_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & input2, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::orgqr_outf(dispatchKeySet, self, input2, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ORGQR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, input2);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_ormqr_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & input2, const at::Tensor & input3, bool left, bool transpose, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ormqr_outf(dispatchKeySet, self, input2, input3, left, transpose, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ORMQR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, input2);trace.append_arg(trace_idx, input3);trace.append_arg(trace_idx, left);trace.append_arg(trace_idx, transpose);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_ormqr(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & input2, const at::Tensor & input3, bool left, bool transpose) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ormqr(dispatchKeySet, self, input2, input3, left, transpose);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ORMQR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, input2);trace.append_arg(trace_idx, input3);trace.append_arg(trace_idx, left);trace.append_arg(trace_idx, transpose);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap__lu_with_info(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool pivot, bool check_errors) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_lu_with_info(dispatchKeySet, self, pivot, check_errors);
}

at::Tensor & wrap_lu_solve_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & LU_data, const at::Tensor & LU_pivots, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::lu_solve_outf(dispatchKeySet, self, LU_data, LU_pivots, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LU_SOLVE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, LU_data);trace.append_arg(trace_idx, LU_pivots);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_lu_solve(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & LU_data, const at::Tensor & LU_pivots) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::lu_solve(dispatchKeySet, self, LU_data, LU_pivots);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LU_SOLVE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, LU_data);trace.append_arg(trace_idx, LU_pivots);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_lu_unpack(c10::DispatchKeySet dispatchKeySet, const at::Tensor & LU_data, const at::Tensor & LU_pivots, bool unpack_data, bool unpack_pivots) {
  ensure_materialized(LU_data);ensure_materialized(LU_pivots);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::lu_unpack(dispatchKeySet, LU_data, LU_pivots, unpack_data, unpack_pivots);
}

std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrap_lu_unpack_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & LU_data, const at::Tensor & LU_pivots, bool unpack_data, bool unpack_pivots, at::Tensor & P, at::Tensor & L, at::Tensor & U) {
  ensure_materialized(LU_data);ensure_materialized(LU_pivots);ensure_materialized(P);ensure_materialized(L);ensure_materialized(U);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::lu_unpack_outf(dispatchKeySet, LU_data, LU_pivots, unpack_data, unpack_pivots, P, L, U);
}

at::Tensor & wrap_multinomial_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t num_samples, bool replacement, c10::optional<at::Generator> generator, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::multinomial_outf(dispatchKeySet, self, num_samples, replacement, std::move(generator), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MULTINOMIAL_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, num_samples);trace.append_arg(trace_idx, replacement);trace.append_arg(trace_idx, std::move(generator));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_multinomial(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t num_samples, bool replacement, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::multinomial(dispatchKeySet, self, num_samples, replacement, std::move(generator));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MULTINOMIAL, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, num_samples);trace.append_arg(trace_idx, replacement);trace.append_arg(trace_idx, std::move(generator));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_lgamma_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::lgamma_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LGAMMA_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_digamma_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::digamma_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_DIGAMMA_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_polygamma_out(c10::DispatchKeySet dispatchKeySet, int64_t n, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::polygamma_outf(dispatchKeySet, n, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_POLYGAMMA_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_polygamma_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t n) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::polygamma_(dispatchKeySet, self, n);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_POLYGAMMA_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, n);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_erfinv_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::erfinv_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ERFINV_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_i0_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::i0_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_I0_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_sign(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sign(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SIGN, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_sign_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sign_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SIGN_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_sign_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sign_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SIGN_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_signbit(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::signbit(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SIGNBIT, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_signbit_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::signbit_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SIGNBIT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_dist(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & p) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::dist(dispatchKeySet, self, other, p);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DIST, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, p);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_atan2_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::atan2_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ATAN2_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_lerp_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & end, const at::Scalar & weight, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::lerp_outf(dispatchKeySet, self, end, weight, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LERP_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, end);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_lerp_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & end, const at::Tensor & weight, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::lerp_outf(dispatchKeySet, self, end, weight, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LERP_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, end);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_lerp_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & end, const at::Scalar & weight) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::lerp(dispatchKeySet, self, end, weight);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LERP_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, end);trace.append_arg(trace_idx, weight);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_lerp_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & end, const at::Tensor & weight) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::lerp(dispatchKeySet, self, end, weight);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LERP_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, end);trace.append_arg(trace_idx, weight);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_histc_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t bins, const at::Scalar & min, const at::Scalar & max, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::histc_outf(dispatchKeySet, self, bins, min, max, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_HISTC_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, bins);trace.append_arg(trace_idx, min);trace.append_arg(trace_idx, max);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_histc(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t bins, const at::Scalar & min, const at::Scalar & max) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::histc(dispatchKeySet, self, bins, min, max);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_HISTC, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, bins);trace.append_arg(trace_idx, min);trace.append_arg(trace_idx, max);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fmod_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fmod_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FMOD_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_fmod_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fmod(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FMOD_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fmod_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fmod_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FMOD_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_fmod_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fmod(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FMOD_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_hypot_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hypot_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_HYPOT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_hypot_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hypot_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_HYPOT_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_igamma_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::igamma_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_IGAMMA_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_igammac_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::igammac_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_IGAMMAC_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_nextafter_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nextafter_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NEXTAFTER_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_nextafter_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nextafter_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NEXTAFTER_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_remainder_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::remainder_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_REMAINDER_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_remainder_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::remainder(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_REMAINDER_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_remainder__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::remainder_(dispatchKeySet, self, other);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_REMAINDER__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_remainder_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::remainder_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_REMAINDER_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_min(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::min(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MIN, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fmin_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fmin_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FMIN_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_max(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::max(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MAX, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fmax_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fmax_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FMAX_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_maximum_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::maximum_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MAXIMUM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_max_other(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::max(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MAX_OTHER, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_max_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::max_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MAX_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_minimum_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::minimum_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MINIMUM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_min_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::min_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MIN_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_min_other(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::min(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MIN_OTHER, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_quantile_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::quantile_outf(dispatchKeySet, self, q, dim, keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_QUANTILE_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, q);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_quantile_scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::quantile(dispatchKeySet, self, q, dim, keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_QUANTILE_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, q);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_quantile_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::quantile_outf(dispatchKeySet, self, q, dim, keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_QUANTILE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, q);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_quantile(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::quantile(dispatchKeySet, self, q, dim, keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_QUANTILE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, q);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_nanquantile_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nanquantile_outf(dispatchKeySet, self, q, dim, keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NANQUANTILE_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, q);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_nanquantile_scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nanquantile(dispatchKeySet, self, q, dim, keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NANQUANTILE_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, q);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_nanquantile_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nanquantile_outf(dispatchKeySet, self, q, dim, keepdim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NANQUANTILE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, q);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_nanquantile(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nanquantile(dispatchKeySet, self, q, dim, keepdim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NANQUANTILE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, q);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_quantile_new_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::quantile_outf(dispatchKeySet, self, q, dim, keepdim, std::move(interpolation), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_QUANTILE_NEW_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, q);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(interpolation));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_quantile_new_scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::quantile(dispatchKeySet, self, q, dim, keepdim, std::move(interpolation));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_QUANTILE_NEW_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, q);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(interpolation));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_quantile_new_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::quantile_outf(dispatchKeySet, self, q, dim, keepdim, std::move(interpolation), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_QUANTILE_NEW_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, q);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(interpolation));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_quantile_new(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::quantile(dispatchKeySet, self, q, dim, keepdim, std::move(interpolation));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_QUANTILE_NEW, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, q);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(interpolation));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_nanquantile_new_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nanquantile_outf(dispatchKeySet, self, q, dim, keepdim, std::move(interpolation), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NANQUANTILE_NEW_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, q);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(interpolation));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_nanquantile_new_scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nanquantile(dispatchKeySet, self, q, dim, keepdim, std::move(interpolation));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NANQUANTILE_NEW_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, q);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(interpolation));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_nanquantile_new_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nanquantile_outf(dispatchKeySet, self, q, dim, keepdim, std::move(interpolation), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NANQUANTILE_NEW_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, q);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(interpolation));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_nanquantile_new(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nanquantile(dispatchKeySet, self, q, dim, keepdim, std::move(interpolation));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NANQUANTILE_NEW, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, q);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(interpolation));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor &,at::Tensor &> wrap_sort_values(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool descending, at::Tensor & values, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(values);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::sort_outf(dispatchKeySet, self, dim, descending, values, indices);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_sort_values_stable(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<bool> stable, int64_t dim, bool descending, at::Tensor & values, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(values);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::sort_outf(dispatchKeySet, self, stable, dim, descending, values, indices);
}

std::tuple<at::Tensor,at::Tensor> wrap_sort(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool descending) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::sort(dispatchKeySet, self, dim, descending);
}

std::tuple<at::Tensor,at::Tensor> wrap_sort_stable(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<bool> stable, int64_t dim, bool descending) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::sort(dispatchKeySet, self, stable, dim, descending);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_sort_dimname_values(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool descending, at::Tensor & values, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(values);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::sort_outf(dispatchKeySet, self, std::move(dim), descending, values, indices);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_sort_dimname_values_stable(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<bool> stable, at::Dimname dim, bool descending, at::Tensor & values, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(values);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::sort_outf(dispatchKeySet, self, stable, std::move(dim), descending, values, indices);
}

std::tuple<at::Tensor,at::Tensor> wrap_sort_dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool descending) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::sort(dispatchKeySet, self, std::move(dim), descending);
}

std::tuple<at::Tensor,at::Tensor> wrap_sort_dimname_stable(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<bool> stable, at::Dimname dim, bool descending) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::sort(dispatchKeySet, self, stable, std::move(dim), descending);
}

at::Tensor & wrap_msort_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::msort_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MSORT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_msort(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::msort(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MSORT, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_argsort(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool descending) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::argsort(dispatchKeySet, self, dim, descending);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ARGSORT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, descending);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_argsort_dimname(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool descending) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::argsort(dispatchKeySet, self, std::move(dim), descending);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ARGSORT_DIMNAME, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, descending);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor &,at::Tensor &> wrap_topk_values(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted, at::Tensor & values, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(values);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::topk_outf(dispatchKeySet, self, k, dim, largest, sorted, values, indices);
}

std::tuple<at::Tensor,at::Tensor> wrap_topk(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::topk(dispatchKeySet, self, k, dim, largest, sorted);
}

at::Tensor wrap_all(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::all(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ALL, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_any(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::any(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ANY, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_renorm_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & p, int64_t dim, const at::Scalar & maxnorm, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::renorm_outf(dispatchKeySet, self, p, dim, maxnorm, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RENORM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, maxnorm);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_renorm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & p, int64_t dim, const at::Scalar & maxnorm) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::renorm(dispatchKeySet, self, p, dim, maxnorm);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RENORM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, maxnorm);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_unfold(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dimension, int64_t size, int64_t step) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::unfold(dispatchKeySet, self, dimension, size, step);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_UNFOLD, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dimension);trace.append_arg(trace_idx, size);trace.append_arg(trace_idx, step);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_unfold_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_in, at::IntArrayRef input_sizes, int64_t dim, int64_t size, int64_t step) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::unfold_backward(dispatchKeySet, grad_in, std::move(input_sizes), dim, size, step);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_in.dtype(), grad_in.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_UNFOLD_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_in);trace.append_arg(trace_idx, std::move(input_sizes));trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, size);trace.append_arg(trace_idx, step);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

bool wrap_equal(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  ensure_materialized(self);ensure_materialized(other);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::equal(dispatchKeySet, self, other);
}

at::Tensor & wrap_pow_Tensor_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & exponent, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::pow_outf(dispatchKeySet, self, exponent, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_POW_TENSOR_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, exponent);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_pow_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & exponent, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::pow_outf(dispatchKeySet, self, exponent, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_POW_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, exponent);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_pow_Tensor_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & exponent, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::pow_outf(dispatchKeySet, self, exponent, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_POW_TENSOR_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, exponent);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_pow_Tensor_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & exponent) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::pow(dispatchKeySet, self, exponent);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_POW_TENSOR_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, exponent);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_float_power_Tensor_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & exponent, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::float_power_outf(dispatchKeySet, self, exponent, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FLOAT_POWER_TENSOR_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, exponent);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_float_power_Tensor_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & exponent) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::float_power(dispatchKeySet, self, exponent);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FLOAT_POWER_TENSOR_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, exponent);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_float_power_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & exponent, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::float_power_outf(dispatchKeySet, self, exponent, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FLOAT_POWER_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, exponent);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_float_power_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & exponent) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::float_power(dispatchKeySet, self, exponent);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(exponent.dtype(), exponent.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FLOAT_POWER_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, exponent);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_float_power_Tensor_Scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & exponent, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::float_power_outf(dispatchKeySet, self, exponent, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FLOAT_POWER_TENSOR_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, exponent);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_float_power_Tensor_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & exponent) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::float_power(dispatchKeySet, self, exponent);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FLOAT_POWER_TENSOR_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, exponent);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_float_power__Scalar(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & exponent) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::float_power_(dispatchKeySet, self, exponent);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FLOAT_POWER__SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, exponent);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_float_power__Tensor(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & exponent) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::float_power_(dispatchKeySet, self, exponent);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FLOAT_POWER__TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, exponent);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_normal_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::normal_(dispatchKeySet, self, mean, std, std::move(generator));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NORMAL_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, mean);trace.append_arg(trace_idx, std);trace.append_arg(trace_idx, std::move(generator));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_normal_Tensor_float_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & mean, double std, c10::optional<at::Generator> generator, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::normal_outf(dispatchKeySet, mean, std, std::move(generator), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NORMAL_TENSOR_FLOAT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, mean);trace.append_arg(trace_idx, std);trace.append_arg(trace_idx, std::move(generator));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_normal_Tensor_float(c10::DispatchKeySet dispatchKeySet, const at::Tensor & mean, double std, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::normal(dispatchKeySet, mean, std, std::move(generator));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(mean.dtype(), mean.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NORMAL_TENSOR_FLOAT, dispatchKeySet);
  trace.append_arg(trace_idx, mean);trace.append_arg(trace_idx, std);trace.append_arg(trace_idx, std::move(generator));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_normal_float_Tensor_out(c10::DispatchKeySet dispatchKeySet, double mean, const at::Tensor & std, c10::optional<at::Generator> generator, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::normal_outf(dispatchKeySet, mean, std, std::move(generator), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NORMAL_FLOAT_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, mean);trace.append_arg(trace_idx, std);trace.append_arg(trace_idx, std::move(generator));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_normal_float_Tensor(c10::DispatchKeySet dispatchKeySet, double mean, const at::Tensor & std, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::normal(dispatchKeySet, mean, std, std::move(generator));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(std.dtype(), std.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NORMAL_FLOAT_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, mean);trace.append_arg(trace_idx, std);trace.append_arg(trace_idx, std::move(generator));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_normal_Tensor_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & mean, const at::Tensor & std, c10::optional<at::Generator> generator, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::normal_outf(dispatchKeySet, mean, std, std::move(generator), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NORMAL_TENSOR_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, mean);trace.append_arg(trace_idx, std);trace.append_arg(trace_idx, std::move(generator));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_normal_Tensor_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & mean, const at::Tensor & std, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::normal(dispatchKeySet, mean, std, std::move(generator));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(mean.dtype(), mean.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NORMAL_TENSOR_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, mean);trace.append_arg(trace_idx, std);trace.append_arg(trace_idx, std::move(generator));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_normal_float_float(c10::DispatchKeySet dispatchKeySet, double mean, double std, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::normal(dispatchKeySet, mean, std, std::move(size), std::move(generator), std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NORMAL_FLOAT_FLOAT, dispatchKeySet);
  trace.append_arg(trace_idx, mean);trace.append_arg(trace_idx, std);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(generator));trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_normal_float_float_out(c10::DispatchKeySet dispatchKeySet, double mean, double std, at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::normal_outf(dispatchKeySet, mean, std, std::move(size), std::move(generator), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NORMAL_FLOAT_FLOAT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, mean);trace.append_arg(trace_idx, std);trace.append_arg(trace_idx, std::move(size));trace.append_arg(trace_idx, std::move(generator));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_alias(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::alias(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ALIAS, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap__index_copy_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_index_copy_(dispatchKeySet, self, dim, index, source);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__INDEX_COPY_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, index);trace.append_arg(trace_idx, source);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap__cumsum(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_cumsum(dispatchKeySet, self, dim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__CUMSUM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap__cumsum_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_cumsum_outf(dispatchKeySet, self, dim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__CUMSUM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap__cumprod(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_cumprod(dispatchKeySet, self, dim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__CUMPROD, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap__cumprod_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_cumprod_outf(dispatchKeySet, self, dim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__CUMPROD_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

void wrap__amp_foreach_non_finite_check_and_unscale_(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::Tensor & found_inf, const at::Tensor & inv_scale) {
  ensure_materialized(self);ensure_materialized(found_inf);ensure_materialized(inv_scale);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_amp_foreach_non_finite_check_and_unscale_(dispatchKeySet, std::move(self), found_inf, inv_scale);
}

at::Tensor & wrap__amp_update_scale_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Tensor & growth_tracker, const at::Tensor & found_inf, double scale_growth_factor, double scale_backoff_factor, int64_t growth_interval) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_amp_update_scale_(dispatchKeySet, self, growth_tracker, found_inf, scale_growth_factor, scale_backoff_factor, growth_interval);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__AMP_UPDATE_SCALE_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, growth_tracker);trace.append_arg(trace_idx, found_inf);trace.append_arg(trace_idx, scale_growth_factor);trace.append_arg(trace_idx, scale_backoff_factor);trace.append_arg(trace_idx, growth_interval);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap__cat(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_cat(dispatchKeySet, std::move(tensors), dim);
  }
  auto defaults = compute_dtype(tensors);
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(default_dtype, default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__CAT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));trace.append_arg(trace_idx, dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap__cat_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_cat_outf(dispatchKeySet, std::move(tensors), dim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__CAT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

std::vector<at::Tensor> wrap__foreach_add_Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, const at::Scalar & scalar) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_add(dispatchKeySet, std::move(tensors), scalar);
}

void wrap__foreach_add__Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_add_(dispatchKeySet, std::move(self), scalar);
}

std::vector<at::Tensor> wrap__foreach_sub_Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, const at::Scalar & scalar) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_sub(dispatchKeySet, std::move(tensors), scalar);
}

void wrap__foreach_sub__Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_sub_(dispatchKeySet, std::move(self), scalar);
}

std::vector<at::Tensor> wrap__foreach_mul_Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, const at::Scalar & scalar) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_mul(dispatchKeySet, std::move(tensors), scalar);
}

void wrap__foreach_mul__Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_mul_(dispatchKeySet, std::move(self), scalar);
}

std::vector<at::Tensor> wrap__foreach_div_Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, const at::Scalar & scalar) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_div(dispatchKeySet, std::move(tensors), scalar);
}

void wrap__foreach_div__Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_div_(dispatchKeySet, std::move(self), scalar);
}

std::vector<at::Tensor> wrap__foreach_add_List(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors1, at::TensorList tensors2, const at::Scalar & alpha) {
  ensure_materialized(tensors1);ensure_materialized(tensors2);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_add(dispatchKeySet, std::move(tensors1), std::move(tensors2), alpha);
}

void wrap__foreach_add__List(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other, const at::Scalar & alpha) {
  ensure_materialized(self);ensure_materialized(other);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_add_(dispatchKeySet, std::move(self), std::move(other), alpha);
}

std::vector<at::Tensor> wrap__foreach_sub_List(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors1, at::TensorList tensors2, const at::Scalar & alpha) {
  ensure_materialized(tensors1);ensure_materialized(tensors2);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_sub(dispatchKeySet, std::move(tensors1), std::move(tensors2), alpha);
}

void wrap__foreach_sub__List(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other, const at::Scalar & alpha) {
  ensure_materialized(self);ensure_materialized(other);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_sub_(dispatchKeySet, std::move(self), std::move(other), alpha);
}

std::vector<at::Tensor> wrap__foreach_mul_List(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors1, at::TensorList tensors2) {
  ensure_materialized(tensors1);ensure_materialized(tensors2);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_mul(dispatchKeySet, std::move(tensors1), std::move(tensors2));
}

void wrap__foreach_mul__List(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other) {
  ensure_materialized(self);ensure_materialized(other);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_mul_(dispatchKeySet, std::move(self), std::move(other));
}

std::vector<at::Tensor> wrap__foreach_div_List(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors1, at::TensorList tensors2) {
  ensure_materialized(tensors1);ensure_materialized(tensors2);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_div(dispatchKeySet, std::move(tensors1), std::move(tensors2));
}

void wrap__foreach_div__List(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other) {
  ensure_materialized(self);ensure_materialized(other);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_div_(dispatchKeySet, std::move(self), std::move(other));
}

std::vector<at::Tensor> wrap__foreach_add_ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::ArrayRef<at::Scalar> scalars) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_add(dispatchKeySet, std::move(tensors), scalars);
}

void wrap__foreach_add__ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_add_(dispatchKeySet, std::move(self), scalars);
}

std::vector<at::Tensor> wrap__foreach_sub_ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::ArrayRef<at::Scalar> scalars) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_sub(dispatchKeySet, std::move(tensors), scalars);
}

void wrap__foreach_sub__ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_sub_(dispatchKeySet, std::move(self), scalars);
}

std::vector<at::Tensor> wrap__foreach_div_ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::ArrayRef<at::Scalar> scalars) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_div(dispatchKeySet, std::move(tensors), scalars);
}

void wrap__foreach_div__ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_div_(dispatchKeySet, std::move(self), scalars);
}

std::vector<at::Tensor> wrap__foreach_mul_ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::ArrayRef<at::Scalar> scalars) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_mul(dispatchKeySet, std::move(tensors), scalars);
}

void wrap__foreach_mul__ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_mul_(dispatchKeySet, std::move(self), scalars);
}

std::vector<at::Tensor> wrap__foreach_exp(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_exp(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_zero_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_zero_(dispatchKeySet, std::move(self));
}

void wrap__foreach_exp_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_exp_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_sqrt(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_sqrt(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_sqrt_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_sqrt_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_abs(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_abs(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_abs_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_abs_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_acos(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_acos(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_acos_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_acos_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_asin(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_asin(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_asin_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_asin_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_atan(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_atan(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_atan_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_atan_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_ceil(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_ceil(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_ceil_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_ceil_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_cos(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_cos(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_cos_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_cos_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_cosh(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_cosh(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_cosh_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_cosh_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_erf(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_erf(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_erf_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_erf_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_erfc(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_erfc(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_erfc_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_erfc_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_expm1(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_expm1(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_expm1_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_expm1_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_floor(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_floor(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_floor_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_floor_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_log(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_log(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_log_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_log_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_log10(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_log10(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_log10_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_log10_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_log1p(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_log1p(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_log1p_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_log1p_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_log2(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_log2(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_log2_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_log2_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_neg(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_neg(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_neg_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_neg_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_tan(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_tan(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_tan_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_tan_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_tanh(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_tanh(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_tanh_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_tanh_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_sin(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_sin(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_sin_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_sin_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_sinh(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_sinh(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_sinh_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_sinh_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_round(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_round(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_round_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_round_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_lgamma(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_lgamma(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_lgamma_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_lgamma_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_frac(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_frac(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_frac_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_frac_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_reciprocal(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_reciprocal(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_reciprocal_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_reciprocal_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_sigmoid(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_sigmoid(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_sigmoid_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_sigmoid_(dispatchKeySet, std::move(self));
}

std::vector<at::Tensor> wrap__foreach_trunc(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_trunc(dispatchKeySet, std::move(tensors));
}

void wrap__foreach_trunc_(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_trunc_(dispatchKeySet, std::move(self));
}

void wrap__foreach_addcdiv__Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
  ensure_materialized(self);ensure_materialized(tensor1);ensure_materialized(tensor2);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_addcdiv_(dispatchKeySet, std::move(self), std::move(tensor1), std::move(tensor2), value);
}

void wrap__foreach_addcmul__Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
  ensure_materialized(self);ensure_materialized(tensor1);ensure_materialized(tensor2);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_addcmul_(dispatchKeySet, std::move(self), std::move(tensor1), std::move(tensor2), value);
}

void wrap__foreach_addcdiv__ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
  ensure_materialized(self);ensure_materialized(tensor1);ensure_materialized(tensor2);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_addcdiv_(dispatchKeySet, std::move(self), std::move(tensor1), std::move(tensor2), scalars);
}

void wrap__foreach_addcmul__ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
  ensure_materialized(self);ensure_materialized(tensor1);ensure_materialized(tensor2);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_addcmul_(dispatchKeySet, std::move(self), std::move(tensor1), std::move(tensor2), scalars);
}

std::vector<at::Tensor> wrap__foreach_addcdiv_Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList input, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
  ensure_materialized(input);ensure_materialized(tensor1);ensure_materialized(tensor2);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_addcdiv(dispatchKeySet, std::move(input), std::move(tensor1), std::move(tensor2), value);
}

std::vector<at::Tensor> wrap__foreach_addcmul_Scalar(c10::DispatchKeySet dispatchKeySet, at::TensorList input, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
  ensure_materialized(input);ensure_materialized(tensor1);ensure_materialized(tensor2);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_addcmul(dispatchKeySet, std::move(input), std::move(tensor1), std::move(tensor2), value);
}

std::vector<at::Tensor> wrap__foreach_addcdiv_ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList input, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
  ensure_materialized(input);ensure_materialized(tensor1);ensure_materialized(tensor2);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_addcdiv(dispatchKeySet, std::move(input), std::move(tensor1), std::move(tensor2), scalars);
}

std::vector<at::Tensor> wrap__foreach_addcmul_ScalarList(c10::DispatchKeySet dispatchKeySet, at::TensorList input, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
  ensure_materialized(input);ensure_materialized(tensor1);ensure_materialized(tensor2);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_addcmul(dispatchKeySet, std::move(input), std::move(tensor1), std::move(tensor2), scalars);
}

std::vector<at::Tensor> wrap__foreach_maximum_List(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors1, at::TensorList tensors2) {
  ensure_materialized(tensors1);ensure_materialized(tensors2);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_maximum(dispatchKeySet, std::move(tensors1), std::move(tensors2));
}

std::vector<at::Tensor> wrap__foreach_minimum_List(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors1, at::TensorList tensors2) {
  ensure_materialized(tensors1);ensure_materialized(tensors2);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_foreach_minimum(dispatchKeySet, std::move(tensors1), std::move(tensors2));
}

at::Tensor wrap_bucketize_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & boundaries, bool out_int32, bool right) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bucketize(dispatchKeySet, self, boundaries, out_int32, right);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BUCKETIZE_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, boundaries);trace.append_arg(trace_idx, out_int32);trace.append_arg(trace_idx, right);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_bucketize_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & boundaries, bool out_int32, bool right, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bucketize_outf(dispatchKeySet, self, boundaries, out_int32, right, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_BUCKETIZE_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, boundaries);trace.append_arg(trace_idx, out_int32);trace.append_arg(trace_idx, right);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_bucketize_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & boundaries, bool out_int32, bool right) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::bucketize(dispatchKeySet, self, boundaries, out_int32, right);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(boundaries.dtype(), boundaries.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_BUCKETIZE_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, boundaries);trace.append_arg(trace_idx, out_int32);trace.append_arg(trace_idx, right);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_searchsorted_Tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & sorted_sequence, const at::Tensor & self, bool out_int32, bool right) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::searchsorted(dispatchKeySet, sorted_sequence, self, out_int32, right);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(sorted_sequence.dtype(), sorted_sequence.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SEARCHSORTED_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, sorted_sequence);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out_int32);trace.append_arg(trace_idx, right);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_searchsorted_Tensor_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & sorted_sequence, const at::Tensor & self, bool out_int32, bool right, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::searchsorted_outf(dispatchKeySet, sorted_sequence, self, out_int32, right, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SEARCHSORTED_TENSOR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, sorted_sequence);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out_int32);trace.append_arg(trace_idx, right);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_searchsorted_Scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & sorted_sequence, const at::Scalar & self, bool out_int32, bool right) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::searchsorted(dispatchKeySet, sorted_sequence, self, out_int32, right);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(sorted_sequence.dtype(), sorted_sequence.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SEARCHSORTED_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, sorted_sequence);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out_int32);trace.append_arg(trace_idx, right);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_mse_loss_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mse_loss_outf(dispatchKeySet, self, target, reduction, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MSE_LOSS_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_mse_loss(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mse_loss(dispatchKeySet, self, target, reduction);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MSE_LOSS, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_mse_loss_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mse_loss_backward_outf(dispatchKeySet, grad_output, self, target, reduction, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MSE_LOSS_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_mse_loss_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mse_loss_backward(dispatchKeySet, grad_output, self, target, reduction);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MSE_LOSS_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_l1_loss_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::l1_loss_outf(dispatchKeySet, self, target, reduction, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_L1_LOSS_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_l1_loss(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::l1_loss(dispatchKeySet, self, target, reduction);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_L1_LOSS, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_l1_loss_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::l1_loss_backward_outf(dispatchKeySet, grad_output, self, target, reduction, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_L1_LOSS_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_l1_loss_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::l1_loss_backward(dispatchKeySet, grad_output, self, target, reduction);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_L1_LOSS_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_multi_margin_loss_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const at::Scalar & p, const at::Scalar & margin, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::multi_margin_loss_outf(dispatchKeySet, self, target, p, margin, weight, reduction, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MULTI_MARGIN_LOSS_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, margin);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_multi_margin_loss(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const at::Scalar & p, const at::Scalar & margin, const c10::optional<at::Tensor> & weight, int64_t reduction) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::multi_margin_loss(dispatchKeySet, self, target, p, margin, weight, reduction);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MULTI_MARGIN_LOSS, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, margin);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, reduction);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_multi_margin_loss_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const at::Scalar & p, const at::Scalar & margin, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::multi_margin_loss_backward_outf(dispatchKeySet, grad_output, self, target, p, margin, weight, reduction, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MULTI_MARGIN_LOSS_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, margin);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_multi_margin_loss_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const at::Scalar & p, const at::Scalar & margin, const c10::optional<at::Tensor> & weight, int64_t reduction) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::multi_margin_loss_backward(dispatchKeySet, grad_output, self, target, p, margin, weight, reduction);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MULTI_MARGIN_LOSS_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, margin);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, reduction);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_multilabel_margin_loss_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::multilabel_margin_loss_outf(dispatchKeySet, self, target, reduction, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MULTILABEL_MARGIN_LOSS_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_multilabel_margin_loss(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::multilabel_margin_loss(dispatchKeySet, self, target, reduction);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MULTILABEL_MARGIN_LOSS, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor &,at::Tensor &> wrap_multilabel_margin_loss_forward_output(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & output, at::Tensor & is_target) {
  ensure_materialized(self);ensure_materialized(target);ensure_materialized(output);ensure_materialized(is_target);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::multilabel_margin_loss_forward_outf(dispatchKeySet, self, target, reduction, output, is_target);
}

std::tuple<at::Tensor,at::Tensor> wrap_multilabel_margin_loss_forward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
  ensure_materialized(self);ensure_materialized(target);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::multilabel_margin_loss_forward(dispatchKeySet, self, target, reduction);
}

at::Tensor & wrap_multilabel_margin_loss_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, const at::Tensor & is_target, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::multilabel_margin_loss_backward_outf(dispatchKeySet, grad_output, self, target, reduction, is_target, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MULTILABEL_MARGIN_LOSS_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, is_target);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_multilabel_margin_loss_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, const at::Tensor & is_target) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::multilabel_margin_loss_backward(dispatchKeySet, grad_output, self, target, reduction, is_target);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MULTILABEL_MARGIN_LOSS_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, is_target);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_nll_loss_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nll_loss_outf(dispatchKeySet, self, target, weight, reduction, ignore_index, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NLL_LOSS_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, ignore_index);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_nll_loss_nd(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nll_loss_nd(dispatchKeySet, self, target, weight, reduction, ignore_index);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NLL_LOSS_ND, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, ignore_index);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_nll_loss(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nll_loss(dispatchKeySet, self, target, weight, reduction, ignore_index);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NLL_LOSS, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, ignore_index);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor &,at::Tensor &> wrap_nll_loss_forward_output(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, at::Tensor & output, at::Tensor & total_weight) {
  ensure_materialized(self);ensure_materialized(target);ensure_materialized(weight);ensure_materialized(output);ensure_materialized(total_weight);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::nll_loss_forward_outf(dispatchKeySet, self, target, weight, reduction, ignore_index, output, total_weight);
}

std::tuple<at::Tensor,at::Tensor> wrap_nll_loss_forward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
  ensure_materialized(self);ensure_materialized(target);ensure_materialized(weight);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::nll_loss_forward(dispatchKeySet, self, target, weight, reduction, ignore_index);
}

at::Tensor & wrap_nll_loss_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nll_loss_backward_outf(dispatchKeySet, grad_output, self, target, weight, reduction, ignore_index, total_weight, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NLL_LOSS_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, ignore_index);trace.append_arg(trace_idx, total_weight);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_nll_loss_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nll_loss_backward(dispatchKeySet, grad_output, self, target, weight, reduction, ignore_index, total_weight);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NLL_LOSS_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, ignore_index);trace.append_arg(trace_idx, total_weight);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_nll_loss2d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nll_loss2d_outf(dispatchKeySet, self, target, weight, reduction, ignore_index, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NLL_LOSS2D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, ignore_index);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_nll_loss2d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nll_loss2d(dispatchKeySet, self, target, weight, reduction, ignore_index);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NLL_LOSS2D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, ignore_index);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor &,at::Tensor &> wrap_nll_loss2d_forward_output(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, at::Tensor & output, at::Tensor & total_weight) {
  ensure_materialized(self);ensure_materialized(target);ensure_materialized(weight);ensure_materialized(output);ensure_materialized(total_weight);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::nll_loss2d_forward_outf(dispatchKeySet, self, target, weight, reduction, ignore_index, output, total_weight);
}

std::tuple<at::Tensor,at::Tensor> wrap_nll_loss2d_forward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
  ensure_materialized(self);ensure_materialized(target);ensure_materialized(weight);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::nll_loss2d_forward(dispatchKeySet, self, target, weight, reduction, ignore_index);
}

at::Tensor & wrap_nll_loss2d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nll_loss2d_backward_outf(dispatchKeySet, grad_output, self, target, weight, reduction, ignore_index, total_weight, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_NLL_LOSS2D_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, ignore_index);trace.append_arg(trace_idx, total_weight);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_nll_loss2d_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::nll_loss2d_backward(dispatchKeySet, grad_output, self, target, weight, reduction, ignore_index, total_weight);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_NLL_LOSS2D_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, ignore_index);trace.append_arg(trace_idx, total_weight);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_smooth_l1_loss_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::smooth_l1_loss_outf(dispatchKeySet, self, target, reduction, beta, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SMOOTH_L1_LOSS_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_smooth_l1_loss(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::smooth_l1_loss(dispatchKeySet, self, target, reduction, beta);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SMOOTH_L1_LOSS, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, beta);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_smooth_l1_loss_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::smooth_l1_loss_backward_outf(dispatchKeySet, grad_output, self, target, reduction, beta, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SMOOTH_L1_LOSS_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_smooth_l1_loss_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::smooth_l1_loss_backward(dispatchKeySet, grad_output, self, target, reduction, beta);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SMOOTH_L1_LOSS_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, beta);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_huber_loss_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::huber_loss_outf(dispatchKeySet, self, target, reduction, delta, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_HUBER_LOSS_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, delta);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_huber_loss(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::huber_loss(dispatchKeySet, self, target, reduction, delta);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_HUBER_LOSS, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, delta);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_huber_loss_backward_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::huber_loss_backward_outf(dispatchKeySet, grad_output, self, target, reduction, delta, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_HUBER_LOSS_BACKWARD_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, delta);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_huber_loss_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::huber_loss_backward(dispatchKeySet, grad_output, self, target, reduction, delta);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_HUBER_LOSS_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, delta);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_soft_margin_loss_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::soft_margin_loss_outf(dispatchKeySet, self, target, reduction, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SOFT_MARGIN_LOSS_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_soft_margin_loss(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::soft_margin_loss(dispatchKeySet, self, target, reduction);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SOFT_MARGIN_LOSS, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_soft_margin_loss_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::soft_margin_loss_backward_outf(dispatchKeySet, grad_output, self, target, reduction, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SOFT_MARGIN_LOSS_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_soft_margin_loss_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::soft_margin_loss_backward(dispatchKeySet, grad_output, self, target, reduction);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SOFT_MARGIN_LOSS_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, target);trace.append_arg(trace_idx, reduction);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_elu_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::elu_outf(dispatchKeySet, self, alpha, scale, input_scale, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ELU_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, alpha);trace.append_arg(trace_idx, scale);trace.append_arg(trace_idx, input_scale);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_elu_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, bool is_result, const at::Tensor & self_or_result, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::elu_backward_outf(dispatchKeySet, grad_output, alpha, scale, input_scale, is_result, self_or_result, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ELU_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, alpha);trace.append_arg(trace_idx, scale);trace.append_arg(trace_idx, input_scale);trace.append_arg(trace_idx, is_result);trace.append_arg(trace_idx, self_or_result);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor & wrap_elu_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::elu_(dispatchKeySet, self, alpha, scale, input_scale);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ELU_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, alpha);trace.append_arg(trace_idx, scale);trace.append_arg(trace_idx, input_scale);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_glu_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::glu_outf(dispatchKeySet, self, dim, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GLU_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_glu(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::glu(dispatchKeySet, self, dim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_GLU, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_glu_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, int64_t dim, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::glu_backward_outf(dispatchKeySet, grad_output, self, dim, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GLU_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_glu_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, int64_t dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::glu_backward(dispatchKeySet, grad_output, self, dim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_GLU_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_hardsigmoid_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hardsigmoid_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_HARDSIGMOID_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_hardsigmoid(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hardsigmoid(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_HARDSIGMOID, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_hardsigmoid_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hardsigmoid_backward_outf(dispatchKeySet, grad_output, self, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_HARDSIGMOID_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor & wrap_hardtanh_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hardtanh_outf(dispatchKeySet, self, min_val, max_val, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_HARDTANH_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min_val);trace.append_arg(trace_idx, max_val);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_hardtanh(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hardtanh(dispatchKeySet, self, min_val, max_val);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_HARDTANH, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min_val);trace.append_arg(trace_idx, max_val);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_hardtanh_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hardtanh_backward_outf(dispatchKeySet, grad_output, self, min_val, max_val, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_HARDTANH_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min_val);trace.append_arg(trace_idx, max_val);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_hardtanh_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hardtanh_backward(dispatchKeySet, grad_output, self, min_val, max_val);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_HARDTANH_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min_val);trace.append_arg(trace_idx, max_val);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_hardtanh_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hardtanh_(dispatchKeySet, self, min_val, max_val);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_HARDTANH_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, min_val);trace.append_arg(trace_idx, max_val);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_hardswish_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hardswish_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_HARDSWISH_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_hardswish(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hardswish(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_HARDSWISH, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_hardswish_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hardswish_(dispatchKeySet, self);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_HARDSWISH_, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_hardswish_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::hardswish_backward(dispatchKeySet, grad_output, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_HARDSWISH_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_leaky_relu_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & negative_slope, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::leaky_relu_outf(dispatchKeySet, self, negative_slope, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LEAKY_RELU_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, negative_slope);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_leaky_relu(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & negative_slope) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::leaky_relu(dispatchKeySet, self, negative_slope);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LEAKY_RELU, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, negative_slope);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_leaky_relu_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & negative_slope, bool self_is_result, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::leaky_relu_backward_outf(dispatchKeySet, grad_output, self, negative_slope, self_is_result, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LEAKY_RELU_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, negative_slope);trace.append_arg(trace_idx, self_is_result);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor & wrap_leaky_relu_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & negative_slope) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::leaky_relu_(dispatchKeySet, self, negative_slope);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LEAKY_RELU_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, negative_slope);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_log_sigmoid_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::log_sigmoid_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOG_SIGMOID_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_log_sigmoid(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::log_sigmoid(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LOG_SIGMOID, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor &,at::Tensor &> wrap_log_sigmoid_forward_output(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & output, at::Tensor & buffer) {
  ensure_materialized(self);ensure_materialized(output);ensure_materialized(buffer);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::log_sigmoid_forward_outf(dispatchKeySet, self, output, buffer);
}

std::tuple<at::Tensor,at::Tensor> wrap_log_sigmoid_forward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::log_sigmoid_forward(dispatchKeySet, self);
}

at::Tensor & wrap_log_sigmoid_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & buffer, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::log_sigmoid_backward_outf(dispatchKeySet, grad_output, self, buffer, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOG_SIGMOID_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, buffer);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_log_sigmoid_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & buffer) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::log_sigmoid_backward(dispatchKeySet, grad_output, self, buffer);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LOG_SIGMOID_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, buffer);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_rrelu_with_noise_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rrelu_with_noise_outf(dispatchKeySet, self, noise, lower, upper, training, std::move(generator), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RRELU_WITH_NOISE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, noise);trace.append_arg(trace_idx, lower);trace.append_arg(trace_idx, upper);trace.append_arg(trace_idx, training);trace.append_arg(trace_idx, std::move(generator));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_rrelu_with_noise(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rrelu_with_noise(dispatchKeySet, self, noise, lower, upper, training, std::move(generator));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RRELU_WITH_NOISE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, noise);trace.append_arg(trace_idx, lower);trace.append_arg(trace_idx, upper);trace.append_arg(trace_idx, training);trace.append_arg(trace_idx, std::move(generator));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_rrelu_with_noise_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, bool self_is_result) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rrelu_with_noise_backward(dispatchKeySet, grad_output, self, noise, lower, upper, training, self_is_result);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_RRELU_WITH_NOISE_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, noise);trace.append_arg(trace_idx, lower);trace.append_arg(trace_idx, upper);trace.append_arg(trace_idx, training);trace.append_arg(trace_idx, self_is_result);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_rrelu_with_noise_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::rrelu_with_noise_(dispatchKeySet, self, noise, lower, upper, training, std::move(generator));
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_RRELU_WITH_NOISE_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, noise);trace.append_arg(trace_idx, lower);trace.append_arg(trace_idx, upper);trace.append_arg(trace_idx, training);trace.append_arg(trace_idx, std::move(generator));
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor & wrap_softplus_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::softplus_outf(dispatchKeySet, self, beta, threshold, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SOFTPLUS_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, threshold);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_softplus_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold, const at::Tensor & output, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::softplus_backward_outf(dispatchKeySet, grad_output, self, beta, threshold, output, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SOFTPLUS_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, beta);trace.append_arg(trace_idx, threshold);trace.append_arg(trace_idx, output);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor & wrap_softshrink_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & lambd, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::softshrink_outf(dispatchKeySet, self, lambd, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SOFTSHRINK_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, lambd);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_softshrink_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & lambd, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::softshrink_backward_outf(dispatchKeySet, grad_output, self, lambd, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SOFTSHRINK_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, lambd);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_softshrink_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & lambd) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::softshrink_backward(dispatchKeySet, grad_output, self, lambd);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SOFTSHRINK_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, lambd);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_adaptive_avg_pool2d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::adaptive_avg_pool2d_outf(dispatchKeySet, self, std::move(output_size), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ADAPTIVE_AVG_POOL2D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_adaptive_avg_pool2d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::adaptive_avg_pool2d(dispatchKeySet, self, std::move(output_size));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ADAPTIVE_AVG_POOL2D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(output_size));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_mkldnn_adaptive_avg_pool2d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mkldnn_adaptive_avg_pool2d(dispatchKeySet, self, std::move(output_size));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MKLDNN_ADAPTIVE_AVG_POOL2D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(output_size));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_mkldnn_adaptive_avg_pool2d_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::mkldnn_adaptive_avg_pool2d_backward(dispatchKeySet, grad_output, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MKLDNN_ADAPTIVE_AVG_POOL2D_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__adaptive_avg_pool2d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_adaptive_avg_pool2d(dispatchKeySet, self, std::move(output_size));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__ADAPTIVE_AVG_POOL2D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(output_size));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__adaptive_avg_pool2d_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_adaptive_avg_pool2d_backward(dispatchKeySet, grad_output, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__ADAPTIVE_AVG_POOL2D_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_adaptive_avg_pool3d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::adaptive_avg_pool3d_outf(dispatchKeySet, self, std::move(output_size), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ADAPTIVE_AVG_POOL3D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_adaptive_avg_pool3d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::adaptive_avg_pool3d(dispatchKeySet, self, std::move(output_size));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ADAPTIVE_AVG_POOL3D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(output_size));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__adaptive_avg_pool3d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_adaptive_avg_pool3d(dispatchKeySet, self, std::move(output_size));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__ADAPTIVE_AVG_POOL3D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(output_size));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_adaptive_avg_pool3d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::adaptive_avg_pool3d_backward_outf(dispatchKeySet, grad_output, self, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ADAPTIVE_AVG_POOL3D_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap__adaptive_avg_pool3d_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_adaptive_avg_pool3d_backward(dispatchKeySet, grad_output, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__ADAPTIVE_AVG_POOL3D_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor &,at::Tensor &> wrap_adaptive_max_pool2d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(out);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::adaptive_max_pool2d_outf(dispatchKeySet, self, std::move(output_size), out, indices);
}

at::Tensor & wrap_adaptive_max_pool2d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::adaptive_max_pool2d_backward_outf(dispatchKeySet, grad_output, self, indices, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ADAPTIVE_MAX_POOL2D_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

std::tuple<at::Tensor &,at::Tensor &> wrap_adaptive_max_pool3d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(out);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::adaptive_max_pool3d_outf(dispatchKeySet, self, std::move(output_size), out, indices);
}

at::Tensor & wrap_adaptive_max_pool3d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::adaptive_max_pool3d_backward_outf(dispatchKeySet, grad_output, self, indices, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ADAPTIVE_MAX_POOL3D_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor & wrap_avg_pool2d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::avg_pool2d_outf(dispatchKeySet, self, std::move(kernel_size), std::move(stride), std::move(padding), ceil_mode, count_include_pad, divisor_override, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_AVG_POOL2D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, ceil_mode);trace.append_arg(trace_idx, count_include_pad);trace.append_arg(trace_idx, divisor_override);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_avg_pool2d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::avg_pool2d(dispatchKeySet, self, std::move(kernel_size), std::move(stride), std::move(padding), ceil_mode, count_include_pad, divisor_override);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_AVG_POOL2D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, ceil_mode);trace.append_arg(trace_idx, count_include_pad);trace.append_arg(trace_idx, divisor_override);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_avg_pool2d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::avg_pool2d_backward_outf(dispatchKeySet, grad_output, self, std::move(kernel_size), std::move(stride), std::move(padding), ceil_mode, count_include_pad, divisor_override, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_AVG_POOL2D_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, ceil_mode);trace.append_arg(trace_idx, count_include_pad);trace.append_arg(trace_idx, divisor_override);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_avg_pool2d_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::avg_pool2d_backward(dispatchKeySet, grad_output, self, std::move(kernel_size), std::move(stride), std::move(padding), ceil_mode, count_include_pad, divisor_override);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_AVG_POOL2D_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, ceil_mode);trace.append_arg(trace_idx, count_include_pad);trace.append_arg(trace_idx, divisor_override);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_avg_pool3d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::avg_pool3d_outf(dispatchKeySet, self, std::move(kernel_size), std::move(stride), std::move(padding), ceil_mode, count_include_pad, divisor_override, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_AVG_POOL3D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, ceil_mode);trace.append_arg(trace_idx, count_include_pad);trace.append_arg(trace_idx, divisor_override);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_avg_pool3d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::avg_pool3d(dispatchKeySet, self, std::move(kernel_size), std::move(stride), std::move(padding), ceil_mode, count_include_pad, divisor_override);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_AVG_POOL3D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, ceil_mode);trace.append_arg(trace_idx, count_include_pad);trace.append_arg(trace_idx, divisor_override);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_avg_pool3d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::avg_pool3d_backward_outf(dispatchKeySet, grad_output, self, std::move(kernel_size), std::move(stride), std::move(padding), ceil_mode, count_include_pad, divisor_override, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_AVG_POOL3D_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, ceil_mode);trace.append_arg(trace_idx, count_include_pad);trace.append_arg(trace_idx, divisor_override);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_avg_pool3d_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::avg_pool3d_backward(dispatchKeySet, grad_output, self, std::move(kernel_size), std::move(stride), std::move(padding), ceil_mode, count_include_pad, divisor_override);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_AVG_POOL3D_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, ceil_mode);trace.append_arg(trace_idx, count_include_pad);trace.append_arg(trace_idx, divisor_override);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor &,at::Tensor &> wrap_fractional_max_pool2d_output(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples, at::Tensor & output, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(random_samples);ensure_materialized(output);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::fractional_max_pool2d_outf(dispatchKeySet, self, std::move(kernel_size), std::move(output_size), random_samples, output, indices);
}

at::Tensor & wrap_fractional_max_pool2d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & indices, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fractional_max_pool2d_backward_outf(dispatchKeySet, grad_output, self, std::move(kernel_size), std::move(output_size), indices, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FRACTIONAL_MAX_POOL2D_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_fractional_max_pool2d_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & indices) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fractional_max_pool2d_backward(dispatchKeySet, grad_output, self, std::move(kernel_size), std::move(output_size), indices);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FRACTIONAL_MAX_POOL2D_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, indices);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor &,at::Tensor &> wrap_fractional_max_pool3d_output(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples, at::Tensor & output, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(random_samples);ensure_materialized(output);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::fractional_max_pool3d_outf(dispatchKeySet, self, std::move(kernel_size), std::move(output_size), random_samples, output, indices);
}

std::tuple<at::Tensor,at::Tensor> wrap_fractional_max_pool3d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples) {
  ensure_materialized(self);ensure_materialized(random_samples);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::fractional_max_pool3d(dispatchKeySet, self, std::move(kernel_size), std::move(output_size), random_samples);
}

at::Tensor & wrap_fractional_max_pool3d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & indices, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fractional_max_pool3d_backward_outf(dispatchKeySet, grad_output, self, std::move(kernel_size), std::move(output_size), indices, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FRACTIONAL_MAX_POOL3D_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_fractional_max_pool3d_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & indices) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fractional_max_pool3d_backward(dispatchKeySet, grad_output, self, std::move(kernel_size), std::move(output_size), indices);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FRACTIONAL_MAX_POOL3D_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, indices);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor &,at::Tensor &> wrap_max_pool2d_with_indices_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(out);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::max_pool2d_with_indices_outf(dispatchKeySet, self, std::move(kernel_size), std::move(stride), std::move(padding), std::move(dilation), ceil_mode, out, indices);
}

at::Tensor & wrap_max_pool2d_with_indices_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::max_pool2d_with_indices_backward_outf(dispatchKeySet, grad_output, self, std::move(kernel_size), std::move(stride), std::move(padding), std::move(dilation), ceil_mode, indices, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MAX_POOL2D_WITH_INDICES_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, ceil_mode);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

std::tuple<at::Tensor &,at::Tensor &> wrap_max_pool3d_with_indices_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out, at::Tensor & indices) {
  ensure_materialized(self);ensure_materialized(out);ensure_materialized(indices);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::max_pool3d_with_indices_outf(dispatchKeySet, self, std::move(kernel_size), std::move(stride), std::move(padding), std::move(dilation), ceil_mode, out, indices);
}

std::tuple<at::Tensor,at::Tensor> wrap_max_pool3d_with_indices(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::max_pool3d_with_indices(dispatchKeySet, self, std::move(kernel_size), std::move(stride), std::move(padding), std::move(dilation), ceil_mode);
}

at::Tensor & wrap_max_pool3d_with_indices_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::max_pool3d_with_indices_backward_outf(dispatchKeySet, grad_output, self, std::move(kernel_size), std::move(stride), std::move(padding), std::move(dilation), ceil_mode, indices, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MAX_POOL3D_WITH_INDICES_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, ceil_mode);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_max_pool3d_with_indices_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::max_pool3d_with_indices_backward(dispatchKeySet, grad_output, self, std::move(kernel_size), std::move(stride), std::move(padding), std::move(dilation), ceil_mode, indices);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MAX_POOL3D_WITH_INDICES_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, ceil_mode);trace.append_arg(trace_idx, indices);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_max_unpool2d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::max_unpool2d_outf(dispatchKeySet, self, indices, std::move(output_size), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MAX_UNPOOL2D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_max_unpool2d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::max_unpool2d(dispatchKeySet, self, indices, std::move(output_size));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MAX_UNPOOL2D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, std::move(output_size));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_max_unpool2d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::max_unpool2d_backward_outf(dispatchKeySet, grad_output, self, indices, std::move(output_size), grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MAX_UNPOOL2D_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_max_unpool2d_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::max_unpool2d_backward(dispatchKeySet, grad_output, self, indices, std::move(output_size));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MAX_UNPOOL2D_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, std::move(output_size));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_max_unpool3d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::max_unpool3d_outf(dispatchKeySet, self, indices, std::move(output_size), std::move(stride), std::move(padding), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MAX_UNPOOL3D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_max_unpool3d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::max_unpool3d(dispatchKeySet, self, indices, std::move(output_size), std::move(stride), std::move(padding));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MAX_UNPOOL3D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_max_unpool3d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::max_unpool3d_backward_outf(dispatchKeySet, grad_output, self, indices, std::move(output_size), std::move(stride), std::move(padding), grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_MAX_UNPOOL3D_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_max_unpool3d_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::max_unpool3d_backward(dispatchKeySet, grad_output, self, indices, std::move(output_size), std::move(stride), std::move(padding));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_MAX_UNPOOL3D_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_reflection_pad1d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::reflection_pad1d_outf(dispatchKeySet, self, std::move(padding), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_REFLECTION_PAD1D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_reflection_pad1d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::reflection_pad1d(dispatchKeySet, self, std::move(padding));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_REFLECTION_PAD1D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_reflection_pad1d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::reflection_pad1d_backward_outf(dispatchKeySet, grad_output, self, std::move(padding), grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_REFLECTION_PAD1D_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_reflection_pad1d_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::reflection_pad1d_backward(dispatchKeySet, grad_output, self, std::move(padding));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_REFLECTION_PAD1D_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_reflection_pad2d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::reflection_pad2d_outf(dispatchKeySet, self, std::move(padding), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_REFLECTION_PAD2D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_reflection_pad2d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::reflection_pad2d(dispatchKeySet, self, std::move(padding));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_REFLECTION_PAD2D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_reflection_pad2d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::reflection_pad2d_backward_outf(dispatchKeySet, grad_output, self, std::move(padding), grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_REFLECTION_PAD2D_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_reflection_pad2d_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::reflection_pad2d_backward(dispatchKeySet, grad_output, self, std::move(padding));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_REFLECTION_PAD2D_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_replication_pad1d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::replication_pad1d_outf(dispatchKeySet, self, std::move(padding), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_REPLICATION_PAD1D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_replication_pad1d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::replication_pad1d_backward_outf(dispatchKeySet, grad_output, self, std::move(padding), grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_REPLICATION_PAD1D_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor & wrap_replication_pad2d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::replication_pad2d_outf(dispatchKeySet, self, std::move(padding), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_REPLICATION_PAD2D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_replication_pad2d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::replication_pad2d_backward_outf(dispatchKeySet, grad_output, self, std::move(padding), grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_REPLICATION_PAD2D_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_replication_pad2d_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::replication_pad2d_backward(dispatchKeySet, grad_output, self, std::move(padding));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_REPLICATION_PAD2D_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_replication_pad3d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::replication_pad3d_outf(dispatchKeySet, self, std::move(padding), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_REPLICATION_PAD3D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_replication_pad3d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::replication_pad3d_backward_outf(dispatchKeySet, grad_output, self, std::move(padding), grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_REPLICATION_PAD3D_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_replication_pad3d_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::replication_pad3d_backward(dispatchKeySet, grad_output, self, std::move(padding));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_REPLICATION_PAD3D_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(padding));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_upsample_linear1d_vec(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_linear1d(dispatchKeySet, input, std::move(output_size), align_corners, scale_factors);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_UPSAMPLE_LINEAR1D_VEC, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, align_corners);trace.append_arg(trace_idx, scale_factors);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_upsample_linear1d_backward_vec(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_linear1d_backward(dispatchKeySet, grad_output, std::move(output_size), std::move(input_size), align_corners, scale_factors);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_UPSAMPLE_LINEAR1D_BACKWARD_VEC, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, std::move(input_size));trace.append_arg(trace_idx, align_corners);trace.append_arg(trace_idx, scale_factors);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_upsample_bilinear2d_vec(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_bilinear2d(dispatchKeySet, input, std::move(output_size), align_corners, scale_factors);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_UPSAMPLE_BILINEAR2D_VEC, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, align_corners);trace.append_arg(trace_idx, scale_factors);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_upsample_bilinear2d_backward_vec(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_bilinear2d_backward(dispatchKeySet, grad_output, std::move(output_size), std::move(input_size), align_corners, scale_factors);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_UPSAMPLE_BILINEAR2D_BACKWARD_VEC, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, std::move(input_size));trace.append_arg(trace_idx, align_corners);trace.append_arg(trace_idx, scale_factors);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_upsample_trilinear3d_vec(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_trilinear3d(dispatchKeySet, input, std::move(output_size), align_corners, scale_factors);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_UPSAMPLE_TRILINEAR3D_VEC, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, align_corners);trace.append_arg(trace_idx, scale_factors);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_upsample_trilinear3d_backward_vec(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_trilinear3d_backward(dispatchKeySet, grad_output, std::move(output_size), std::move(input_size), align_corners, scale_factors);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_UPSAMPLE_TRILINEAR3D_BACKWARD_VEC, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, std::move(input_size));trace.append_arg(trace_idx, align_corners);trace.append_arg(trace_idx, scale_factors);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_upsample_bicubic2d_vec(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_bicubic2d(dispatchKeySet, input, std::move(output_size), align_corners, scale_factors);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_UPSAMPLE_BICUBIC2D_VEC, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, align_corners);trace.append_arg(trace_idx, scale_factors);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_upsample_bicubic2d_backward_vec(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_bicubic2d_backward(dispatchKeySet, grad_output, std::move(output_size), std::move(input_size), align_corners, scale_factors);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_UPSAMPLE_BICUBIC2D_BACKWARD_VEC, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, std::move(input_size));trace.append_arg(trace_idx, align_corners);trace.append_arg(trace_idx, scale_factors);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_upsample_nearest1d_vec(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_nearest1d(dispatchKeySet, input, std::move(output_size), scale_factors);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_UPSAMPLE_NEAREST1D_VEC, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, scale_factors);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_upsample_nearest1d_backward_vec(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, c10::optional<at::ArrayRef<double>> scale_factors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_nearest1d_backward(dispatchKeySet, grad_output, std::move(output_size), std::move(input_size), scale_factors);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_UPSAMPLE_NEAREST1D_BACKWARD_VEC, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, std::move(input_size));trace.append_arg(trace_idx, scale_factors);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_upsample_nearest2d_vec(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_nearest2d(dispatchKeySet, input, std::move(output_size), scale_factors);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_UPSAMPLE_NEAREST2D_VEC, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, scale_factors);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_upsample_nearest2d_backward_vec(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, c10::optional<at::ArrayRef<double>> scale_factors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_nearest2d_backward(dispatchKeySet, grad_output, std::move(output_size), std::move(input_size), scale_factors);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_UPSAMPLE_NEAREST2D_BACKWARD_VEC, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, std::move(input_size));trace.append_arg(trace_idx, scale_factors);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_upsample_nearest3d_vec(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_nearest3d(dispatchKeySet, input, std::move(output_size), scale_factors);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_UPSAMPLE_NEAREST3D_VEC, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, scale_factors);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_upsample_nearest3d_backward_vec(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, c10::optional<at::ArrayRef<double>> scale_factors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_nearest3d_backward(dispatchKeySet, grad_output, std::move(output_size), std::move(input_size), scale_factors);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_UPSAMPLE_NEAREST3D_BACKWARD_VEC, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, std::move(input_size));trace.append_arg(trace_idx, scale_factors);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_upsample_linear1d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_linear1d_outf(dispatchKeySet, self, std::move(output_size), align_corners, scales, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_UPSAMPLE_LINEAR1D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, align_corners);trace.append_arg(trace_idx, scales);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_upsample_linear1d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_linear1d_backward_outf(dispatchKeySet, grad_output, std::move(output_size), std::move(input_size), align_corners, scales, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_UPSAMPLE_LINEAR1D_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, std::move(input_size));trace.append_arg(trace_idx, align_corners);trace.append_arg(trace_idx, scales);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor & wrap_upsample_bilinear2d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_bilinear2d_outf(dispatchKeySet, self, std::move(output_size), align_corners, scales_h, scales_w, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_UPSAMPLE_BILINEAR2D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, align_corners);trace.append_arg(trace_idx, scales_h);trace.append_arg(trace_idx, scales_w);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_upsample_bilinear2d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_bilinear2d(dispatchKeySet, self, std::move(output_size), align_corners, scales_h, scales_w);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_UPSAMPLE_BILINEAR2D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, align_corners);trace.append_arg(trace_idx, scales_h);trace.append_arg(trace_idx, scales_w);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_upsample_bilinear2d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_bilinear2d_backward_outf(dispatchKeySet, grad_output, std::move(output_size), std::move(input_size), align_corners, scales_h, scales_w, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_UPSAMPLE_BILINEAR2D_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, std::move(input_size));trace.append_arg(trace_idx, align_corners);trace.append_arg(trace_idx, scales_h);trace.append_arg(trace_idx, scales_w);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor & wrap_upsample_bicubic2d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_bicubic2d_outf(dispatchKeySet, self, std::move(output_size), align_corners, scales_h, scales_w, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_UPSAMPLE_BICUBIC2D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, align_corners);trace.append_arg(trace_idx, scales_h);trace.append_arg(trace_idx, scales_w);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_upsample_bicubic2d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_bicubic2d_backward_outf(dispatchKeySet, grad_output, std::move(output_size), std::move(input_size), align_corners, scales_h, scales_w, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_UPSAMPLE_BICUBIC2D_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, std::move(input_size));trace.append_arg(trace_idx, align_corners);trace.append_arg(trace_idx, scales_h);trace.append_arg(trace_idx, scales_w);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor & wrap_upsample_trilinear3d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_trilinear3d_outf(dispatchKeySet, self, std::move(output_size), align_corners, scales_d, scales_h, scales_w, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_UPSAMPLE_TRILINEAR3D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, align_corners);trace.append_arg(trace_idx, scales_d);trace.append_arg(trace_idx, scales_h);trace.append_arg(trace_idx, scales_w);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_upsample_trilinear3d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_trilinear3d_backward_outf(dispatchKeySet, grad_output, std::move(output_size), std::move(input_size), align_corners, scales_d, scales_h, scales_w, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_UPSAMPLE_TRILINEAR3D_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, std::move(input_size));trace.append_arg(trace_idx, align_corners);trace.append_arg(trace_idx, scales_d);trace.append_arg(trace_idx, scales_h);trace.append_arg(trace_idx, scales_w);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor & wrap_upsample_nearest1d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_nearest1d_outf(dispatchKeySet, self, std::move(output_size), scales, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_UPSAMPLE_NEAREST1D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, scales);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_upsample_nearest1d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_nearest1d_backward_outf(dispatchKeySet, grad_output, std::move(output_size), std::move(input_size), scales, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_UPSAMPLE_NEAREST1D_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, std::move(input_size));trace.append_arg(trace_idx, scales);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor & wrap_upsample_nearest2d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_nearest2d_outf(dispatchKeySet, self, std::move(output_size), scales_h, scales_w, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_UPSAMPLE_NEAREST2D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, scales_h);trace.append_arg(trace_idx, scales_w);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_upsample_nearest2d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_nearest2d(dispatchKeySet, self, std::move(output_size), scales_h, scales_w);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_UPSAMPLE_NEAREST2D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, scales_h);trace.append_arg(trace_idx, scales_w);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_upsample_nearest2d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_nearest2d_backward_outf(dispatchKeySet, grad_output, std::move(output_size), std::move(input_size), scales_h, scales_w, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_UPSAMPLE_NEAREST2D_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, std::move(input_size));trace.append_arg(trace_idx, scales_h);trace.append_arg(trace_idx, scales_w);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor & wrap_upsample_nearest3d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_nearest3d_outf(dispatchKeySet, self, std::move(output_size), scales_d, scales_h, scales_w, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_UPSAMPLE_NEAREST3D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, scales_d);trace.append_arg(trace_idx, scales_h);trace.append_arg(trace_idx, scales_w);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_upsample_nearest3d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_nearest3d(dispatchKeySet, self, std::move(output_size), scales_d, scales_h, scales_w);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_UPSAMPLE_NEAREST3D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, scales_d);trace.append_arg(trace_idx, scales_h);trace.append_arg(trace_idx, scales_w);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_upsample_nearest3d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::upsample_nearest3d_backward_outf(dispatchKeySet, grad_output, std::move(output_size), std::move(input_size), scales_d, scales_h, scales_w, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_UPSAMPLE_NEAREST3D_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, std::move(input_size));trace.append_arg(trace_idx, scales_d);trace.append_arg(trace_idx, scales_h);trace.append_arg(trace_idx, scales_w);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor & wrap_sigmoid_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sigmoid_backward_outf(dispatchKeySet, grad_output, output, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SIGMOID_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, output);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_sigmoid_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::sigmoid_backward(dispatchKeySet, grad_output, output);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SIGMOID_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, output);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_logit_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, c10::optional<double> eps, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logit_backward_outf(dispatchKeySet, grad_output, self, eps, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LOGIT_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, eps);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_logit_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, c10::optional<double> eps) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::logit_backward(dispatchKeySet, grad_output, self, eps);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LOGIT_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, eps);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_tanh_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::tanh_backward_outf(dispatchKeySet, grad_output, output, grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_TANH_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, output);trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_tanh_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::tanh_backward(dispatchKeySet, grad_output, output);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_TANH_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, output);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_slow_conv_transpose2d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::slow_conv_transpose2d_outf(dispatchKeySet, self, weight, std::move(kernel_size), bias, std::move(stride), std::move(padding), std::move(output_padding), std::move(dilation), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SLOW_CONV_TRANSPOSE2D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(output_padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_slow_conv_transpose2d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::slow_conv_transpose2d(dispatchKeySet, self, weight, std::move(kernel_size), bias, std::move(stride), std::move(padding), std::move(output_padding), std::move(dilation));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SLOW_CONV_TRANSPOSE2D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(output_padding));trace.append_arg(trace_idx, std::move(dilation));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrap_slow_conv_transpose2d_backward_grad_output(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation, const at::Tensor & columns, const at::Tensor & ones, at::Tensor & grad_input, at::Tensor & grad_weight, at::Tensor & grad_bias) {
  ensure_materialized(grad_output);ensure_materialized(self);ensure_materialized(weight);ensure_materialized(columns);ensure_materialized(ones);ensure_materialized(grad_input);ensure_materialized(grad_weight);ensure_materialized(grad_bias);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::slow_conv_transpose2d_backward_outf(dispatchKeySet, grad_output, self, weight, std::move(kernel_size), std::move(stride), std::move(padding), std::move(output_padding), std::move(dilation), columns, ones, grad_input, grad_weight, grad_bias);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_slow_conv_transpose2d_backward_output_mask(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation, const at::Tensor & columns, const at::Tensor & ones, std::array<bool,3> output_mask) {
  ensure_materialized(grad_output);ensure_materialized(self);ensure_materialized(weight);ensure_materialized(columns);ensure_materialized(ones);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::slow_conv_transpose2d_backward(dispatchKeySet, grad_output, self, weight, std::move(kernel_size), std::move(stride), std::move(padding), std::move(output_padding), std::move(dilation), columns, ones, std::move(output_mask));
}

at::Tensor & wrap_slow_conv_transpose3d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::slow_conv_transpose3d_outf(dispatchKeySet, self, weight, std::move(kernel_size), bias, std::move(stride), std::move(padding), std::move(output_padding), std::move(dilation), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SLOW_CONV_TRANSPOSE3D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(output_padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_slow_conv_transpose3d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::slow_conv_transpose3d(dispatchKeySet, self, weight, std::move(kernel_size), bias, std::move(stride), std::move(padding), std::move(output_padding), std::move(dilation));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SLOW_CONV_TRANSPOSE3D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(output_padding));trace.append_arg(trace_idx, std::move(dilation));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrap_slow_conv_transpose3d_backward_grad_output(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation, const at::Tensor & finput, const at::Tensor & fgrad_input, at::Tensor & grad_input, at::Tensor & grad_weight, at::Tensor & grad_bias) {
  ensure_materialized(grad_output);ensure_materialized(self);ensure_materialized(weight);ensure_materialized(finput);ensure_materialized(fgrad_input);ensure_materialized(grad_input);ensure_materialized(grad_weight);ensure_materialized(grad_bias);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::slow_conv_transpose3d_backward_outf(dispatchKeySet, grad_output, self, weight, std::move(kernel_size), std::move(stride), std::move(padding), std::move(output_padding), std::move(dilation), finput, fgrad_input, grad_input, grad_weight, grad_bias);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_slow_conv_transpose3d_backward_output_mask(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation, const at::Tensor & finput, const at::Tensor & fgrad_input, std::array<bool,3> output_mask) {
  ensure_materialized(grad_output);ensure_materialized(self);ensure_materialized(weight);ensure_materialized(finput);ensure_materialized(fgrad_input);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::slow_conv_transpose3d_backward(dispatchKeySet, grad_output, self, weight, std::move(kernel_size), std::move(stride), std::move(padding), std::move(output_padding), std::move(dilation), finput, fgrad_input, std::move(output_mask));
}

at::Tensor & wrap_thnn_conv2d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::thnn_conv2d_outf(dispatchKeySet, self, weight, std::move(kernel_size), bias, std::move(stride), std::move(padding), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_THNN_CONV2D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_thnn_conv2d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::thnn_conv2d(dispatchKeySet, self, weight, std::move(kernel_size), bias, std::move(stride), std::move(padding));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_THNN_CONV2D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrap_thnn_conv2d_forward_output(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & output, at::Tensor & finput, at::Tensor & fgrad_input) {
  ensure_materialized(self);ensure_materialized(weight);ensure_materialized(bias);ensure_materialized(output);ensure_materialized(finput);ensure_materialized(fgrad_input);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::thnn_conv2d_forward_outf(dispatchKeySet, self, weight, std::move(kernel_size), bias, std::move(stride), std::move(padding), output, finput, fgrad_input);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_thnn_conv2d_forward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
  ensure_materialized(self);ensure_materialized(weight);ensure_materialized(bias);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::thnn_conv2d_forward(dispatchKeySet, self, weight, std::move(kernel_size), bias, std::move(stride), std::move(padding));
}

std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrap_thnn_conv2d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, const at::Tensor & finput, const at::Tensor & fgrad_input, at::Tensor & grad_input, at::Tensor & grad_weight, at::Tensor & grad_bias) {
  ensure_materialized(grad_output);ensure_materialized(self);ensure_materialized(weight);ensure_materialized(finput);ensure_materialized(fgrad_input);ensure_materialized(grad_input);ensure_materialized(grad_weight);ensure_materialized(grad_bias);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::thnn_conv2d_backward_outf(dispatchKeySet, grad_output, self, weight, std::move(kernel_size), std::move(stride), std::move(padding), finput, fgrad_input, grad_input, grad_weight, grad_bias);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_thnn_conv2d_backward_output_mask(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, const at::Tensor & finput, const at::Tensor & fgrad_input, std::array<bool,3> output_mask) {
  ensure_materialized(grad_output);ensure_materialized(self);ensure_materialized(weight);ensure_materialized(finput);ensure_materialized(fgrad_input);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::thnn_conv2d_backward(dispatchKeySet, grad_output, self, weight, std::move(kernel_size), std::move(stride), std::move(padding), finput, fgrad_input, std::move(output_mask));
}

at::Tensor & wrap_thnn_conv_depthwise2d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::thnn_conv_depthwise2d_outf(dispatchKeySet, self, weight, std::move(kernel_size), bias, std::move(stride), std::move(padding), std::move(dilation), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_THNN_CONV_DEPTHWISE2D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_thnn_conv_depthwise2d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::thnn_conv_depthwise2d(dispatchKeySet, self, weight, std::move(kernel_size), bias, std::move(stride), std::move(padding), std::move(dilation));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_THNN_CONV_DEPTHWISE2D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_thnn_conv_depthwise2d_forward_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::thnn_conv_depthwise2d_forward_outf(dispatchKeySet, self, weight, std::move(kernel_size), bias, std::move(stride), std::move(padding), std::move(dilation), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_THNN_CONV_DEPTHWISE2D_FORWARD_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_thnn_conv_depthwise2d_forward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::thnn_conv_depthwise2d_forward(dispatchKeySet, self, weight, std::move(kernel_size), bias, std::move(stride), std::move(padding), std::move(dilation));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_THNN_CONV_DEPTHWISE2D_FORWARD, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor &,at::Tensor &> wrap_thnn_conv_depthwise2d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, at::Tensor & grad_input, at::Tensor & grad_weight) {
  ensure_materialized(grad_output);ensure_materialized(self);ensure_materialized(weight);ensure_materialized(grad_input);ensure_materialized(grad_weight);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::thnn_conv_depthwise2d_backward_outf(dispatchKeySet, grad_output, self, weight, std::move(kernel_size), std::move(stride), std::move(padding), std::move(dilation), grad_input, grad_weight);
}

std::tuple<at::Tensor,at::Tensor> wrap_thnn_conv_depthwise2d_backward_output_mask(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, std::array<bool,2> output_mask) {
  ensure_materialized(grad_output);ensure_materialized(self);ensure_materialized(weight);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::thnn_conv_depthwise2d_backward(dispatchKeySet, grad_output, self, weight, std::move(kernel_size), std::move(stride), std::move(padding), std::move(dilation), std::move(output_mask));
}

at::Tensor wrap_conv_depthwise3d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::conv_depthwise3d(dispatchKeySet, self, weight, std::move(kernel_size), bias, std::move(stride), std::move(padding), std::move(dilation));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_CONV_DEPTHWISE3D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrap_conv_depthwise3d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, at::Tensor & grad_input, at::Tensor & grad_weight, at::Tensor & grad_bias) {
  ensure_materialized(grad_output);ensure_materialized(self);ensure_materialized(weight);ensure_materialized(grad_input);ensure_materialized(grad_weight);ensure_materialized(grad_bias);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::conv_depthwise3d_backward_outf(dispatchKeySet, grad_output, self, weight, std::move(kernel_size), std::move(stride), std::move(padding), std::move(dilation), grad_input, grad_weight, grad_bias);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_conv_depthwise3d_backward_output_mask(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, std::array<bool,3> output_mask) {
  ensure_materialized(grad_output);ensure_materialized(self);ensure_materialized(weight);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::conv_depthwise3d_backward(dispatchKeySet, grad_output, self, weight, std::move(kernel_size), std::move(stride), std::move(padding), std::move(dilation), std::move(output_mask));
}

at::Tensor & wrap_slow_conv3d_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::slow_conv3d_outf(dispatchKeySet, self, weight, std::move(kernel_size), bias, std::move(stride), std::move(padding), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SLOW_CONV3D_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_slow_conv3d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::slow_conv3d(dispatchKeySet, self, weight, std::move(kernel_size), bias, std::move(stride), std::move(padding));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SLOW_CONV3D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrap_slow_conv3d_forward_output(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & output, at::Tensor & finput, at::Tensor & fgrad_input) {
  ensure_materialized(self);ensure_materialized(weight);ensure_materialized(bias);ensure_materialized(output);ensure_materialized(finput);ensure_materialized(fgrad_input);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::slow_conv3d_forward_outf(dispatchKeySet, self, weight, std::move(kernel_size), bias, std::move(stride), std::move(padding), output, finput, fgrad_input);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_slow_conv3d_forward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
  ensure_materialized(self);ensure_materialized(weight);ensure_materialized(bias);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::slow_conv3d_forward(dispatchKeySet, self, weight, std::move(kernel_size), bias, std::move(stride), std::move(padding));
}

std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrap_slow_conv3d_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, const at::Tensor & finput, const at::Tensor & fgrad_input, at::Tensor & grad_input, at::Tensor & grad_weight, at::Tensor & grad_bias) {
  ensure_materialized(grad_output);ensure_materialized(self);ensure_materialized(weight);ensure_materialized(finput);ensure_materialized(fgrad_input);ensure_materialized(grad_input);ensure_materialized(grad_weight);ensure_materialized(grad_bias);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::slow_conv3d_backward_outf(dispatchKeySet, grad_output, self, weight, std::move(kernel_size), std::move(stride), std::move(padding), finput, fgrad_input, grad_input, grad_weight, grad_bias);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_slow_conv3d_backward_output_mask(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, const at::Tensor & finput, const at::Tensor & fgrad_input, std::array<bool,3> output_mask) {
  ensure_materialized(grad_output);ensure_materialized(self);ensure_materialized(weight);ensure_materialized(finput);ensure_materialized(fgrad_input);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::slow_conv3d_backward(dispatchKeySet, grad_output, self, weight, std::move(kernel_size), std::move(stride), std::move(padding), finput, fgrad_input, std::move(output_mask));
}

at::Tensor wrap_slow_conv_dilated2d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::slow_conv_dilated2d(dispatchKeySet, self, weight, std::move(kernel_size), bias, std::move(stride), std::move(padding), std::move(dilation));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SLOW_CONV_DILATED2D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_slow_conv_dilated2d_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, std::array<bool,3> output_mask) {
  ensure_materialized(grad_output);ensure_materialized(self);ensure_materialized(weight);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::slow_conv_dilated2d_backward(dispatchKeySet, grad_output, self, weight, std::move(kernel_size), std::move(stride), std::move(padding), std::move(dilation), std::move(output_mask));
}

at::Tensor wrap_slow_conv_dilated3d(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::slow_conv_dilated3d(dispatchKeySet, self, weight, std::move(kernel_size), bias, std::move(stride), std::move(padding), std::move(dilation));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SLOW_CONV_DILATED3D, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, weight);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, bias);trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(dilation));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_slow_conv_dilated3d_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, std::array<bool,3> output_mask) {
  ensure_materialized(grad_output);ensure_materialized(self);ensure_materialized(weight);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::slow_conv_dilated3d_backward(dispatchKeySet, grad_output, self, weight, std::move(kernel_size), std::move(stride), std::move(padding), std::move(dilation), std::move(output_mask));
}

at::Tensor & wrap_col2im_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::col2im_outf(dispatchKeySet, self, std::move(output_size), std::move(kernel_size), std::move(dilation), std::move(padding), std::move(stride), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_COL2IM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_col2im(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::col2im(dispatchKeySet, self, std::move(output_size), std::move(kernel_size), std::move(dilation), std::move(padding), std::move(stride));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_COL2IM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(output_size));trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_col2im_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::col2im_backward_outf(dispatchKeySet, grad_output, std::move(kernel_size), std::move(dilation), std::move(padding), std::move(stride), grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_COL2IM_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_col2im_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::col2im_backward(dispatchKeySet, grad_output, std::move(kernel_size), std::move(dilation), std::move(padding), std::move(stride));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_COL2IM_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_column_stack(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::column_stack(dispatchKeySet, std::move(tensors));
  }
  auto defaults = compute_dtype(tensors);
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(default_dtype, default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_COLUMN_STACK, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_column_stack_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::column_stack_outf(dispatchKeySet, std::move(tensors), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_COLUMN_STACK_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_im2col_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::im2col_outf(dispatchKeySet, self, std::move(kernel_size), std::move(dilation), std::move(padding), std::move(stride), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_IM2COL_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_im2col(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::im2col(dispatchKeySet, self, std::move(kernel_size), std::move(dilation), std::move(padding), std::move(stride));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_IM2COL, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_im2col_backward_grad_input(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef input_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride, at::Tensor & grad_input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::im2col_backward_outf(dispatchKeySet, grad_output, std::move(input_size), std::move(kernel_size), std::move(dilation), std::move(padding), std::move(stride), grad_input);
  }
  TorchyTensor *tt = prepare_in_place(grad_input);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_IM2COL_BACKWARD_GRAD_INPUT, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, std::move(input_size));trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));trace.append_arg(trace_idx, grad_input);
  finish_in_place(tt, trace_idx);
  return grad_input;
}

at::Tensor wrap_im2col_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef input_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::im2col_backward(dispatchKeySet, grad_output, std::move(input_size), std::move(kernel_size), std::move(dilation), std::move(padding), std::move(stride));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad_output.dtype(), grad_output.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_IM2COL_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad_output);trace.append_arg(trace_idx, std::move(input_size));trace.append_arg(trace_idx, std::move(kernel_size));trace.append_arg(trace_idx, std::move(dilation));trace.append_arg(trace_idx, std::move(padding));trace.append_arg(trace_idx, std::move(stride));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_isfinite(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::isfinite(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(kBool), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ISFINITE, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_isinf(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::isinf(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(kBool), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ISINF, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

void wrap_record_stream(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Stream s) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::record_stream(dispatchKeySet, self, std::move(s));
}

at::Tensor wrap_isposinf(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::isposinf(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(kBool), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ISPOSINF, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_isposinf_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::isposinf_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ISPOSINF_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_isneginf(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::isneginf(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(scalarTypeToTypeMeta(kBool), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_ISNEGINF, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_isneginf_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::isneginf_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_ISNEGINF_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap__add_batch_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t batch_dim, int64_t level) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_add_batch_dim(dispatchKeySet, self, batch_dim, level);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__ADD_BATCH_DIM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, batch_dim);trace.append_arg(trace_idx, level);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__remove_batch_dim(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t level, int64_t batch_size, int64_t out_dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_remove_batch_dim(dispatchKeySet, self, level, batch_size, out_dim);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__REMOVE_BATCH_DIM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, level);trace.append_arg(trace_idx, batch_size);trace.append_arg(trace_idx, out_dim);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_special_entr_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_entr_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SPECIAL_ENTR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_special_expm1(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_expm1(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SPECIAL_EXPM1, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_special_expm1_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_expm1_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SPECIAL_EXPM1_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_special_exp2(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_exp2(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SPECIAL_EXP2, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_special_exp2_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_exp2_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SPECIAL_EXP2_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_special_gammaln(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_gammaln(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SPECIAL_GAMMALN, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_special_gammaln_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_gammaln_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SPECIAL_GAMMALN_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_special_erf(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_erf(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SPECIAL_ERF, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_special_erf_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_erf_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SPECIAL_ERF_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_special_erfc(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_erfc(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SPECIAL_ERFC, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_special_erfc_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_erfc_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SPECIAL_ERFC_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_special_erfinv(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_erfinv(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SPECIAL_ERFINV, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_special_erfinv_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_erfinv_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SPECIAL_ERFINV_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_special_xlog1py_self_scalar(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_xlog1py(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(other.dtype(), other.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SPECIAL_XLOG1PY_SELF_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_special_xlog1py_other_scalar(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_xlog1py(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SPECIAL_XLOG1PY_OTHER_SCALAR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_special_xlog1py_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_xlog1py_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SPECIAL_XLOG1PY_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_special_xlog1py_self_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_xlog1py_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SPECIAL_XLOG1PY_SELF_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_special_xlog1py_other_scalar_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_xlog1py_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SPECIAL_XLOG1PY_OTHER_SCALAR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_special_i0e_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_i0e_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SPECIAL_I0E_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_special_logit(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> eps) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_logit(dispatchKeySet, self, eps);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SPECIAL_LOGIT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, eps);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_special_logit_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> eps, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_logit_outf(dispatchKeySet, self, eps, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SPECIAL_LOGIT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, eps);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_special_expit(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_expit(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SPECIAL_EXPIT, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_special_expit_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::special_expit_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_SPECIAL_EXPIT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_fft_fft(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_fft(dispatchKeySet, self, n, dim, std::move(norm));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FFT_FFT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(norm));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fft_fft_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_fft_outf(dispatchKeySet, self, n, dim, std::move(norm), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FFT_FFT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(norm));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_fft_ifft(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_ifft(dispatchKeySet, self, n, dim, std::move(norm));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FFT_IFFT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(norm));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fft_ifft_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_ifft_outf(dispatchKeySet, self, n, dim, std::move(norm), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FFT_IFFT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(norm));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_fft_rfft(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_rfft(dispatchKeySet, self, n, dim, std::move(norm));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FFT_RFFT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(norm));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fft_rfft_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_rfft_outf(dispatchKeySet, self, n, dim, std::move(norm), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FFT_RFFT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(norm));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_fft_irfft(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_irfft(dispatchKeySet, self, n, dim, std::move(norm));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FFT_IRFFT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(norm));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fft_irfft_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_irfft_outf(dispatchKeySet, self, n, dim, std::move(norm), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FFT_IRFFT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(norm));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_fft_hfft(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_hfft(dispatchKeySet, self, n, dim, std::move(norm));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FFT_HFFT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(norm));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fft_hfft_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_hfft_outf(dispatchKeySet, self, n, dim, std::move(norm), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FFT_HFFT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(norm));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_fft_ihfft(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_ihfft(dispatchKeySet, self, n, dim, std::move(norm));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FFT_IHFFT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(norm));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fft_ihfft_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_ihfft_outf(dispatchKeySet, self, n, dim, std::move(norm), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FFT_IHFFT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, dim);trace.append_arg(trace_idx, std::move(norm));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_fft_fft2(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_fft2(dispatchKeySet, self, std::move(s), std::move(dim), std::move(norm));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FFT_FFT2, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(s));trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(norm));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fft_fft2_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_fft2_outf(dispatchKeySet, self, std::move(s), std::move(dim), std::move(norm), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FFT_FFT2_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(s));trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(norm));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_fft_ifft2(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_ifft2(dispatchKeySet, self, std::move(s), std::move(dim), std::move(norm));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FFT_IFFT2, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(s));trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(norm));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fft_ifft2_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_ifft2_outf(dispatchKeySet, self, std::move(s), std::move(dim), std::move(norm), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FFT_IFFT2_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(s));trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(norm));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_fft_rfft2(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_rfft2(dispatchKeySet, self, std::move(s), std::move(dim), std::move(norm));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FFT_RFFT2, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(s));trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(norm));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fft_rfft2_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_rfft2_outf(dispatchKeySet, self, std::move(s), std::move(dim), std::move(norm), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FFT_RFFT2_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(s));trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(norm));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_fft_irfft2(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_irfft2(dispatchKeySet, self, std::move(s), std::move(dim), std::move(norm));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FFT_IRFFT2, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(s));trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(norm));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fft_irfft2_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_irfft2_outf(dispatchKeySet, self, std::move(s), std::move(dim), std::move(norm), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FFT_IRFFT2_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(s));trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(norm));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_fft_fftn(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_fftn(dispatchKeySet, self, std::move(s), std::move(dim), std::move(norm));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FFT_FFTN, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(s));trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(norm));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fft_fftn_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_fftn_outf(dispatchKeySet, self, std::move(s), std::move(dim), std::move(norm), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FFT_FFTN_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(s));trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(norm));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_fft_ifftn(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_ifftn(dispatchKeySet, self, std::move(s), std::move(dim), std::move(norm));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FFT_IFFTN, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(s));trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(norm));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fft_ifftn_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_ifftn_outf(dispatchKeySet, self, std::move(s), std::move(dim), std::move(norm), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FFT_IFFTN_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(s));trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(norm));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_fft_rfftn(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_rfftn(dispatchKeySet, self, std::move(s), std::move(dim), std::move(norm));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FFT_RFFTN, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(s));trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(norm));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fft_rfftn_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_rfftn_outf(dispatchKeySet, self, std::move(s), std::move(dim), std::move(norm), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FFT_RFFTN_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(s));trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(norm));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_fft_irfftn(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_irfftn(dispatchKeySet, self, std::move(s), std::move(dim), std::move(norm));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FFT_IRFFTN, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(s));trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(norm));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fft_irfftn_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_irfftn_outf(dispatchKeySet, self, std::move(s), std::move(dim), std::move(norm), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FFT_IRFFTN_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(s));trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, std::move(norm));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_fft_fftfreq(c10::DispatchKeySet dispatchKeySet, int64_t n, double d, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_fftfreq(dispatchKeySet, n, d, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FFT_FFTFREQ, dispatchKeySet);
  trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, d);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fft_fftfreq_out(c10::DispatchKeySet dispatchKeySet, int64_t n, double d, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_fftfreq_outf(dispatchKeySet, n, d, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FFT_FFTFREQ_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, d);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_fft_rfftfreq(c10::DispatchKeySet dispatchKeySet, int64_t n, double d, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_rfftfreq(dispatchKeySet, n, d, std::move(dtype), std::move(layout), std::move(device), pin_memory);
  }
  auto defaults = compute_dtype();
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : default_dtype, device ? *device : default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FFT_RFFTFREQ, dispatchKeySet);
  trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, d);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, std::move(layout));trace.append_arg(trace_idx, std::move(device));trace.append_arg(trace_idx, pin_memory);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_fft_rfftfreq_out(c10::DispatchKeySet dispatchKeySet, int64_t n, double d, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_rfftfreq_outf(dispatchKeySet, n, d, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_FFT_RFFTFREQ_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, d);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_fft_fftshift(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_fftshift(dispatchKeySet, self, std::move(dim));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FFT_FFTSHIFT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_fft_ifftshift(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> dim) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::fft_ifftshift(dispatchKeySet, self, std::move(dim));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FFT_IFFTSHIFT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(dim));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor> wrap_linalg_cholesky_ex(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool check_errors) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::linalg_cholesky_ex(dispatchKeySet, self, check_errors);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_linalg_cholesky_ex_L(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool check_errors, at::Tensor & L, at::Tensor & info) {
  ensure_materialized(self);ensure_materialized(L);ensure_materialized(info);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::linalg_cholesky_ex_outf(dispatchKeySet, self, check_errors, L, info);
}

at::Tensor wrap_linalg_cholesky(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_cholesky(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_CHOLESKY, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_linalg_cholesky_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_cholesky_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_CHOLESKY_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_linalg_det(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_det(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_DET, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_linalg_det_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_det_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_DET_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_det(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::det(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_DET, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrap_linalg_lstsq(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & b, c10::optional<double> rcond, c10::optional<c10::string_view> driver) {
  ensure_materialized(self);ensure_materialized(b);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::linalg_lstsq(dispatchKeySet, self, b, rcond, std::move(driver));
}

std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> wrap_linalg_lstsq_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & b, c10::optional<double> rcond, c10::optional<c10::string_view> driver, at::Tensor & solution, at::Tensor & residuals, at::Tensor & rank, at::Tensor & singular_values) {
  ensure_materialized(self);ensure_materialized(b);ensure_materialized(solution);ensure_materialized(residuals);ensure_materialized(rank);ensure_materialized(singular_values);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::linalg_lstsq_outf(dispatchKeySet, self, b, rcond, std::move(driver), solution, residuals, rank, singular_values);
}

std::tuple<at::Tensor,at::Tensor> wrap_linalg_slogdet(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::linalg_slogdet(dispatchKeySet, self);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_linalg_slogdet_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & sign, at::Tensor & logabsdet) {
  ensure_materialized(self);ensure_materialized(sign);ensure_materialized(logabsdet);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::linalg_slogdet_outf(dispatchKeySet, self, sign, logabsdet);
}

std::tuple<at::Tensor,at::Tensor> wrap_linalg_eig(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::linalg_eig(dispatchKeySet, self);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_linalg_eig_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & eigenvalues, at::Tensor & eigenvectors) {
  ensure_materialized(self);ensure_materialized(eigenvalues);ensure_materialized(eigenvectors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::linalg_eig_outf(dispatchKeySet, self, eigenvalues, eigenvectors);
}

at::Tensor wrap_linalg_eigvals(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_eigvals(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_EIGVALS, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_linalg_eigvals_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_eigvals_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_EIGVALS_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

std::tuple<at::Tensor,at::Tensor> wrap_linalg_eigh(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view UPLO) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::linalg_eigh(dispatchKeySet, self, std::move(UPLO));
}

std::tuple<at::Tensor &,at::Tensor &> wrap_linalg_eigh_eigvals(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view UPLO, at::Tensor & eigvals, at::Tensor & eigvecs) {
  ensure_materialized(self);ensure_materialized(eigvals);ensure_materialized(eigvecs);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::linalg_eigh_outf(dispatchKeySet, self, std::move(UPLO), eigvals, eigvecs);
}

at::Tensor wrap_linalg_eigvalsh(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view UPLO) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_eigvalsh(dispatchKeySet, self, std::move(UPLO));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_EIGVALSH, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(UPLO));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_linalg_eigvalsh_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view UPLO, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_eigvalsh_outf(dispatchKeySet, self, std::move(UPLO), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_EIGVALSH_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(UPLO));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_linalg_householder_product(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & tau) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_householder_product(dispatchKeySet, input, tau);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_HOUSEHOLDER_PRODUCT, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, tau);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_linalg_householder_product_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & tau, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_householder_product_outf(dispatchKeySet, input, tau, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_HOUSEHOLDER_PRODUCT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, tau);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap__linalg_inv_out_helper_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Tensor & infos_lu, at::Tensor & infos_getri) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_linalg_inv_out_helper_(dispatchKeySet, self, infos_lu, infos_getri);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__LINALG_INV_OUT_HELPER_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, infos_lu);trace.append_arg(trace_idx, infos_getri);
  finish_in_place(tt, trace_idx);
  return self;
}

std::tuple<at::Tensor,at::Tensor> wrap_linalg_inv_ex(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool check_errors) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::linalg_inv_ex(dispatchKeySet, self, check_errors);
}

std::tuple<at::Tensor &,at::Tensor &> wrap_linalg_inv_ex_inverse(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool check_errors, at::Tensor & inverse, at::Tensor & info) {
  ensure_materialized(self);ensure_materialized(inverse);ensure_materialized(info);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::linalg_inv_ex_outf(dispatchKeySet, self, check_errors, inverse, info);
}

at::Tensor wrap_linalg_inv(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_inv(dispatchKeySet, self);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_INV, dispatchKeySet);
  trace.append_arg(trace_idx, self);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_linalg_inv_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_inv_outf(dispatchKeySet, self, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_INV_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_inner(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::inner(dispatchKeySet, self, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_INNER, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_inner_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::inner_outf(dispatchKeySet, self, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_INNER_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_outer(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec2) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::outer(dispatchKeySet, self, vec2);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_OUTER, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, vec2);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_outer_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec2, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::outer_outf(dispatchKeySet, self, vec2, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_OUTER_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, vec2);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_ger(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec2) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ger(dispatchKeySet, self, vec2);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_GER, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, vec2);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_ger_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec2, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::ger_outf(dispatchKeySet, self, vec2, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_GER_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, vec2);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_linalg_norm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_norm(dispatchKeySet, self, ord, std::move(dim), keepdim, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_NORM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, ord);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_linalg_norm_ord_str(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_norm(dispatchKeySet, self, std::move(ord), std::move(dim), keepdim, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_NORM_ORD_STR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(ord));trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_linalg_norm_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_norm_outf(dispatchKeySet, self, ord, std::move(dim), keepdim, std::move(dtype), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_NORM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, ord);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_linalg_norm_ord_str_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_norm_outf(dispatchKeySet, self, std::move(ord), std::move(dim), keepdim, std::move(dtype), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_NORM_ORD_STR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(ord));trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_linalg_vector_norm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_vector_norm(dispatchKeySet, self, ord, std::move(dim), keepdim, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_VECTOR_NORM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, ord);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_linalg_vector_norm_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_vector_norm_outf(dispatchKeySet, self, ord, std::move(dim), keepdim, std::move(dtype), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_VECTOR_NORM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, ord);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_linalg_matrix_norm(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_matrix_norm(dispatchKeySet, self, ord, std::move(dim), keepdim, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_MATRIX_NORM, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, ord);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_linalg_matrix_norm_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_matrix_norm_outf(dispatchKeySet, self, ord, std::move(dim), keepdim, std::move(dtype), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_MATRIX_NORM_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, ord);trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_linalg_matrix_norm_str_ord(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_matrix_norm(dispatchKeySet, self, std::move(ord), std::move(dim), keepdim, std::move(dtype));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dtype ? scalarTypeToTypeMeta(*dtype) : self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_MATRIX_NORM_STR_ORD, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(ord));trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_linalg_matrix_norm_str_ord_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_matrix_norm_outf(dispatchKeySet, self, std::move(ord), std::move(dim), keepdim, std::move(dtype), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_MATRIX_NORM_STR_ORD_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(ord));trace.append_arg(trace_idx, std::move(dim));trace.append_arg(trace_idx, keepdim);trace.append_arg(trace_idx, std::move(dtype));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrap_linalg_svd_U(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool full_matrices, at::Tensor & U, at::Tensor & S, at::Tensor & Vh) {
  ensure_materialized(self);ensure_materialized(U);ensure_materialized(S);ensure_materialized(Vh);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::linalg_svd_outf(dispatchKeySet, self, full_matrices, U, S, Vh);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> wrap_linalg_svd(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool full_matrices) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::linalg_svd(dispatchKeySet, self, full_matrices);
}

at::Tensor wrap_linalg_svdvals(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_svdvals(dispatchKeySet, input);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_SVDVALS, dispatchKeySet);
  trace.append_arg(trace_idx, input);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_linalg_svdvals_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_svdvals_outf(dispatchKeySet, input, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_SVDVALS_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_linalg_cond(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_cond(dispatchKeySet, self, p);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_COND, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_linalg_cond_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_cond_outf(dispatchKeySet, self, p, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_COND_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, p);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_linalg_cond_p_str(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view p) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_cond(dispatchKeySet, self, std::move(p));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_COND_P_STR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(p));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_linalg_cond_p_str_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view p, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_cond_outf(dispatchKeySet, self, std::move(p), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_COND_P_STR_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, std::move(p));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_linalg_pinv(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double rcond, bool hermitian) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_pinv(dispatchKeySet, self, rcond, hermitian);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_PINV, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, rcond);trace.append_arg(trace_idx, hermitian);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_linalg_pinv_rcond_tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & rcond, bool hermitian) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_pinv(dispatchKeySet, self, rcond, hermitian);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_PINV_RCOND_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, rcond);trace.append_arg(trace_idx, hermitian);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_linalg_pinv_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double rcond, bool hermitian, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_pinv_outf(dispatchKeySet, self, rcond, hermitian, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_PINV_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, rcond);trace.append_arg(trace_idx, hermitian);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap_linalg_pinv_out_rcond_tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & rcond, bool hermitian, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_pinv_outf(dispatchKeySet, self, rcond, hermitian, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_PINV_OUT_RCOND_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, rcond);trace.append_arg(trace_idx, hermitian);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor & wrap__linalg_solve_out_helper_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Tensor & other, at::Tensor & infos) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_linalg_solve_out_helper_(dispatchKeySet, self, other, infos);
  }
  TorchyTensor *tt = prepare_in_place(self);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H__LINALG_SOLVE_OUT_HELPER_, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, infos);
  finish_in_place(tt, trace_idx);
  return self;
}

at::Tensor wrap_linalg_solve(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & other) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_solve(dispatchKeySet, input, other);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_SOLVE, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, other);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_linalg_solve_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & other, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_solve_outf(dispatchKeySet, input, other, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_SOLVE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_linalg_tensorinv(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t ind) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_tensorinv(dispatchKeySet, self, ind);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_TENSORINV, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, ind);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_linalg_tensorinv_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t ind, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_tensorinv_outf(dispatchKeySet, self, ind, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_TENSORINV_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, ind);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_linalg_tensorsolve(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<at::IntArrayRef> dims) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_tensorsolve(dispatchKeySet, self, other, std::move(dims));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_TENSORSOLVE, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, std::move(dims));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_linalg_tensorsolve_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<at::IntArrayRef> dims, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_tensorsolve_outf(dispatchKeySet, self, other, std::move(dims), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_TENSORSOLVE_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, std::move(dims));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

std::tuple<at::Tensor,at::Tensor> wrap_linalg_qr(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view mode) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::linalg_qr(dispatchKeySet, self, std::move(mode));
}

std::tuple<at::Tensor &,at::Tensor &> wrap_linalg_qr_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view mode, at::Tensor & Q, at::Tensor & R) {
  ensure_materialized(self);ensure_materialized(Q);ensure_materialized(R);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::linalg_qr_outf(dispatchKeySet, self, std::move(mode), Q, R);
}

std::tuple<at::Tensor,at::Tensor> wrap__linalg_qr_helper(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view mode) {
  ensure_materialized(self);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::_linalg_qr_helper(dispatchKeySet, self, std::move(mode));
}

at::Tensor wrap_linalg_matrix_power(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t n) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_matrix_power(dispatchKeySet, self, n);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_MATRIX_POWER, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, n);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_linalg_matrix_power_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t n, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_matrix_power_outf(dispatchKeySet, self, n, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_MATRIX_POWER_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, n);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_linalg_matrix_rank(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> tol, bool hermitian) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_matrix_rank(dispatchKeySet, self, tol, hermitian);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_MATRIX_RANK, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, tol);trace.append_arg(trace_idx, hermitian);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_linalg_matrix_rank_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> tol, bool hermitian, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_matrix_rank_outf(dispatchKeySet, self, tol, hermitian, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_MATRIX_RANK_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, tol);trace.append_arg(trace_idx, hermitian);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_linalg_matrix_rank_tol_tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & tol, bool hermitian) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_matrix_rank(dispatchKeySet, input, tol, hermitian);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(input.dtype(), input.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_MATRIX_RANK_TOL_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, tol);trace.append_arg(trace_idx, hermitian);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_linalg_matrix_rank_out_tol_tensor(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & tol, bool hermitian, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_matrix_rank_outf(dispatchKeySet, input, tol, hermitian, out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_MATRIX_RANK_OUT_TOL_TENSOR, dispatchKeySet);
  trace.append_arg(trace_idx, input);trace.append_arg(trace_idx, tol);trace.append_arg(trace_idx, hermitian);trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap_linalg_multi_dot(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_multi_dot(dispatchKeySet, std::move(tensors));
  }
  auto defaults = compute_dtype(tensors);
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(default_dtype, default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_LINALG_MULTI_DOT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor & wrap_linalg_multi_dot_out(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Tensor & out) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::linalg_multi_dot_outf(dispatchKeySet, std::move(tensors), out);
  }
  TorchyTensor *tt = prepare_in_place(out);
  unsigned trace_idx = trace.register_tensor(tt ? (uintptr_t)tt : DUMMY_TORCHY, H_LINALG_MULTI_DOT_OUT, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));trace.append_arg(trace_idx, out);
  finish_in_place(tt, trace_idx);
  return out;
}

at::Tensor wrap__test_serialization_subcmul(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_test_serialization_subcmul(dispatchKeySet, self, other, alpha);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(self.dtype(), self.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__TEST_SERIALIZATION_SUBCMUL, dispatchKeySet);
  trace.append_arg(trace_idx, self);trace.append_arg(trace_idx, other);trace.append_arg(trace_idx, alpha);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__test_optional_intlist(c10::DispatchKeySet dispatchKeySet, const at::Tensor & values, c10::optional<at::IntArrayRef> addends) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_test_optional_intlist(dispatchKeySet, values, std::move(addends));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(values.dtype(), values.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__TEST_OPTIONAL_INTLIST, dispatchKeySet);
  trace.append_arg(trace_idx, values);trace.append_arg(trace_idx, std::move(addends));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__test_optional_filled_intlist(c10::DispatchKeySet dispatchKeySet, const at::Tensor & values, c10::optional<at::IntArrayRef> addends) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_test_optional_filled_intlist(dispatchKeySet, values, std::move(addends));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(values.dtype(), values.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__TEST_OPTIONAL_FILLED_INTLIST, dispatchKeySet);
  trace.append_arg(trace_idx, values);trace.append_arg(trace_idx, std::move(addends));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__test_optional_floatlist(c10::DispatchKeySet dispatchKeySet, const at::Tensor & values, c10::optional<at::ArrayRef<double>> addends) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_test_optional_floatlist(dispatchKeySet, values, addends);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(values.dtype(), values.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__TEST_OPTIONAL_FLOATLIST, dispatchKeySet);
  trace.append_arg(trace_idx, values);trace.append_arg(trace_idx, addends);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__test_string_default(c10::DispatchKeySet dispatchKeySet, const at::Tensor & dummy, c10::string_view a, c10::string_view b) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_test_string_default(dispatchKeySet, dummy, std::move(a), std::move(b));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dummy.dtype(), dummy.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__TEST_STRING_DEFAULT, dispatchKeySet);
  trace.append_arg(trace_idx, dummy);trace.append_arg(trace_idx, std::move(a));trace.append_arg(trace_idx, std::move(b));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__test_ambiguous_defaults_a(c10::DispatchKeySet dispatchKeySet, const at::Tensor & dummy, int64_t a, int64_t b) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_test_ambiguous_defaults(dispatchKeySet, dummy, a, b);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dummy.dtype(), dummy.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__TEST_AMBIGUOUS_DEFAULTS_A, dispatchKeySet);
  trace.append_arg(trace_idx, dummy);trace.append_arg(trace_idx, a);trace.append_arg(trace_idx, b);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap__test_ambiguous_defaults_b(c10::DispatchKeySet dispatchKeySet, const at::Tensor & dummy, int64_t a, c10::string_view b) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::_test_ambiguous_defaults(dispatchKeySet, dummy, a, std::move(b));
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(dummy.dtype(), dummy.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H__TEST_AMBIGUOUS_DEFAULTS_B, dispatchKeySet);
  trace.append_arg(trace_idx, dummy);trace.append_arg(trace_idx, a);trace.append_arg(trace_idx, std::move(b));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_segment_reduce(c10::DispatchKeySet dispatchKeySet, const at::Tensor & data, c10::string_view reduce, const c10::optional<at::Tensor> & lengths, const c10::optional<at::Tensor> & indices, int64_t axis, bool unsafe, const c10::optional<at::Scalar> & initial) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::segment_reduce(dispatchKeySet, data, std::move(reduce), lengths, indices, axis, unsafe, initial);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(data.dtype(), data.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SEGMENT_REDUCE, dispatchKeySet);
  trace.append_arg(trace_idx, data);trace.append_arg(trace_idx, std::move(reduce));trace.append_arg(trace_idx, lengths);trace.append_arg(trace_idx, indices);trace.append_arg(trace_idx, axis);trace.append_arg(trace_idx, unsafe);trace.append_arg(trace_idx, initial);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_segment_reduce_backward(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & output, const at::Tensor & data, const c10::optional<at::Tensor> & lengths) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::segment_reduce_backward(dispatchKeySet, grad, output, data, lengths);
  }
  auto tt = at::detail::make_tensor<TorchyTensor>(grad.dtype(), grad.device());
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_SEGMENT_REDUCE_BACKWARD, dispatchKeySet);
  trace.append_arg(trace_idx, grad);trace.append_arg(trace_idx, output);trace.append_arg(trace_idx, data);trace.append_arg(trace_idx, lengths);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_pad_sequence(c10::DispatchKeySet dispatchKeySet, at::TensorList sequences, bool batch_first, double padding_value) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::pad_sequence(dispatchKeySet, std::move(sequences), batch_first, padding_value);
  }
  auto defaults = compute_dtype(sequences);
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(default_dtype, default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_PAD_SEQUENCE, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(sequences));trace.append_arg(trace_idx, batch_first);trace.append_arg(trace_idx, padding_value);
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

at::Tensor wrap_flatten_dense_tensors(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
  if (trace.is_flushing()) {
    dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
    return at::redispatch::flatten_dense_tensors(dispatchKeySet, std::move(tensors));
  }
  auto defaults = compute_dtype(tensors);
  auto &default_dtype = defaults.first;
  auto &default_device = defaults.second;
  auto tt = at::detail::make_tensor<TorchyTensor>(default_dtype, default_device);
  auto tt_ptr = tt.getIntrusivePtr().get();
  unsigned trace_idx = trace.register_tensor((uintptr_t)tt_ptr, H_FLATTEN_DENSE_TENSORS, dispatchKeySet);
  trace.append_arg(trace_idx, std::move(tensors));
  static_cast<TorchyTensor*>(tt_ptr)->set_idx(trace_idx);
  return tt;
}

std::vector<at::Tensor> wrap_unflatten_dense_tensors(c10::DispatchKeySet dispatchKeySet, const at::Tensor & flat, at::TensorList tensors) {
  ensure_materialized(flat);ensure_materialized(tensors);
  dispatchKeySet = dispatchKeySet & DispatchKeySet(DispatchKeySet::FULL_AFTER, DISPATCHKEY);
  return at::redispatch::unflatten_dense_tensors(dispatchKeySet, flat, std::move(tensors));
}
